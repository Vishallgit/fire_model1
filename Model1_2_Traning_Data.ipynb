{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97e0df8-7591-4ae0-9d85-ebb8ed387a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xarray\n",
      "  Downloading xarray-2025.4.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "Collecting netcdf4\n",
      "  Downloading netCDF4-1.7.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (9.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "Collecting dask\n",
      "  Downloading dask-2025.5.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m134.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting packaging>=23.2\n",
      "  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 KB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting numpy>=1.24\n",
      "  Downloading numpy-2.2.5-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting pandas>=2.1\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (66.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from netcdf4) (2020.6.20)\n",
      "Collecting cftime\n",
      "  Downloading cftime-1.6.4.post1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m156.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /usr/lib/python3/dist-packages (from dask) (2024.3.1)\n",
      "Collecting partd>=1.4.0\n",
      "  Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Collecting importlib_metadata>=4.13.0\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting cloudpickle>=3.0.0\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting toolz>=0.10.0\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting click>=8.1\n",
      "  Downloading click-8.2.0-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 KB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.3.1 in /usr/lib/python3/dist-packages (from dask) (5.4.1)\n",
      "Collecting zipp>=3.20\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=2.1->xarray) (2022.1)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 KB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 KB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting locket\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray) (1.16.0)\n",
      "Installing collected packages: zipp, tzdata, toolz, python-dateutil, packaging, numpy, locket, cloudpickle, click, partd, pandas, importlib_metadata, cftime, xarray, netcdf4, dask\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pipx 1.0.0 requires argcomplete>=1.9.4, but you have argcomplete 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cftime-1.6.4.post1 click-8.2.0 cloudpickle-3.1.1 dask-2025.5.0 importlib_metadata-8.7.0 locket-1.0.0 netcdf4-1.7.2 numpy-2.2.5 packaging-25.0 pandas-2.2.3 partd-1.4.2 python-dateutil-2.9.0.post0 toolz-1.0.0 tzdata-2025.2 xarray-2025.4.0 zipp-3.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xarray netcdf4 dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b1d0cbd-024a-4f10-994c-d9893f9e3257",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy>=1.12.0 --upgrade -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77942e42-431b-4453-ac66-117f4ed4b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde12051-b09f-4926-9c9d-9aad4838119d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.2.5\n",
      "SciPy version: 1.15.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"SciPy version: {sp.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a78e718e-7cee-4465-bb5c-aae6f571c59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask[distributed] in /home/ubuntu/.local/lib/python3.10/site-packages (2025.5.0)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask[distributed]) (3.1.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.13.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask[distributed]) (8.7.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask[distributed]) (1.4.2)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /usr/lib/python3/dist-packages (from dask[distributed]) (2024.3.1)\n",
      "Requirement already satisfied: click>=8.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask[distributed]) (8.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/lib/python3/dist-packages (from dask[distributed]) (5.4.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask[distributed]) (1.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask[distributed]) (25.0)\n",
      "Collecting distributed==2025.5.0\n",
      "  Downloading distributed-2025.5.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting tblib>=1.6.0\n",
      "  Downloading tblib-3.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in /usr/lib/python3/dist-packages (from distributed==2025.5.0->dask[distributed]) (1.0.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/lib/python3/dist-packages (from distributed==2025.5.0->dask[distributed]) (5.9.0)\n",
      "Collecting zict>=3.0.0\n",
      "  Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: locket>=1.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from distributed==2025.5.0->dask[distributed]) (1.0.0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /usr/lib/python3/dist-packages (from distributed==2025.5.0->dask[distributed]) (1.26.5)\n",
      "Collecting sortedcontainers>=2.0.5\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting tornado>=6.2.0\n",
      "  Downloading tornado-6.4.2-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (437 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 KB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.10.3 in /usr/lib/python3/dist-packages (from distributed==2025.5.0->dask[distributed]) (3.0.3)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/ubuntu/.local/lib/python3.10/site-packages (from importlib_metadata>=4.13.0->dask[distributed]) (3.21.0)\n",
      "Installing collected packages: sortedcontainers, zict, tornado, tblib, distributed\n",
      "Successfully installed distributed-2025.5.0 sortedcontainers-2.4.0 tblib-3.1.0 tornado-6.4.2 zict-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install \"dask[distributed]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79eebfad-c8da-4890-a16b-14c2efed0a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (5.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d04f5d9-88f7-4025-b500-7d87bb59d5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading static data into memory...\n",
      "Static shape = (11, 2051, 128, 128) [channels, patches, 128, 128]\n",
      "213 valid (year, month) combos for 3-month input => next-month label.\n",
      "Starting pool with 8 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Months: 100%|██████████| 213/213 [1:56:24<00:00, 32.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed: 213 month-chunks OK, 0 errors.\n",
      "\n",
      "All done. Generated Stage-1 samples in 'Stage1_Samples'.\n",
      "Total time: 6988.96 s (116.48 min).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create Stage-1 ConvLSTM samples for a 3-month lookback → next-month fire classification,\n",
    "treating static features as extra channels only at the final time step (t=2).\n",
    "\n",
    "Data Paths (based on your previous scripts and storage):\n",
    "  • Fire features:   \"fire/{year}/{FeatureName}.npz\"     # shape [12, num_patches, 128, 128]\n",
    "  • Climate features:\"climate/{year}/{FeatureName}.npz\"  # shape [12, num_patches, 128, 128]\n",
    "  • Static features: \"static_patches/{FeatureName}.npz\"  # shape [num_patches, 128, 128]\n",
    "  • Labels (binary): \"Patch_Labels/{year}/BurnedArea_Labels.npz\"  # shape [12, num_patches]\n",
    "\n",
    "Script Outline:\n",
    "  1) We loop over valid (year, month) pairs where we have (t-2, t-1, t) input data and (t+1) label.\n",
    "  2) For each dynamic directory (\"fire\", \"climate\"), load all .npz features for each needed month.\n",
    "  3) Combine these monthly patches into [3, dynamic_channels, num_patches, 128, 128].\n",
    "  4) Load static features once. For time steps 0 & 1, no static channels. For time=2, append static along channels.\n",
    "  5) Load label at (t+1) → shape [num_patches].\n",
    "  6) Save final sample as .npz: shape [num_patches, channels, time=3, 128, 128], plus label array [num_patches].\n",
    "  7) Parallelize over months to handle large data efficiently (450GB RAM).\n",
    "\n",
    "Adjust or extend to match your exact file naming & feature sets.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Customize these as needed\n",
    "START_YEAR = 2005\n",
    "END_YEAR   = 2022\n",
    "DYNAMIC_DIRS = [\"fire\", \"climate\"]     # Where monthly dynamic features live\n",
    "STATIC_DIR   = \"static_patches\"        # Where static .npz live\n",
    "LABELS_ROOT  = \"Patch_Labels\"          # Where monthly labels live\n",
    "OUTPUT_DIR   = \"Stage1_Samples\"        # Where final samples go\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def valid_year_months():\n",
    "    \"\"\"\n",
    "    Generate (year, month) combos for which we have (t-2, t-1, t) input and (t+1) label.\n",
    "    - Earliest possible t is START_YEAR, March  (since t-2 => January).\n",
    "    - Latest possible t is END_YEAR, November (since t+1 => December).\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    total_months = (END_YEAR - START_YEAR + 1) * 12\n",
    "    for y in range(START_YEAR, END_YEAR + 1):\n",
    "        for m in range(1, 13):\n",
    "            g = (y - START_YEAR)*12 + (m - 1)  # zero-based\n",
    "            if g >= 2 and g <= total_months - 2:\n",
    "                pairs.append((y, m))\n",
    "    return pairs\n",
    "\n",
    "def get_global_index(year, month):\n",
    "    \"\"\"Convert (year, month) to zero-based index from (START_YEAR,1)=0 up to (END_YEAR,12).\"\"\"\n",
    "    return (year - START_YEAR)*12 + (month - 1)\n",
    "\n",
    "def get_year_month(global_idx):\n",
    "    \"\"\"Inverse of get_global_index -> (year, month).\"\"\"\n",
    "    y = global_idx // 12 + START_YEAR\n",
    "    m = global_idx % 12 + 1\n",
    "    return (y, m)\n",
    "\n",
    "def load_single_month_feature(dir_path, year, feature_name, month_idx):\n",
    "    \"\"\"\n",
    "    Loads one month's data from e.g. \"dir_path/{year}/{feature_name}.npz\".\n",
    "    Shape inside: [12, num_patches, 128, 128].\n",
    "    month_idx is 0-based (Jan=0..Dec=11).\n",
    "    Returns float32 array [num_patches, 128, 128].\n",
    "    \"\"\"\n",
    "    fpath = os.path.join(dir_path, str(year), f\"{feature_name}.npz\")\n",
    "    if not os.path.exists(fpath):\n",
    "        raise FileNotFoundError(f\"File not found: {fpath}\")\n",
    "    with np.load(fpath, allow_pickle=True) as npz:\n",
    "        data_4d = npz[\"data\"]  # shape [12, num_patches, 128, 128]\n",
    "    if month_idx < 0 or month_idx >= data_4d.shape[0]:\n",
    "        raise ValueError(f\"Invalid month_idx={month_idx} for file={fpath}\")\n",
    "    return data_4d[month_idx]  # [num_patches, 128, 128]\n",
    "\n",
    "def load_labels(year, month):\n",
    "    \"\"\"\n",
    "    Loads binary labels from Patch_Labels/{year}/BurnedArea_Labels.npz,\n",
    "    shape [12, num_patches]. month is 1-based, so index=month-1.\n",
    "    Returns shape [num_patches].\n",
    "    \"\"\"\n",
    "    label_file = os.path.join(LABELS_ROOT, str(year), \"BurnedArea_Labels.npz\")\n",
    "    if not os.path.exists(label_file):\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_file}\")\n",
    "    with np.load(label_file, allow_pickle=True) as npz:\n",
    "        labels_2d = npz[\"labels\"]  # shape [12, num_patches]\n",
    "    return labels_2d[month - 1]\n",
    "\n",
    "def load_static_data():\n",
    "    \"\"\"\n",
    "    Loads all static .npz in STATIC_DIR and stacks them along a channel dim:\n",
    "      final shape: [channels, num_patches, 128, 128].\n",
    "    Returns an empty array if none found.\n",
    "    \"\"\"\n",
    "    files = sorted([f for f in os.listdir(STATIC_DIR) if f.endswith(\".npz\")])\n",
    "    if not files:\n",
    "        return np.empty((0,0,0,0), dtype=np.float32)\n",
    "\n",
    "    arrays = []\n",
    "    num_patches = None\n",
    "    for f in files:\n",
    "        fpath = os.path.join(STATIC_DIR, f)\n",
    "        with np.load(fpath, allow_pickle=True) as npz:\n",
    "            static_3d = npz[\"data\"]  # [num_patches, 128, 128]\n",
    "        if num_patches is None:\n",
    "            num_patches = static_3d.shape[0]\n",
    "        elif static_3d.shape[0] != num_patches:\n",
    "            raise ValueError(\"Mismatch in #patches among static features.\")\n",
    "        arrays.append(static_3d[None, ...])  # => [1, num_patches, 128, 128]\n",
    "\n",
    "    # Combine into shape [channels, num_patches, 128, 128]\n",
    "    return np.concatenate(arrays, axis=0).astype(np.float32)\n",
    "\n",
    "def build_sample(year, month, static_arr):\n",
    "    \"\"\"\n",
    "    Build a single sample chunk for time t=(year,month),\n",
    "    with 3-month lookback => label at t+1,\n",
    "    and static channels only at final time index (t=2).\n",
    "    Saves to Stage1_Samples/samples_YYYY_MM.npz as:\n",
    "      x_data: [num_patches, (dyn_ch+static_ch), 3, 128, 128]\n",
    "      y_label: [num_patches]\n",
    "    \"\"\"\n",
    "    # Convert (year, month) -> global index\n",
    "    t_idx = get_global_index(year, month)\n",
    "    # We'll load data for t-2, t-1, t => in_months, label from t+1 => label_month\n",
    "    in_indices = [t_idx-2, t_idx-1, t_idx]\n",
    "    label_idx = t_idx + 1\n",
    "    in_ym = [get_year_month(i) for i in in_indices]\n",
    "    label_ym = get_year_month(label_idx)\n",
    "\n",
    "    # 1) Gather dynamic features from \"fire\" + \"climate\"\n",
    "    # Let's discover feature files in each directory's {year} folder\n",
    "    # Then load each feature for each of the 3 months -> shape [3, num_patches, 128, 128], stack them in channel dimension\n",
    "    dynamic_stacks = []  # will hold arrays [3, num_patches, 128, 128] for each feature\n",
    "    dyn_file_list = []\n",
    "    for ddir in DYNAMIC_DIRS:\n",
    "        y_dir = os.path.join(ddir, str(in_ym[0][0]))  # e.g. \"fire/2005\"\n",
    "        if not os.path.isdir(ddir):\n",
    "            raise FileNotFoundError(f\"Dynamic directory not found: {ddir}\")\n",
    "        # Collect .npz in that directory's year subfolder\n",
    "        if os.path.isdir(y_dir):\n",
    "            for f in os.listdir(y_dir):\n",
    "                if f.endswith(\".npz\"):\n",
    "                    # e.g. \"E0.npz\", \"NDVI.npz\" etc.\n",
    "                    dyn_file_list.append((ddir, f.replace(\".npz\",\"\")))\n",
    "\n",
    "    # Remove duplicates if any, and sort for consistency\n",
    "    dyn_file_list = list(set(dyn_file_list))\n",
    "    dyn_file_list.sort()\n",
    "\n",
    "    # For each feature, load t-2, t-1, t\n",
    "    dynamic_data_list = []\n",
    "    for (dir_path, feat_name) in dyn_file_list:\n",
    "        # Each feature -> [3, num_patches, 128, 128]\n",
    "        three_stack = []\n",
    "        for (yy, mm) in in_ym:\n",
    "            # 0-based index for the month data\n",
    "            m_idx = mm - 1\n",
    "            arr = load_single_month_feature(dir_path, yy, feat_name, m_idx)\n",
    "            three_stack.append(arr[None, ...])  # shape [1, num_patches, 128, 128]\n",
    "        feat_3d = np.concatenate(three_stack, axis=0)  # => [3, num_patches, 128, 128]\n",
    "        dynamic_data_list.append(feat_3d)\n",
    "\n",
    "    if dynamic_data_list:\n",
    "        # Concatenate along a new \"channel\" dimension => final shape [3, dyn_channels, num_patches, 128, 128]\n",
    "        # But each item is [3, num_patches, 128, 128], so we stack them along axis=1\n",
    "        # We'll do the standard trick:\n",
    "        # 1) combine them along axis=0 => shape [ (n_features*3), num_patches, 128, 128]\n",
    "        # 2) reshape => [n_features, 3, num_patches, 128, 128]\n",
    "        merged = np.concatenate(dynamic_data_list, axis=0)  # shape [n_feats*3, num_patches, 128, 128]\n",
    "        n_feats = len(dynamic_data_list)\n",
    "        merged = merged.reshape(n_feats, 3, -1, 128, 128)   # => [n_feats, 3, num_patches, 128, 128]\n",
    "        merged = np.swapaxes(merged, 0, 1)                 # => [3, n_feats, num_patches, 128, 128]\n",
    "    else:\n",
    "        # No dynamic data found\n",
    "        merged = np.zeros((3, 0, 0, 128, 128), dtype=np.float32)\n",
    "\n",
    "    # merged => [3, dyn_channels, num_patches, 128, 128]\n",
    "    dyn_channels = merged.shape[1]\n",
    "    num_patches  = merged.shape[2]\n",
    "\n",
    "    # 2) Insert static at final time step => t=2\n",
    "    # static_arr: [static_channels, num_patches, 128, 128]\n",
    "    static_ch = static_arr.shape[0] if static_arr.size > 0 else 0\n",
    "\n",
    "    # final_data => [3, dyn_channels+static_ch, num_patches, 128, 128]\n",
    "    final_data = np.zeros(\n",
    "        (3, dyn_channels + static_ch, num_patches, 128, 128),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # Place dynamic data first\n",
    "    final_data[:, :dyn_channels, :, :, :] = merged\n",
    "\n",
    "    # Place static in final time step = 2, channels [dyn_channels : dyn_channels+static_ch]\n",
    "    if static_ch > 0:\n",
    "        # static_arr => [static_ch, num_patches, 128, 128]\n",
    "        # final_data[2, dyn_channels: ...] => shape [static_ch, num_patches, 128, 128]\n",
    "        final_data[2, dyn_channels:dyn_channels+static_ch, :, :, :] = static_arr\n",
    "\n",
    "    # 3) Load label at t+1 => label_ym\n",
    "    labels_1d = load_labels(label_ym[0], label_ym[1])  # shape [num_patches]\n",
    "\n",
    "    # 4) Reorder to [num_patches, channels, time=3, 128, 128]\n",
    "    # Currently => [time=3, ch, patches, 128, 128]\n",
    "    final_data = np.swapaxes(final_data, 1, 2)   # => [3, num_patches, (dyn+st), 128, 128]\n",
    "    final_data = np.swapaxes(final_data, 0, 1)   # => [num_patches, 3, (dyn+st), 128, 128]\n",
    "    final_data = np.swapaxes(final_data, 1, 2)   # => [num_patches, (dyn+st), 3, 128, 128]\n",
    "\n",
    "    out_name = f\"samples_{year:04d}_{month:02d}.npz\"\n",
    "    out_path = os.path.join(OUTPUT_DIR, out_name)\n",
    "\n",
    "    # Save\n",
    "    metadata = {\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"num_patches\": num_patches,\n",
    "        \"time_steps\": 3,\n",
    "        \"dyn_channels\": dyn_channels,\n",
    "        \"static_channels\": static_ch,\n",
    "        \"label_year_month\": label_ym,\n",
    "        \"description\": \"3-month input => next-month label; static at final time index only.\"\n",
    "    }\n",
    "\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        x_data=final_data.astype(np.float32),\n",
    "        y_label=labels_1d.astype(np.uint8),\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "    del merged, final_data, labels_1d\n",
    "    gc.collect()\n",
    "\n",
    "def process_month(arg):\n",
    "    \"\"\"Wrapper for multiprocessing: arg=(year, month, static_arr).\"\"\"\n",
    "    (year, month, static_arr) = arg\n",
    "    try:\n",
    "        build_sample(year, month, static_arr)\n",
    "        return (year, month, \"OK\")\n",
    "    except Exception as e:\n",
    "        return (year, month, f\"ERROR: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1) Load static features once\n",
    "    print(\"Loading static data into memory...\")\n",
    "    static_arr = load_static_data()\n",
    "    print(f\"Static shape = {static_arr.shape} [channels, patches, 128, 128]\")\n",
    "\n",
    "    # 2) Create list of valid (year, month)\n",
    "    vm_list = valid_year_months()\n",
    "    print(f\"{len(vm_list)} valid (year, month) combos for 3-month input => next-month label.\")\n",
    "\n",
    "    # Prepare args for parallel map\n",
    "    # We'll bundle static_arr in the argument so each process can access it read-only.\n",
    "    tasks = [(y, m, static_arr) for (y, m) in vm_list]\n",
    "\n",
    "    # 3) Parallel processing\n",
    "    num_workers = min(mp.cpu_count(), 8)\n",
    "    print(f\"Starting pool with {num_workers} workers...\")\n",
    "    with mp.Pool(num_workers) as pool:\n",
    "        results = list(\n",
    "            tqdm(pool.imap_unordered(process_month, tasks), total=len(tasks), desc=\"Processing Months\")\n",
    "        )\n",
    "\n",
    "    # 4) Summarize\n",
    "    ok_count = sum(1 for r in results if r[2] == \"OK\")\n",
    "    err_count = len(results) - ok_count\n",
    "    print(f\"\\nCompleted: {ok_count} month-chunks OK, {err_count} errors.\")\n",
    "    if err_count > 0:\n",
    "        print(\"Errors:\")\n",
    "        for r in results:\n",
    "            if r[2].startswith(\"ERROR\"):\n",
    "                print(r)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nAll done. Generated Stage-1 samples in '{OUTPUT_DIR}'.\")\n",
    "    print(f\"Total time: {elapsed:.2f} s ({elapsed/60:.2f} min).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.freeze_support()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab85d20-9883-4952-b006-28dd05feef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will read chunk files from: Stage1_Samples\n",
      "Splits saved to: splits\n",
      "Train years: {2016, 2017, 2018, 2019, 2020, 2012, 2013, 2014, 2015}\n",
      "Val year:    {2021}\n",
      "Test years:  {2010, 2011, 2022}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "159443c7-a25d-4e4b-8310-c0c1149941d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will read chunk files from: Stage1_Samples\n",
      "Splits saved to: splits\n",
      "Train years: {2016, 2017, 2018, 2019, 2020, 2012, 2013, 2014, 2015}\n",
      "Val year:    {2021}\n",
      "Test years:  {2010, 2011, 2022}\n",
      "Found 213 candidate chunk files in 'Stage1_Samples'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting & Counting: 100%|██████████| 213/213 [00:01<00:00, 107.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved split info to:\n",
      "  splits/split_info.json\n",
      "  splits/class_distribution.json\n",
      "\n",
      "=== Class Distribution Summary ===\n",
      "Train => Fire: 37298, Non-Fire: 184210, Ratio: 0.168, Total: 221508\n",
      "Val   => Fire: 3402, Non-Fire: 21210, Ratio: 0.138, Total: 24612\n",
      "Test  => Fire: 11698, Non-Fire: 60087, Ratio: 0.163, Total: 71785\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Cell [1.1]: Imports & Config ---\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Fixed random seed for reproducibility (if further random ops are needed)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "SAMPLES_DIR = \"Stage1_Samples\"  # where your 213 chunk files are\n",
    "SPLITS_DIR = \"splits\"           # output directory for split info\n",
    "os.makedirs(SPLITS_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_YEARS = set(range(2012, 2021))  # 2012..2020 inclusive\n",
    "VAL_YEARS   = {2021}\n",
    "TEST_YEARS  = {2010, 2011, 2022}\n",
    "\n",
    "print(f\"Will read chunk files from: {SAMPLES_DIR}\")\n",
    "print(f\"Splits saved to: {SPLITS_DIR}\")\n",
    "print(f\"Train years: {TRAIN_YEARS}\")\n",
    "print(f\"Val year:    {VAL_YEARS}\")\n",
    "print(f\"Test years:  {TEST_YEARS}\")\n",
    "\n",
    "\n",
    "# --- Cell [1.2]: Utility Functions ---\n",
    "\n",
    "def parse_year_month_from_filename(fname):\n",
    "    \"\"\"\n",
    "    Expects a filename like 'samples_YYYY_MM.npz'.\n",
    "    Returns (year, month) as integers.\n",
    "    Raises ValueError if the format is unexpected.\n",
    "    \"\"\"\n",
    "    # Remove extension\n",
    "    base = fname.replace(\".npz\", \"\")\n",
    "    # base should be 'samples_YYYY_MM'\n",
    "    parts = base.split(\"_\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Filename '{fname}' does not match 'samples_YYYY_MM.npz'\")\n",
    "    # parts[0] = 'samples'\n",
    "    # parts[1] = YYYY\n",
    "    # parts[2] = MM\n",
    "    try:\n",
    "        year = int(parts[1])\n",
    "        month = int(parts[2])\n",
    "    except:\n",
    "        raise ValueError(f\"Cannot parse year/month from '{fname}'\")\n",
    "    return year, month\n",
    "\n",
    "\n",
    "def load_labels_and_count_fire(npz_path):\n",
    "    \"\"\"\n",
    "    Opens 'npz_path', which should contain:\n",
    "      - y_label: shape [num_patches], values in {0,1}\n",
    "    Returns a tuple (count_fire, count_non_fire).\n",
    "    \"\"\"\n",
    "    with np.load(npz_path, allow_pickle=True) as data:\n",
    "        y = data[\"y_label\"]  # shape [num_patches], 0 or 1\n",
    "    y = y.astype(np.int32)\n",
    "    count_fire = int(np.sum(y == 1))\n",
    "    count_non_fire = int(np.sum(y == 0))\n",
    "    return count_fire, count_non_fire\n",
    "\n",
    "# --- Cell [1.3]: Identify Splits, Count Class Imbalance ---\n",
    "\n",
    "def split_and_analyze(samples_dir):\n",
    "    # Initialize containers\n",
    "    train_files = []\n",
    "    val_files   = []\n",
    "    test_files  = []\n",
    "\n",
    "    # Accumulators for class imbalance\n",
    "    # We'll store total fire / non-fire patch counts for each split\n",
    "    train_fire = train_nonfire = 0\n",
    "    val_fire   = val_nonfire   = 0\n",
    "    test_fire  = test_nonfire  = 0\n",
    "    \n",
    "    # Gather all chunk files\n",
    "    chunk_files = [f for f in os.listdir(samples_dir) if f.endswith(\".npz\")]\n",
    "    chunk_files.sort()\n",
    "\n",
    "    print(f\"Found {len(chunk_files)} candidate chunk files in '{samples_dir}'.\")\n",
    "\n",
    "    for fname in tqdm(chunk_files, desc=\"Splitting & Counting\"):\n",
    "        # Parse (year, month)\n",
    "        year, month = parse_year_month_from_filename(fname)\n",
    "\n",
    "        # Decide which split it belongs to\n",
    "        if year in TRAIN_YEARS:\n",
    "            split_name = \"train\"\n",
    "        elif year in VAL_YEARS:\n",
    "            split_name = \"val\"\n",
    "        elif year in TEST_YEARS:\n",
    "            split_name = \"test\"\n",
    "        else:\n",
    "            # According to your rule, we skip chunks outside those sets (e.g. 2005..2009)\n",
    "            # Or place them in test if you want. We'll skip them:\n",
    "            continue\n",
    "        \n",
    "        # Load label distribution\n",
    "        npz_path = os.path.join(samples_dir, fname)\n",
    "        c_fire, c_nonfire = load_labels_and_count_fire(npz_path)\n",
    "\n",
    "        # Accumulate counts\n",
    "        if split_name == \"train\":\n",
    "            train_files.append(fname)\n",
    "            train_fire    += c_fire\n",
    "            train_nonfire += c_nonfire\n",
    "        elif split_name == \"val\":\n",
    "            val_files.append(fname)\n",
    "            val_fire    += c_fire\n",
    "            val_nonfire += c_nonfire\n",
    "        else:  # test\n",
    "            test_files.append(fname)\n",
    "            test_fire    += c_fire\n",
    "            test_nonfire += c_nonfire\n",
    "\n",
    "    # Summaries\n",
    "    train_total = train_fire + train_nonfire\n",
    "    val_total   = val_fire + val_nonfire\n",
    "    test_total  = test_fire + test_nonfire\n",
    "\n",
    "    # Prepare results dictionary\n",
    "    results = {\n",
    "        \"splits\": {\n",
    "            \"train\": train_files,\n",
    "            \"validation\": val_files,\n",
    "            \"test\": test_files\n",
    "        },\n",
    "        \"class_distribution\": {\n",
    "            \"train\": {\n",
    "                \"fire_count\": train_fire,\n",
    "                \"non_fire_count\": train_nonfire,\n",
    "                \"total_patches\": train_total,\n",
    "                \"fire_ratio\": float(train_fire / train_total) if train_total>0 else 0.0\n",
    "            },\n",
    "            \"validation\": {\n",
    "                \"fire_count\": val_fire,\n",
    "                \"non_fire_count\": val_nonfire,\n",
    "                \"total_patches\": val_total,\n",
    "                \"fire_ratio\": float(val_fire / val_total) if val_total>0 else 0.0\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"fire_count\": test_fire,\n",
    "                \"non_fire_count\": test_nonfire,\n",
    "                \"total_patches\": test_total,\n",
    "                \"fire_ratio\": float(test_fire / test_total) if test_total>0 else 0.0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the splitting and analysis\n",
    "split_info = split_and_analyze(SAMPLES_DIR)\n",
    "\n",
    "\n",
    "# --- Cell [1.4]: Save Outputs & Final Summary ---\n",
    "\n",
    "import json\n",
    "\n",
    "# 1) Save the entire split info in a single JSON\n",
    "split_info_path = os.path.join(SPLITS_DIR, \"split_info.json\")\n",
    "with open(split_info_path, \"w\") as f:\n",
    "    json.dump(split_info, f, indent=2)\n",
    "\n",
    "# 2) Optionally, we can save the class distribution alone in a separate JSON\n",
    "dist_info_path = os.path.join(SPLITS_DIR, \"class_distribution.json\")\n",
    "with open(dist_info_path, \"w\") as f:\n",
    "    json.dump(split_info[\"class_distribution\"], f, indent=2)\n",
    "\n",
    "print(f\"Saved split info to:\\n  {split_info_path}\\n  {dist_info_path}\")\n",
    "\n",
    "# 3) Print a quick summary\n",
    "cdist = split_info[\"class_distribution\"]\n",
    "print(\"\\n=== Class Distribution Summary ===\")\n",
    "print(f\"Train => Fire: {cdist['train']['fire_count']}, Non-Fire: {cdist['train']['non_fire_count']}, \"\n",
    "      f\"Ratio: {cdist['train']['fire_ratio']:.3f}, Total: {cdist['train']['total_patches']}\")\n",
    "print(f\"Val   => Fire: {cdist['validation']['fire_count']}, Non-Fire: {cdist['validation']['non_fire_count']}, \"\n",
    "      f\"Ratio: {cdist['validation']['fire_ratio']:.3f}, Total: {cdist['validation']['total_patches']}\")\n",
    "print(f\"Test  => Fire: {cdist['test']['fire_count']}, Non-Fire: {cdist['test']['non_fire_count']}, \"\n",
    "      f\"Ratio: {cdist['test']['fire_ratio']:.3f}, Total: {cdist['test']['total_patches']}\")\n",
    "print(\"\\nDone.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bb4a947-40de-49ab-ae51-857ffc53837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.24.3\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.5\n",
      "    Uninstalling numpy-2.2.5:\n",
      "      Successfully uninstalled numpy-2.2.5\n",
      "Successfully installed numpy-1.24.3\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6019a0f6-ea83-44ca-8332-ec711672c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow==2.14.0\n",
      "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.1 kB)\n",
      "Collecting tensorflow-cpu-aws==2.14.0\n",
      "  Downloading tensorflow_cpu_aws-2.14.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (262.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.8/262.8 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting tensorboard<2.15,>=2.14\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m151.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 KB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (1.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/lib/python3/dist-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (0.2.0)\n",
      "Collecting ml-dtypes==0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m154.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/lib/python3/dist-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (4.10.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/lib/python3/dist-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (1.6.3)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2014_aarch64.whl (23.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m143.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting keras<2.15,>=2.14.0\n",
      "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m221.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (59.6.0)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/lib/python3/dist-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (3.6.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/lib/python3/dist-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (1.1.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/lib/python3/dist-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (4.21.12)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m202.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/lib/python3/dist-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (2.1.0)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (25.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/lib/python3/dist-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (1.13.3)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (1.24.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/lib/python3/dist-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (1.30.2)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/lib/python3/dist-packages (from tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (3.3.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/lib/python3/dist-packages (from tensorboard<2.15,>=2.14->tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (2.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.15,>=2.14->tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (2.25.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.15,>=2.14->tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (3.3.6)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.1/216.1 KB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_aarch64.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m276.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (0.2.1)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (3.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-cpu-aws==2.14.0->tensorflow==2.14.0) (0.4.8)\n",
      "Installing collected packages: libclang, flatbuffers, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, requests-oauthlib, ml-dtypes, keras, grpcio, gast, cachetools, google-auth, google-auth-oauthlib, tensorboard, tensorflow-cpu-aws, tensorflow\n",
      "Successfully installed cachetools-5.5.2 flatbuffers-25.2.10 gast-0.6.0 google-auth-2.40.1 google-auth-oauthlib-1.0.0 grpcio-1.71.0 keras-2.14.0 libclang-18.1.1 ml-dtypes-0.2.0 requests-oauthlib-2.0.0 rsa-4.9.1 tensorboard-2.14.1 tensorboard-data-server-0.7.2 tensorflow-2.14.0 tensorflow-cpu-aws-2.14.0 tensorflow-estimator-2.14.0 tensorflow-io-gcs-filesystem-0.37.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05a0f9-cae3-4b72-a4a6-eaac8a3e09f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c670a33d-aa33-444c-b4dc-b52ab86eea08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded splits from: splits/split_info.json\n",
      "Train files: 108\n",
      "Val   files: 12\n",
      "Test  files: 35\n",
      "tf.data Datasets created for train, val, and test.\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================\n",
    "# CELL [1]: Imports & Configuration\n",
    "# =================================================================================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Fixed seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "SPLITS_DIR   = \"splits\"          # Where split_info.json is stored\n",
    "SAMPLES_DIR  = \"Stage1_Samples\"  # Where the .npz chunk files are\n",
    "SPLIT_FILE   = \"split_info.json\"\n",
    "\n",
    "BATCH_SIZE   = 64\n",
    "OVERSAMPLE   = True              # Whether to oversample minority class in train\n",
    "MAX_RAM_USAGE= 1_000_000_000     # ~1GB, adjust to your memory constraints\n",
    "\n",
    "# Optionally specify data types for tf.data\n",
    "TF_DTYPE = tf.float32\n",
    "TF_LABEL_DTYPE = tf.int32\n",
    "\n",
    "# =================================================================================\n",
    "# CELL [2]: Load Splits & Check\n",
    "# =================================================================================\n",
    "\n",
    "def load_splits(splits_dir=SPLITS_DIR, split_file=SPLIT_FILE):\n",
    "    \"\"\"Load split_info.json containing train/val/test file lists.\"\"\"\n",
    "    path = os.path.join(splits_dir, split_file)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Split file not found: {path}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        info = json.load(f)\n",
    "    return info\n",
    "\n",
    "split_info = load_splits()\n",
    "train_files = split_info[\"splits\"][\"train\"]      # e.g. list of chunk filenames\n",
    "val_files   = split_info[\"splits\"][\"validation\"]\n",
    "test_files  = split_info[\"splits\"][\"test\"]\n",
    "\n",
    "print(\"Loaded splits from:\", os.path.join(SPLITS_DIR, SPLIT_FILE))\n",
    "print(f\"Train files: {len(train_files)}\")\n",
    "print(f\"Val   files: {len(val_files)}\")\n",
    "print(f\"Test  files: {len(test_files)}\")\n",
    "\n",
    "# =================================================================================\n",
    "# CELL [3]: Data Generator (Python) With Oversampling\n",
    "# =================================================================================\n",
    "\n",
    "def chunk_file_generator(file_list, samples_dir, batch_size, oversample=False, shuffle_files=True):\n",
    "    \"\"\"\n",
    "    A Python generator that:\n",
    "      1) Shuffles 'file_list' if requested.\n",
    "      2) Iterates over each chunk file in random order (if shuffle_files=True).\n",
    "      3) Loads x_data, y_label from the .npz file.\n",
    "      4) Optionally oversamples minority class.\n",
    "      5) Yields mini-batches of shape (batch_size, ...).\n",
    "\n",
    "    Yields:\n",
    "      (x_batch, y_batch) as NumPy arrays.\n",
    "\n",
    "    Use:\n",
    "      gen = chunk_file_generator(train_files, SAMPLES_DIR, BATCH_SIZE, oversample=True)\n",
    "      for x, y in gen:\n",
    "          ...\n",
    "\n",
    "    We'll later wrap this in tf.data.Dataset.from_generator(...)\n",
    "    \"\"\"\n",
    "    # Make a copy so we don’t modify the original list\n",
    "    files = list(file_list)\n",
    "    if shuffle_files:\n",
    "        random.shuffle(files)\n",
    "\n",
    "    for fname in files:\n",
    "        # Load chunk data from .npz\n",
    "        fpath = os.path.join(samples_dir, fname)\n",
    "        if not os.path.exists(fpath):\n",
    "            raise FileNotFoundError(f\"Missing chunk file: {fpath}\")\n",
    "\n",
    "        with np.load(fpath, allow_pickle=True) as data:\n",
    "            x_data = data[\"x_data\"]  # shape [N, channels, time=3, 128, 128]\n",
    "            y_data = data[\"y_label\"] # shape [N]\n",
    "        \n",
    "        # Approx memory check\n",
    "        approx_mem = x_data.size * x_data.itemsize + y_data.size * y_data.itemsize\n",
    "        if approx_mem > MAX_RAM_USAGE:\n",
    "            print(f\"Warning: chunk {fname} size {approx_mem/1e9:.2f} GB > limit of {MAX_RAM_USAGE/1e9:.2f} GB\")\n",
    "\n",
    "        # Oversample minority class if requested\n",
    "        if oversample:\n",
    "            pos_idx = np.where(y_data == 1)[0]\n",
    "            neg_idx = np.where(y_data == 0)[0]\n",
    "            num_pos = len(pos_idx)\n",
    "            num_neg = len(neg_idx)\n",
    "            if num_pos > 0 and num_neg > 0:\n",
    "                if num_pos < num_neg:\n",
    "                    # replicate positives\n",
    "                    factor = num_neg // num_pos\n",
    "                    remainder = num_neg % num_pos\n",
    "                    replicated_idx = np.concatenate(\n",
    "                        [pos_idx]*factor + \n",
    "                        [np.random.choice(pos_idx, remainder, replace=False)] if remainder>0 else []\n",
    "                    )\n",
    "                    new_idx = np.concatenate([neg_idx, replicated_idx])\n",
    "                else:\n",
    "                    # replicate negatives\n",
    "                    factor = num_pos // num_neg\n",
    "                    remainder = num_pos % num_neg\n",
    "                    replicated_idx = np.concatenate(\n",
    "                        [neg_idx]*factor + \n",
    "                        [np.random.choice(neg_idx, remainder, replace=False)] if remainder>0 else []\n",
    "                    )\n",
    "                    new_idx = np.concatenate([pos_idx, replicated_idx])\n",
    "                \n",
    "                # shuffle new index set\n",
    "                np.random.shuffle(new_idx)\n",
    "                x_data = x_data[new_idx]\n",
    "                y_data = y_data[new_idx]\n",
    "\n",
    "        # Build local index array for mini-batch sampling\n",
    "        indices = np.arange(x_data.shape[0])\n",
    "        np.random.shuffle(indices)  # shuffle to avoid chunk-level order bias\n",
    "\n",
    "        start = 0\n",
    "        while start < len(indices):\n",
    "            end = min(start + batch_size, len(indices))\n",
    "            batch_idx = indices[start:end]\n",
    "            start = end\n",
    "\n",
    "            x_batch = x_data[batch_idx]\n",
    "            y_batch = y_data[batch_idx]\n",
    "            yield x_batch, y_batch\n",
    "\n",
    "        # Clean up this chunk from RAM\n",
    "        del x_data, y_data\n",
    "\n",
    "# =================================================================================\n",
    "# CELL [4]: Build tf.data Datasets\n",
    "# =================================================================================\n",
    "\n",
    "def make_dataset(file_list, samples_dir, batch_size, oversample, shuffle_files, name=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Wraps chunk_file_generator into a tf.data.Dataset.\n",
    "    We rely on from_generator(...), specifying output_types and output_shapes\n",
    "    for TensorFlow.\n",
    "    \"\"\"\n",
    "    # 1) Define a generator function with no args, capturing the local parameters\n",
    "    def gen():\n",
    "        return chunk_file_generator(\n",
    "            file_list    = file_list,\n",
    "            samples_dir  = samples_dir,\n",
    "            batch_size   = batch_size,\n",
    "            oversample   = oversample,\n",
    "            shuffle_files= shuffle_files\n",
    "        )\n",
    "\n",
    "    # 2) Peek into one sample to define shapes\n",
    "    #    We'll do a small hack: run the generator once\n",
    "    #    or we can just define None for flexible batch dimension.\n",
    "    #    We'll do shape (None, channels, time=3, 128, 128), label => (None,).\n",
    "    #    So we won't do strict shape checks at the dataset level, we allow variable batch size for the last batch.\n",
    "    output_types = (TF_DTYPE, TF_LABEL_DTYPE)\n",
    "    # We'll guess the shapes at runtime, but we do know \"channels\"=unknown, \"time\"=3, \"height\"=128, \"width\"=128\n",
    "    # We'll set them to (None, None, None, None, None) to avoid strict shape constraints.\n",
    "    output_shapes = (\n",
    "        tf.TensorShape([None, None, None, None, None]),  # x_batch\n",
    "        tf.TensorShape([None])                           # y_batch\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_types=output_types,\n",
    "        output_shapes=output_shapes\n",
    "    )\n",
    "\n",
    "    # Return a repeatable dataset if needed (the user can call dataset.repeat() or pass steps_per_epoch).\n",
    "    # We'll just return it as is for now.\n",
    "    return dataset\n",
    "\n",
    "# Create train/val/test datasets using the above function\n",
    "train_ds = make_dataset(\n",
    "    file_list     = train_files,\n",
    "    samples_dir   = SAMPLES_DIR,\n",
    "    batch_size    = BATCH_SIZE,\n",
    "    oversample    = OVERSAMPLE,\n",
    "    shuffle_files = True,\n",
    "    name=\"train\"\n",
    ")\n",
    "\n",
    "val_ds = make_dataset(\n",
    "    file_list     = val_files,\n",
    "    samples_dir   = SAMPLES_DIR,\n",
    "    batch_size    = BATCH_SIZE,\n",
    "    oversample    = False,  # typically no oversampling in validation\n",
    "    shuffle_files = False,\n",
    "    name=\"val\"\n",
    ")\n",
    "\n",
    "test_ds = make_dataset(\n",
    "    file_list     = test_files,\n",
    "    samples_dir   = SAMPLES_DIR,\n",
    "    batch_size    = BATCH_SIZE,\n",
    "    oversample    = False,  # no oversampling in test\n",
    "    shuffle_files = False,\n",
    "    name=\"test\"\n",
    ")\n",
    "\n",
    "print(\"tf.data Datasets created for train, val, and test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0e7208-669e-45ee-bf07-e5d34f9abf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-addons==0.20.0 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-addons==0.20.0\u001b[0m\u001b[31m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-addons==0.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e8c366-8d9a-49a9-8194-4a3852a9aee9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7784/3011584535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;31m# Detect channels from first train file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No training files in 'train' split.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0mfirst_train_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAMPLES_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_files' is not defined"
     ]
    }
   ],
   "source": [
    "# ===========================================================================================\n",
    "# CELL [5]: Enhanced ConvLSTM Training Demo (No TensorFlow Addons)\n",
    "#           - Custom Focal Loss Implementation\n",
    "#           - Gradient clipping\n",
    "#           - Debug printing of random slices for NaN analysis & label check\n",
    "#           - Stronger ConvLSTM architecture\n",
    "# ===========================================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Increase memory threshold if your chunk files are ~9.68 GB\n",
    "MAX_RAM_USAGE = 15_000_000_000  # ~15 GB; adjust if you have 450GB\n",
    "\n",
    "# 1) Custom Focal Loss Implementation\n",
    "def sigmoid_focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Custom focal loss for binary classification.\n",
    "      - y_true, y_pred are same shape [batch].\n",
    "      - 'gamma' focuses on hard, misclassified examples.\n",
    "      - 'alpha' is weighting for positive class.\n",
    "    \"\"\"\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)  # avoid log(0)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    # cross-entropy\n",
    "    ce = tf.keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "    # p_t: prob assigned to true class\n",
    "    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "    # alpha weighting\n",
    "    alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "    # modulating factor (1 - p_t)^gamma\n",
    "    modulating_factor = tf.pow((1.0 - p_t), gamma)\n",
    "\n",
    "    focal_loss = alpha_factor * modulating_factor * ce\n",
    "    return tf.reduce_mean(focal_loss)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2) Updated chunk_file_generator with debug prints & oversampling\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def debug_print_sample_slices(x_data, y_data, num_checks=3):\n",
    "    \"\"\"\n",
    "    Print random patch slices for NaN ratio & label.\n",
    "    x_data shape: [N, channels, time=3, 128, 128]\n",
    "    y_data shape: [N]\n",
    "    \"\"\"\n",
    "    if x_data.shape[0] == 0:\n",
    "        return\n",
    "    # We'll sample a few patch indices at random\n",
    "    indices = np.random.choice(x_data.shape[0], size=min(num_checks, x_data.shape[0]), replace=False)\n",
    "    for i in indices:\n",
    "        c = np.random.randint(0, x_data.shape[1])  # channels\n",
    "        t = np.random.randint(0, x_data.shape[2])  # time steps\n",
    "        patch_slice = x_data[i, c, t]  # shape: [128, 128]\n",
    "        nan_count = np.isnan(patch_slice).sum()\n",
    "        total_cells = 128 * 128\n",
    "        nan_pct = (nan_count / total_cells) * 100.0\n",
    "        label_val = y_data[i]\n",
    "        print(f\"  Debug patch idx={i}, channel={c}, time={t}, NaN%={nan_pct:.2f}, label={label_val}\")\n",
    "\n",
    "OVERSAMPLE = True\n",
    "\n",
    "def chunk_file_generator(file_list, samples_dir, batch_size, oversample=False, shuffle_files=True, debug_checks=2):\n",
    "    files = list(file_list)\n",
    "    if shuffle_files:\n",
    "        random.shuffle(files)\n",
    "\n",
    "    for fname in files:\n",
    "        fpath = os.path.join(samples_dir, fname)\n",
    "        if not os.path.exists(fpath):\n",
    "            raise FileNotFoundError(f\"Missing chunk file: {fpath}\")\n",
    "\n",
    "        with np.load(fpath, allow_pickle=True) as data:\n",
    "            x_data = data[\"x_data\"]\n",
    "            y_data = data[\"y_label\"]\n",
    "\n",
    "        approx_mem = x_data.size * x_data.itemsize + y_data.size * y_data.itemsize\n",
    "        if approx_mem > MAX_RAM_USAGE:\n",
    "            print(f\"Warning: chunk {fname} size {approx_mem/1e9:.2f} GB > limit of {MAX_RAM_USAGE/1e9:.2f} GB\")\n",
    "\n",
    "        # Print random debug slices\n",
    "        print(f\"\\n>>> Debug Info: file={fname}, shape={x_data.shape}\")\n",
    "        debug_print_sample_slices(x_data, y_data, num_checks=debug_checks)\n",
    "\n",
    "        # Oversample minority class if requested\n",
    "        if oversample:\n",
    "            pos_idx = np.where(y_data == 1)[0]\n",
    "            neg_idx = np.where(y_data == 0)[0]\n",
    "            num_pos = len(pos_idx)\n",
    "            num_neg = len(neg_idx)\n",
    "            if num_pos > 0 and num_neg > 0:\n",
    "                if num_pos < num_neg:\n",
    "                    factor = num_neg // num_pos\n",
    "                    remainder = num_neg % num_pos\n",
    "                    replicated_idx = np.concatenate(\n",
    "                        [pos_idx]*factor +\n",
    "                        ([np.random.choice(pos_idx, remainder, replace=False)] if remainder>0 else [])\n",
    "                    )\n",
    "                    new_idx = np.concatenate([neg_idx, replicated_idx])\n",
    "                else:\n",
    "                    factor = num_pos // num_neg\n",
    "                    remainder = num_pos % num_neg\n",
    "                    replicated_idx = np.concatenate(\n",
    "                        [neg_idx]*factor +\n",
    "                        ([np.random.choice(neg_idx, remainder, replace=False)] if remainder>0 else [])\n",
    "                    )\n",
    "                    new_idx = np.concatenate([pos_idx, replicated_idx])\n",
    "                np.random.shuffle(new_idx)\n",
    "                x_data = x_data[new_idx]\n",
    "                y_data = y_data[new_idx]\n",
    "\n",
    "        # Shuffle local indices\n",
    "        indices = np.arange(x_data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        start = 0\n",
    "        while start < len(indices):\n",
    "            end = min(start + batch_size, len(indices))\n",
    "            batch_idx = indices[start:end]\n",
    "            start = end\n",
    "\n",
    "            yield x_data[batch_idx], y_data[batch_idx]\n",
    "\n",
    "        del x_data, y_data\n",
    "\n",
    "def make_dataset(file_list, samples_dir, batch_size, oversample=True, shuffle_files=True):\n",
    "    def gen():\n",
    "        return chunk_file_generator(\n",
    "            file_list=file_list,\n",
    "            samples_dir=samples_dir,\n",
    "            batch_size=batch_size,\n",
    "            oversample=oversample,\n",
    "            shuffle_files=shuffle_files,\n",
    "            debug_checks=3\n",
    "        )\n",
    "    output_types = (tf.float32, tf.int32)\n",
    "    output_shapes = (\n",
    "        tf.TensorShape([None, None, 3, 128, 128]),  # x_batch\n",
    "        tf.TensorShape([None])                      # y_batch\n",
    "    )\n",
    "    ds = tf.data.Dataset.from_generator(gen, output_types=output_types, output_shapes=output_shapes)\n",
    "    return ds\n",
    "\n",
    "def build_stronger_convlstm(channels):\n",
    "    \"\"\"\n",
    "    Deeper ConvLSTM model with larger filters and BN for better capacity.\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=(channels, 3, 128, 128), name=\"Input_ConvLSTM\")\n",
    "    \n",
    "    x = layers.Permute((2, 3, 4, 1), name=\"Permute\")(inp)\n",
    "    # => (time=3, 128, 128, channels)\n",
    "\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=32, kernel_size=(3,3), padding=\"same\", activation=\"tanh\",\n",
    "        return_sequences=True, name=\"ConvLSTM_1\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"BN_1\")(x)\n",
    "\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=32, kernel_size=(3,3), padding=\"same\", activation=\"tanh\",\n",
    "        return_sequences=False, name=\"ConvLSTM_2\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"BN_2\")(x)\n",
    "\n",
    "    x = layers.Flatten(name=\"Flatten\")(x)\n",
    "    x = layers.Dense(128, activation=\"relu\", name=\"Dense_128\")(x)\n",
    "    x = layers.Dropout(0.3, name=\"Dropout\")(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\", name=\"Output\")(x)\n",
    "\n",
    "    model = models.Model(inp, out, name=\"Stronger_ConvLSTM\")\n",
    "    return model\n",
    "\n",
    "# Detect channels from first train file\n",
    "if len(train_files) == 0:\n",
    "    raise ValueError(\"No training files in 'train' split.\")\n",
    "first_train_file = os.path.join(SAMPLES_DIR, train_files[0])\n",
    "with np.load(first_train_file, allow_pickle=True) as data:\n",
    "    x_sample = data[\"x_data\"]\n",
    "detected_channels = x_sample.shape[1]\n",
    "print(f\"Detected channel dimension = {detected_channels}\")\n",
    "\n",
    "model = build_stronger_convlstm(detected_channels)\n",
    "\n",
    "# Use the custom focal loss\n",
    "def custom_focal_loss(y_true, y_pred):\n",
    "    return sigmoid_focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, clipnorm=5.0),\n",
    "    loss=custom_focal_loss,\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "EPOCHS = 2\n",
    "STEPS_PER_EPOCH = 50\n",
    "VAL_STEPS = 10\n",
    "TEST_STEPS = 10\n",
    "train_ds = make_dataset(train_files, SAMPLES_DIR, batch_size=64, oversample=True, shuffle_files=True)\n",
    "val_ds   = make_dataset(val_files,   SAMPLES_DIR, batch_size=64, oversample=False, shuffle_files=False)\n",
    "test_ds  = make_dataset(test_files,  SAMPLES_DIR, batch_size=64, oversample=False, shuffle_files=False)\n",
    "\n",
    "print(\"\\n>>> Starting demonstration training with custom focal loss + oversampling...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=VAL_STEPS\n",
    ")\n",
    "\n",
    "print(\"\\n>>> Evaluating on test dataset...\")\n",
    "test_result = model.evaluate(test_ds, steps=TEST_STEPS, return_dict=True)\n",
    "print(\"Test metrics:\", test_result)\n",
    "\n",
    "print(\"\\nAll done. Now using a custom focal loss (no tensorflow_addons)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e220c0-c4e7-4d84-8b26-8bdfe2194c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded split info from: splits/split_info.json\n",
      "  => Train files: 108\n",
      "  => Val   files: 12\n",
      "  => Test  files: 35\n",
      "[INFO] Detected channel dimension = 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747201837.201665    2481 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 94412 MB memory:  -> device: 0, name: NVIDIA GH200 480GB, pci bus id: 0000:dd:00.0, compute capability: 9.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Stronger_ConvLSTM\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Stronger_ConvLSTM\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_ConvLSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Permute (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ ConvLSTM_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">64,640</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ BN_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ ConvLSTM_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ BN_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">524288</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dense_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">67,108,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_ConvLSTM (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m,     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m128\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Permute (\u001b[38;5;33mPermute\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m24\u001b[0m)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ ConvLSTM_1 (\u001b[38;5;33mConvLSTM2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,    │        \u001b[38;5;34m64,640\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m32\u001b[0m)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ BN_1 (\u001b[38;5;33mBatchNormalization\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,    │           \u001b[38;5;34m128\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m32\u001b[0m)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ ConvLSTM_2 (\u001b[38;5;33mConvLSTM2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ BN_2 (\u001b[38;5;33mBatchNormalization\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m524288\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dense_128 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m67,108,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,247,873</span> (256.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,247,873\u001b[0m (256.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,247,745</span> (256.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,247,745\u001b[0m (256.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Starting demonstration training for 2 epochs ...\n",
      "Epoch 1/2\n",
      "\n",
      "[DEBUG] Loading samples_2017_09.npz, x_data.shape=(2051, 24, 3, 128, 128)\n",
      "   Debug patch idx=1124, channel=10, time=0, NaN%=0.00, label=0\n",
      "   Debug patch idx=188, channel=14, time=0, NaN%=0.00, label=0\n",
      "   Debug patch idx=1727, channel=11, time=0, NaN%=0.00, label=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1747201887.438677    4969 service.cc:148] XLA service 0xe531dc00de40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1747201887.438709    4969 service.cc:156]   StreamExecutor device (0): NVIDIA GH200 480GB, Compute Capability 9.0\n",
      "2025-05-14 05:51:27.541304: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1747201888.026067    4969 cuda_dnn.cc:529] Loaded cuDNN version 90701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/50\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35:17\u001b[0m 43s/step - accuracy: 0.5156 - loss: 0.2261"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747201891.123502    4969 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.6716 - loss: 1.7540\n",
      "[DEBUG] Loading samples_2014_08.npz, x_data.shape=(2051, 24, 3, 128, 128)\n",
      "   Debug patch idx=1300, channel=6, time=2, NaN%=0.00, label=1\n",
      "   Debug patch idx=1373, channel=14, time=2, NaN%=0.00, label=0\n",
      "   Debug patch idx=451, channel=6, time=1, NaN%=0.00, label=0\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853ms/step - accuracy: 0.6728 - loss: 1.7536\n",
      "[DEBUG] Loading samples_2021_01.npz, x_data.shape=(2051, 24, 3, 128, 128)\n",
      "   Debug patch idx=606, channel=18, time=0, NaN%=0.00, label=0\n",
      "   Debug patch idx=2033, channel=10, time=0, NaN%=0.00, label=0\n",
      "   Debug patch idx=208, channel=23, time=0, NaN%=0.00, label=0\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 2s/step - accuracy: 0.6734 - loss: 1.7535 - val_accuracy: 0.8625 - val_loss: 0.9729\n",
      "Epoch 2/2\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.7284 - loss: 1.8361 - val_accuracy: 0.8641 - val_loss: 0.9988\n",
      "\n",
      "[DEBUG] Loading samples_2018_11.npz, x_data.shape=(2051, 24, 3, 128, 128)\n",
      "   Debug patch idx=1515, channel=19, time=1, NaN%=0.00, label=0\n",
      "   Debug patch idx=1185, channel=21, time=0, NaN%=0.00, label=0\n",
      "   Debug patch idx=167, channel=7, time=1, NaN%=0.01, label=1\n",
      "\n",
      ">>> Evaluating on test dataset ...\n",
      "\n",
      "[DEBUG] Loading samples_2010_01.npz, x_data.shape=(2051, 24, 3, 128, 128)\n",
      "   Debug patch idx=99, channel=0, time=0, NaN%=0.00, label=1\n",
      "   Debug patch idx=1035, channel=11, time=0, NaN%=0.00, label=0\n",
      "   Debug patch idx=1626, channel=22, time=2, NaN%=0.00, label=0\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 56ms/step - accuracy: 0.8237 - loss: 1.1530\n",
      "Test metrics: {'accuracy': 0.817187488079071, 'loss': 1.1477205753326416}\n",
      "\n",
      "All done. This single cell handles:\n",
      " - Split loading, chunk generator, oversampling, debug prints.\n",
      " - Custom focal loss for imbalance, gradient clipping, and ConvLSTM.\n",
      " - NaN replacement to avoid nan in forward pass.\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================================\n",
    "# FINAL COMPLETE SCRIPT (All in One Cell)\n",
    "# ===========================================================================================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# [1] Configuration & Globals\n",
    "# ----------------------------------------------------------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "SPLITS_DIR   = \"splits\"          # Folder containing split_info.json\n",
    "SPLIT_FILE   = \"split_info.json\" # The split file name\n",
    "SAMPLES_DIR  = \"Stage1_Samples\"  # Where chunk .npz files (samples_YYYY_MM.npz) live\n",
    "\n",
    "MAX_RAM_USAGE = 15_000_000_000  # ~15 GB memory threshold\n",
    "BATCH_SIZE    = 64\n",
    "\n",
    "EPOCHS          = 2\n",
    "STEPS_PER_EPOCH = 50\n",
    "VAL_STEPS       = 10\n",
    "TEST_STEPS      = 10\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# [2] Load Splits\n",
    "# ----------------------------------------------------------------------------\n",
    "def load_splits(splits_dir=SPLITS_DIR, split_file=SPLIT_FILE):\n",
    "    \"\"\"Load split_info.json containing train/val/test chunk file lists.\"\"\"\n",
    "    path = os.path.join(splits_dir, split_file)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Split file not found: {path}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        info = json.load(f)\n",
    "    return info\n",
    "\n",
    "split_info = load_splits()\n",
    "train_files = split_info[\"splits\"][\"train\"]\n",
    "val_files   = split_info[\"splits\"][\"validation\"]\n",
    "test_files  = split_info[\"splits\"][\"test\"]\n",
    "\n",
    "print(\"[INFO] Loaded split info from:\", os.path.join(SPLITS_DIR, SPLIT_FILE))\n",
    "print(f\"  => Train files: {len(train_files)}\")\n",
    "print(f\"  => Val   files: {len(val_files)}\")\n",
    "print(f\"  => Test  files: {len(test_files)}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# [3] Generator with Oversampling, Debug Prints, Memory Check\n",
    "# ----------------------------------------------------------------------------\n",
    "def debug_print_sample_slices(x_data, y_data, num_checks=3):\n",
    "    \"\"\"Print random patch slices for NaN ratio & label.\"\"\"\n",
    "    if x_data.shape[0] == 0:\n",
    "        return\n",
    "    idx = np.random.choice(x_data.shape[0], size=min(num_checks, x_data.shape[0]), replace=False)\n",
    "    for i in idx:\n",
    "        c = np.random.randint(0, x_data.shape[1])  # random channel\n",
    "        t = np.random.randint(0, x_data.shape[2])  # random time step\n",
    "        patch_slice = x_data[i, c, t]  # shape: [128,128]\n",
    "        nan_count = np.isnan(patch_slice).sum()\n",
    "        total_cells = 128 * 128\n",
    "        nan_pct = (nan_count / total_cells) * 100.0\n",
    "        label_val = y_data[i]\n",
    "        print(f\"   Debug patch idx={i}, channel={c}, time={t}, NaN%={nan_pct:.2f}, label={label_val}\")\n",
    "\n",
    "def chunk_file_generator(file_list, samples_dir, batch_size, oversample=True, shuffle_files=True, debug_checks=3):\n",
    "    \"\"\"\n",
    "    Loads each chunk file in random order, prints debug slices, optionally oversamples\n",
    "    the minority class, and yields mini-batches (x_batch, y_batch).\n",
    "    \"\"\"\n",
    "    files = list(file_list)\n",
    "    if shuffle_files:\n",
    "        random.shuffle(files)\n",
    "\n",
    "    for fname in files:\n",
    "        fpath = os.path.join(samples_dir, fname)\n",
    "        if not os.path.exists(fpath):\n",
    "            raise FileNotFoundError(f\"Chunk file not found: {fpath}\")\n",
    "\n",
    "        # Load data from .npz\n",
    "        with np.load(fpath, allow_pickle=True) as data:\n",
    "            x_data = data[\"x_data\"]  # shape: [N, channels, time=3, 128,128]\n",
    "            y_data = data[\"y_label\"] # shape: [N]\n",
    "\n",
    "        # Approx memory check\n",
    "        approx_mem = x_data.size * x_data.itemsize + y_data.size * y_data.itemsize\n",
    "        if approx_mem > MAX_RAM_USAGE:\n",
    "            print(f\"[Warning] chunk {fname} size {approx_mem/1e9:.2f} GB > limit of {MAX_RAM_USAGE/1e9:.2f} GB\")\n",
    "\n",
    "        print(f\"\\n[DEBUG] Loading {fname}, x_data.shape={x_data.shape}\")\n",
    "        debug_print_sample_slices(x_data, y_data, num_checks=debug_checks)\n",
    "\n",
    "        # Replace NaNs with 0.0 to avoid nan in forward pass\n",
    "        x_data = np.nan_to_num(x_data, nan=0.0)\n",
    "\n",
    "        # Oversample minority class\n",
    "        if oversample:\n",
    "            pos_idx = np.where(y_data == 1)[0]\n",
    "            neg_idx = np.where(y_data == 0)[0]\n",
    "            num_pos = len(pos_idx)\n",
    "            num_neg = len(neg_idx)\n",
    "            if num_pos > 0 and num_neg > 0:\n",
    "                if num_pos < num_neg:\n",
    "                    factor = num_neg // num_pos\n",
    "                    remainder = num_neg % num_pos\n",
    "                    replicated_idx = np.concatenate(\n",
    "                        [pos_idx]*factor + \n",
    "                        ([np.random.choice(pos_idx, remainder, replace=False)] if remainder>0 else [])\n",
    "                    )\n",
    "                    new_idx = np.concatenate([neg_idx, replicated_idx])\n",
    "                else:\n",
    "                    factor = num_pos // num_neg\n",
    "                    remainder = num_pos % num_neg\n",
    "                    replicated_idx = np.concatenate(\n",
    "                        [neg_idx]*factor +\n",
    "                        ([np.random.choice(neg_idx, remainder, replace=False)] if remainder>0 else [])\n",
    "                    )\n",
    "                    new_idx = np.concatenate([pos_idx, replicated_idx])\n",
    "                np.random.shuffle(new_idx)\n",
    "                x_data = x_data[new_idx]\n",
    "                y_data = y_data[new_idx]\n",
    "\n",
    "        # Shuffle local indices for mini-batch iteration\n",
    "        indices = np.arange(x_data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        start = 0\n",
    "        while start < len(indices):\n",
    "            end = min(start + batch_size, len(indices))\n",
    "            batch_idx = indices[start:end]\n",
    "            start = end\n",
    "\n",
    "            yield x_data[batch_idx], y_data[batch_idx]\n",
    "\n",
    "        del x_data, y_data\n",
    "\n",
    "def make_dataset(file_list, samples_dir, batch_size, oversample=True, shuffle_files=True):\n",
    "    \"\"\"Wrap chunk_file_generator in a tf.data.Dataset.\"\"\"\n",
    "    def gen():\n",
    "        return chunk_file_generator(\n",
    "            file_list=file_list,\n",
    "            samples_dir=samples_dir,\n",
    "            batch_size=batch_size,\n",
    "            oversample=oversample,\n",
    "            shuffle_files=shuffle_files,\n",
    "            debug_checks=3\n",
    "        )\n",
    "    out_types = (tf.float32, tf.int32)\n",
    "    out_shapes= (\n",
    "        tf.TensorShape([None, None, 3, 128, 128]),\n",
    "        tf.TensorShape([None])\n",
    "    )\n",
    "    return tf.data.Dataset.from_generator(gen, output_types=out_types, output_shapes=out_shapes)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# [4] Stronger ConvLSTM + Custom Focal Loss\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_stronger_convlstm(channels):\n",
    "    \"\"\"\n",
    "    Deeper ConvLSTM model with more capacity:\n",
    "      - 2 stacked ConvLSTM layers w/ 32 filters\n",
    "      - BN, Flatten, Dense(128), Dropout\n",
    "      - Sigmoid output\n",
    "    Expects input shape: (channels, 3, 128, 128).\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=(channels, 3, 128, 128), name=\"Input_ConvLSTM\")\n",
    "\n",
    "    # Permute => (batch, time=3, height=128, width=128, channels)\n",
    "    x = layers.Permute((2, 3, 4, 1), name=\"Permute\")(inp)\n",
    "\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=32, kernel_size=(3, 3), padding=\"same\", \n",
    "        activation=\"tanh\", return_sequences=True, name=\"ConvLSTM_1\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"BN_1\")(x)\n",
    "\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=32, kernel_size=(3, 3), padding=\"same\", \n",
    "        activation=\"tanh\", return_sequences=False, name=\"ConvLSTM_2\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"BN_2\")(x)\n",
    "\n",
    "    # Flatten -> Dense -> Dropout -> Output\n",
    "    x = layers.Flatten(name=\"Flatten\")(x)\n",
    "    x = layers.Dense(128, activation=\"relu\", name=\"Dense_128\")(x)\n",
    "    x = layers.Dropout(0.3, name=\"Dropout\")(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\", name=\"Output\")(x)\n",
    "\n",
    "    return models.Model(inp, out, name=\"Stronger_ConvLSTM\")\n",
    "\n",
    "def custom_sigmoid_focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Custom focal loss for binary classification:\n",
    "      - Helps w/ class imbalance\n",
    "      - (gamma=2.0, alpha=0.25) typical values\n",
    "    \"\"\"\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    ce = tf.keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "    alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "    modulating_factor = tf.pow(1.0 - p_t, gamma)\n",
    "    return tf.reduce_mean(alpha_factor * modulating_factor * ce)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# [5] Build Model, Datasets & Train\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# 1) Detect channel dimension from first train file\n",
    "if len(train_files) == 0:\n",
    "    raise ValueError(\"No training files in 'train' split. Check your split_info.json.\")\n",
    "\n",
    "first_train_file = os.path.join(SAMPLES_DIR, train_files[0])\n",
    "if not os.path.exists(first_train_file):\n",
    "    raise FileNotFoundError(f\"Cannot find chunk file: {first_train_file}\")\n",
    "\n",
    "with np.load(first_train_file, allow_pickle=True) as data:\n",
    "    x_sample = data[\"x_data\"]\n",
    "detected_channels = x_sample.shape[1]\n",
    "print(f\"[INFO] Detected channel dimension = {detected_channels}\")\n",
    "\n",
    "# 2) Build the model\n",
    "model = build_stronger_convlstm(detected_channels)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-4, clipnorm=5.0),\n",
    "    loss=lambda yt, yp: custom_sigmoid_focal_loss(yt, yp, gamma=2.0, alpha=0.25),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# 3) Create tf.data Datasets for train, val, test\n",
    "train_ds = make_dataset(train_files, SAMPLES_DIR, BATCH_SIZE, oversample=True,  shuffle_files=True)\n",
    "val_ds   = make_dataset(val_files,   SAMPLES_DIR, BATCH_SIZE, oversample=False, shuffle_files=False)\n",
    "test_ds  = make_dataset(test_files,  SAMPLES_DIR, BATCH_SIZE, oversample=False, shuffle_files=False)\n",
    "\n",
    "# 4) Fit the model for a small demonstration\n",
    "print(f\"\\n>>> Starting demonstration training for {EPOCHS} epochs ...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=VAL_STEPS\n",
    ")\n",
    "\n",
    "# 5) Evaluate on test dataset\n",
    "print(\"\\n>>> Evaluating on test dataset ...\")\n",
    "test_metrics = model.evaluate(\n",
    "    test_ds,\n",
    "    steps=TEST_STEPS,\n",
    "    return_dict=True\n",
    ")\n",
    "print(\"Test metrics:\", test_metrics)\n",
    "\n",
    "print(\"\\nAll done. This single cell handles:\")\n",
    "print(\" - Split loading, chunk generator, oversampling, debug prints.\")\n",
    "print(\" - Custom focal loss for imbalance, gradient clipping, and ConvLSTM.\")\n",
    "print(\" - NaN replacement to avoid nan in forward pass.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45af1781-e9b4-4814-bafd-b5d748db1ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded split info from: splits/split_info.json\n",
      "  => Train files: 108\n",
      "  => Val   files: 12\n",
      "  => Test  files: 35\n",
      "[INFO] Detected channel dimension = 24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Stronger_ConvLSTM\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Stronger_ConvLSTM\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_ConvLSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Permute (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_lstm2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">64,640</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_lstm2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">524288</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">67,108,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_ConvLSTM (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m,     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m128\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Permute (\u001b[38;5;33mPermute\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m24\u001b[0m)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_lstm2d_2 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,    │        \u001b[38;5;34m64,640\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m32\u001b[0m)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,    │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │ \u001b[38;5;34m32\u001b[0m)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_lstm2d_3 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m524288\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m67,108,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,247,873</span> (256.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,247,873\u001b[0m (256.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,247,745</span> (256.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,247,745\u001b[0m (256.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Starting demonstration training for 5 epochs ...\n",
      "Epoch 1/5\n",
      "\n",
      "[DEBUG] Loading samples_2017_09.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=1124, ch=10, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=188, ch=14, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=1727, ch=11, t=0, NaN%=0.00, label=0\n",
      "\u001b[1m48/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.6716 - loss: 1.7540\n",
      "[DEBUG] Loading samples_2014_08.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=1300, ch=6, t=2, NaN%=0.00, label=1\n",
      "   Debug idx=1373, ch=14, t=2, NaN%=0.00, label=0\n",
      "   Debug idx=451, ch=6, t=1, NaN%=0.00, label=0\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841ms/step - accuracy: 0.6728 - loss: 1.7536\n",
      "[DEBUG] Loading samples_2021_01.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=606, ch=18, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=2033, ch=10, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=208, ch=23, t=0, NaN%=0.00, label=0\n",
      "\n",
      "[DEBUG] Loading samples_2021_01.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=1515, ch=19, t=1, NaN%=0.00, label=0\n",
      "   Debug idx=1185, ch=21, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=167, ch=7, t=1, NaN%=0.02, label=0\n",
      "\n",
      "Confusion Matrix (Val) After Epoch 1:\n",
      "[[538  63]\n",
      " [ 31   8]]\n",
      "\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 2s/step - accuracy: 0.6734 - loss: 1.7535 - val_accuracy: 0.8625 - val_loss: 0.9729\n",
      "Epoch 2/5\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.7283 - loss: 1.8333\n",
      "[DEBUG] Loading samples_2018_11.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=1552, ch=8, t=2, NaN%=0.00, label=1\n",
      "   Debug idx=2009, ch=2, t=1, NaN%=0.00, label=0\n",
      "   Debug idx=307, ch=17, t=2, NaN%=0.00, label=0\n",
      "\n",
      "[DEBUG] Loading samples_2021_01.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=884, ch=2, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=2003, ch=7, t=1, NaN%=0.00, label=0\n",
      "   Debug idx=471, ch=7, t=1, NaN%=0.00, label=0\n",
      "\n",
      "Confusion Matrix (Val) After Epoch 2:\n",
      "[[527  70]\n",
      " [ 29  14]]\n",
      "\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 859ms/step - accuracy: 0.7284 - loss: 1.8361 - val_accuracy: 0.8641 - val_loss: 0.9988\n",
      "Epoch 3/5\n",
      "\u001b[1m49/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.6194 - loss: 2.3965\n",
      "[DEBUG] Loading samples_2016_03.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=1995, ch=18, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=1398, ch=0, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=1431, ch=23, t=1, NaN%=0.00, label=0\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862ms/step - accuracy: 0.6198 - loss: 2.3911\n",
      "[DEBUG] Loading samples_2021_01.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=1893, ch=15, t=1, NaN%=0.00, label=0\n",
      "   Debug idx=308, ch=12, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=1410, ch=21, t=2, NaN%=0.00, label=0\n",
      "\n",
      "[DEBUG] Loading samples_2021_02.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=1287, ch=23, t=2, NaN%=0.00, label=0\n",
      "   Debug idx=1887, ch=2, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=744, ch=13, t=1, NaN%=0.00, label=0\n",
      "\n",
      "Confusion Matrix (Val) After Epoch 3:\n",
      "[[592  13]\n",
      " [ 34   1]]\n",
      "\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.6201 - loss: 2.3859 - val_accuracy: 0.9219 - val_loss: 0.4250\n",
      "Epoch 4/5\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.6756 - loss: 1.8278\n",
      "[DEBUG] Loading samples_2021_01.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=1765, ch=10, t=2, NaN%=18.75, label=0\n",
      "   Debug idx=13, ch=11, t=1, NaN%=0.29, label=0\n",
      "   Debug idx=190, ch=22, t=0, NaN%=0.00, label=0\n",
      "\n",
      "Confusion Matrix (Val) After Epoch 4:\n",
      "[[466 140]\n",
      " [ 21  13]]\n",
      "\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 851ms/step - accuracy: 0.6762 - loss: 1.8271 - val_accuracy: 0.7703 - val_loss: 2.3186\n",
      "Epoch 5/5\n",
      "\u001b[1m 5/50\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 742ms/step - accuracy: 0.7227 - loss: 1.8110\n",
      "[DEBUG] Loading samples_2015_06.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=939, ch=2, t=1, NaN%=0.00, label=0\n",
      "   Debug idx=902, ch=3, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=354, ch=5, t=2, NaN%=0.00, label=0\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858ms/step - accuracy: 0.7723 - loss: 1.5569 \n",
      "[DEBUG] Loading samples_2021_01.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=1229, ch=5, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=1041, ch=4, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=351, ch=20, t=2, NaN%=0.00, label=0\n",
      "\n",
      "Confusion Matrix (Val) After Epoch 5:\n",
      "[[586  20]\n",
      " [ 31   3]]\n",
      "\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 2s/step - accuracy: 0.7724 - loss: 1.5563 - val_accuracy: 0.8734 - val_loss: 0.6715\n",
      "\n",
      ">>> Evaluating on test dataset ...\n",
      "\n",
      "[DEBUG] Loading samples_2010_01.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=833, ch=17, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=626, ch=23, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=1311, ch=14, t=0, NaN%=0.00, label=0\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - accuracy: 0.8317 - loss: 1.2550\n",
      "Test metrics: {'accuracy': 0.828125, 'loss': 1.3076136112213135}\n",
      "\n",
      "[DEBUG] Loading samples_2010_01.npz, shape=(2051, 24, 3, 128, 128)\n",
      "   Debug idx=208, ch=4, t=0, NaN%=0.00, label=0\n",
      "   Debug idx=1713, ch=7, t=1, NaN%=0.00, label=0\n",
      "   Debug idx=503, ch=0, t=0, NaN%=0.00, label=0\n",
      "\n",
      "Final Test Confusion Matrix:\n",
      " [[532  54]\n",
      " [ 39  15]]\n",
      "\n",
      "All done. We now print a confusion matrix after every epoch on the validation set, and finally on the test set.\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================================\n",
    "# SINGLE-CELL NOTEBOOK SCRIPT\n",
    "# Includes:\n",
    "#   - Split loading\n",
    "#   - Generator with oversampling & debug\n",
    "#   - Strong ConvLSTM + Focal Loss\n",
    "#   - Validation confusion matrix after each epoch\n",
    "#   - Final test confusion matrix\n",
    "# ===========================================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1) Configuration\n",
    "# ----------------------------------------------------------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "SPLITS_DIR   = \"splits\"           # Folder containing split_info.json\n",
    "SPLIT_FILE   = \"split_info.json\"  # The split file name\n",
    "SAMPLES_DIR  = \"Stage1_Samples\"   # Folder with chunk .npz\n",
    "\n",
    "MAX_RAM_USAGE = 15_000_000_000    # ~15 GB memory threshold\n",
    "BATCH_SIZE    = 64\n",
    "\n",
    "EPOCHS          = 5   \n",
    "STEPS_PER_EPOCH = 50\n",
    "VAL_STEPS       = 10\n",
    "TEST_STEPS      = 10\n",
    "\n",
    "# How many steps to compute confusion matrix on validation set after each epoch\n",
    "VAL_CM_STEPS    = 10   # partial coverage\n",
    "TEST_CM_STEPS   = 10\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2) Load Splits\n",
    "# ----------------------------------------------------------------------------\n",
    "def load_splits(splits_dir=SPLITS_DIR, split_file=SPLIT_FILE):\n",
    "    path = os.path.join(splits_dir, split_file)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Split file not found: {path}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        info = json.load(f)\n",
    "    return info\n",
    "\n",
    "split_info = load_splits()\n",
    "train_files = split_info[\"splits\"][\"train\"]\n",
    "val_files   = split_info[\"splits\"][\"validation\"]\n",
    "test_files  = split_info[\"splits\"][\"test\"]\n",
    "\n",
    "print(\"[INFO] Loaded split info from:\", os.path.join(SPLITS_DIR, SPLIT_FILE))\n",
    "print(f\"  => Train files: {len(train_files)}\")\n",
    "print(f\"  => Val   files: {len(val_files)}\")\n",
    "print(f\"  => Test  files: {len(test_files)}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3) Generator with Oversampling, Debug Prints, Memory Check\n",
    "# ----------------------------------------------------------------------------\n",
    "def debug_print_sample_slices(x_data, y_data, num_checks=3):\n",
    "    \"\"\"Print random patch slices for NaN ratio & label.\"\"\"\n",
    "    if x_data.shape[0] == 0:\n",
    "        return\n",
    "    idx = np.random.choice(x_data.shape[0], size=min(num_checks, x_data.shape[0]), replace=False)\n",
    "    for i in idx:\n",
    "        c = np.random.randint(0, x_data.shape[1])  # random channel\n",
    "        t = np.random.randint(0, x_data.shape[2])  # random time step\n",
    "        patch_slice = x_data[i, c, t]  # [128,128]\n",
    "        nan_count = np.isnan(patch_slice).sum()\n",
    "        nan_pct = (nan_count / (128*128)) * 100.0\n",
    "        label_val = y_data[i]\n",
    "        print(f\"   Debug idx={i}, ch={c}, t={t}, NaN%={nan_pct:.2f}, label={label_val}\")\n",
    "\n",
    "def chunk_file_generator(file_list, samples_dir, batch_size,\n",
    "                         oversample=True, shuffle_files=True, debug_checks=3):\n",
    "    \"\"\"Loads chunk files in random order, prints debug, oversamples, yields mini-batches.\"\"\"\n",
    "    files = list(file_list)\n",
    "    if shuffle_files:\n",
    "        random.shuffle(files)\n",
    "\n",
    "    for fname in files:\n",
    "        fpath = os.path.join(samples_dir, fname)\n",
    "        if not os.path.exists(fpath):\n",
    "            raise FileNotFoundError(f\"Chunk file not found: {fpath}\")\n",
    "\n",
    "        with np.load(fpath, allow_pickle=True) as data:\n",
    "            x_data = data[\"x_data\"]  # [N, channels, time=3, 128,128]\n",
    "            y_data = data[\"y_label\"] # [N]\n",
    "\n",
    "        # Memory check\n",
    "        approx_mem = x_data.size * x_data.itemsize + y_data.size * y_data.itemsize\n",
    "        if approx_mem > MAX_RAM_USAGE:\n",
    "            print(f\"[Warning] chunk {fname} => ~{approx_mem/1e9:.2f} GB > limit {MAX_RAM_USAGE/1e9:.2f} GB\")\n",
    "\n",
    "        print(f\"\\n[DEBUG] Loading {fname}, shape={x_data.shape}\")\n",
    "        debug_print_sample_slices(x_data, y_data, debug_checks)\n",
    "\n",
    "        # Replace NaNs\n",
    "        x_data = np.nan_to_num(x_data, nan=0.0)\n",
    "\n",
    "        # Oversample minority class\n",
    "        if oversample:\n",
    "            pos_idx = np.where(y_data == 1)[0]\n",
    "            neg_idx = np.where(y_data == 0)[0]\n",
    "            if len(pos_idx) > 0 and len(neg_idx) > 0:\n",
    "                if len(pos_idx) < len(neg_idx):\n",
    "                    factor = len(neg_idx) // len(pos_idx)\n",
    "                    remainder = len(neg_idx) % len(pos_idx)\n",
    "                    replicated = np.concatenate(\n",
    "                        [pos_idx]*factor +\n",
    "                        ([np.random.choice(pos_idx, remainder, replace=False)] if remainder>0 else [])\n",
    "                    )\n",
    "                    new_idx = np.concatenate([neg_idx, replicated])\n",
    "                else:\n",
    "                    factor = len(pos_idx) // len(neg_idx)\n",
    "                    remainder = len(pos_idx) % len(neg_idx)\n",
    "                    replicated = np.concatenate(\n",
    "                        [neg_idx]*factor +\n",
    "                        ([np.random.choice(neg_idx, remainder, replace=False)] if remainder>0 else [])\n",
    "                    )\n",
    "                    new_idx = np.concatenate([pos_idx, replicated])\n",
    "                np.random.shuffle(new_idx)\n",
    "                x_data = x_data[new_idx]\n",
    "                y_data = y_data[new_idx]\n",
    "\n",
    "        # Shuffle for mini-batching\n",
    "        indices = np.arange(x_data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        start = 0\n",
    "        while start < len(indices):\n",
    "            end = min(start + batch_size, len(indices))\n",
    "            batch_idx = indices[start:end]\n",
    "            start = end\n",
    "\n",
    "            yield x_data[batch_idx], y_data[batch_idx]\n",
    "\n",
    "        del x_data, y_data\n",
    "\n",
    "def make_dataset(file_list, samples_dir, batch_size, oversample=True, shuffle_files=True):\n",
    "    def gen():\n",
    "        return chunk_file_generator(\n",
    "            file_list=file_list,\n",
    "            samples_dir=samples_dir,\n",
    "            batch_size=batch_size,\n",
    "            oversample=oversample,\n",
    "            shuffle_files=shuffle_files,\n",
    "            debug_checks=3\n",
    "        )\n",
    "    output_types = (tf.float32, tf.int32)\n",
    "    output_shapes = (\n",
    "        tf.TensorShape([None, None, 3, 128, 128]),\n",
    "        tf.TensorShape([None])\n",
    "    )\n",
    "    return tf.data.Dataset.from_generator(gen, output_types, output_shapes)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4) ConvLSTM + Focal Loss\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_stronger_convlstm(channels):\n",
    "    \"\"\"\n",
    "    A deeper ConvLSTM model with two ConvLSTM2D layers, BN, Flatten, Dense, Dropout\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=(channels, 3, 128, 128), name=\"Input_ConvLSTM\")\n",
    "    x = layers.Permute((2,3,4,1), name=\"Permute\")(inp)  # => (batch, time=3, h=128, w=128, channels)\n",
    "\n",
    "    x = layers.ConvLSTM2D(32, (3,3), padding=\"same\", activation=\"tanh\", return_sequences=True)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ConvLSTM2D(32, (3,3), padding=\"same\", activation=\"tanh\", return_sequences=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    return models.Model(inp, out, name=\"Stronger_ConvLSTM\")\n",
    "\n",
    "def custom_sigmoid_focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    \"\"\"Focal loss for binary classification, helps handle class imbalance.\"\"\"\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    ce = tf.keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "    alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "    modulating_factor = tf.pow(1.0 - p_t, gamma)\n",
    "    return tf.reduce_mean(alpha_factor * modulating_factor * ce)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5) Callback: Print confusion matrix on val set after each epoch\n",
    "# ----------------------------------------------------------------------------\n",
    "class ConfusionMatrixCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    After each epoch, runs through 'val_dataset' up to 'val_steps' batches,\n",
    "    collects predictions, prints confusion matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, val_dataset, val_steps, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.val_dataset = val_dataset\n",
    "        self.val_steps = val_steps\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        all_preds = []\n",
    "        all_labels= []\n",
    "        steps_count = 0\n",
    "        for x_batch, y_batch in self.val_dataset:\n",
    "            preds = self.model.predict(x_batch, verbose=0)\n",
    "            preds_bin = (preds > 0.5).astype(np.int32)\n",
    "\n",
    "            all_preds.append(preds_bin)\n",
    "            all_labels.append(y_batch)\n",
    "\n",
    "            steps_count += 1\n",
    "            if (self.val_steps is not None) and (steps_count >= self.val_steps):\n",
    "                break\n",
    "\n",
    "        if len(all_preds) > 0:\n",
    "            all_preds = np.concatenate(all_preds, axis=0).flatten()\n",
    "            all_labels= np.concatenate(all_labels, axis=0).flatten()\n",
    "            cm = tf.math.confusion_matrix(labels=all_labels, predictions=all_preds, num_classes=2)\n",
    "            print(f\"\\nConfusion Matrix (Val) After Epoch {epoch+1}:\\n{cm.numpy()}\\n\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 6) Build Model, Datasets & Train\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Detect channel dimension\n",
    "if len(train_files) == 0:\n",
    "    raise ValueError(\"No training files in 'train' split.\")\n",
    "first_train_file = os.path.join(SAMPLES_DIR, train_files[0])\n",
    "with np.load(first_train_file, allow_pickle=True) as data:\n",
    "    x_sample = data[\"x_data\"]\n",
    "detected_channels = x_sample.shape[1]\n",
    "print(f\"[INFO] Detected channel dimension = {detected_channels}\")\n",
    "\n",
    "# Build & compile model\n",
    "model = build_stronger_convlstm(detected_channels)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-4, clipnorm=5.0),\n",
    "    loss=lambda yt, yp: custom_sigmoid_focal_loss(yt, yp, gamma=2.0, alpha=0.25),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# Create train/val/test datasets\n",
    "train_ds = make_dataset(train_files, SAMPLES_DIR, BATCH_SIZE, oversample=True,  shuffle_files=True)\n",
    "val_ds   = make_dataset(val_files,   SAMPLES_DIR, BATCH_SIZE, oversample=False, shuffle_files=False)\n",
    "test_ds  = make_dataset(test_files,  SAMPLES_DIR, BATCH_SIZE, oversample=False, shuffle_files=False)\n",
    "\n",
    "# Confusion matrix callback for validation\n",
    "cm_callback = ConfusionMatrixCallback(val_dataset=val_ds, val_steps=VAL_CM_STEPS)\n",
    "\n",
    "# Train with confusion-matrix callback\n",
    "print(f\"\\n>>> Starting demonstration training for {EPOCHS} epochs ...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=VAL_STEPS,\n",
    "    callbacks=[cm_callback]\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n>>> Evaluating on test dataset ...\")\n",
    "test_result = model.evaluate(test_ds, steps=TEST_STEPS, return_dict=True)\n",
    "print(\"Test metrics:\", test_result)\n",
    "\n",
    "# Print confusion matrix on test set\n",
    "all_test_preds = []\n",
    "all_test_labels= []\n",
    "steps_count = 0\n",
    "for x_batch, y_batch in test_ds:\n",
    "    pred = model.predict(x_batch, verbose=0)\n",
    "    pred_bin = (pred > 0.5).astype(np.int32)\n",
    "\n",
    "    all_test_preds.append(pred_bin)\n",
    "    all_test_labels.append(y_batch)\n",
    "\n",
    "    steps_count += 1\n",
    "    if (TEST_CM_STEPS is not None) and (steps_count >= TEST_CM_STEPS):\n",
    "        break\n",
    "\n",
    "if len(all_test_preds) > 0:\n",
    "    all_test_preds = np.concatenate(all_test_preds).flatten()\n",
    "    all_test_labels= np.concatenate(all_test_labels).flatten()\n",
    "    test_cm = tf.math.confusion_matrix(labels=all_test_labels, predictions=all_test_preds, num_classes=2)\n",
    "    print(\"\\nFinal Test Confusion Matrix:\\n\", test_cm.numpy())\n",
    "\n",
    "print(\"\\nAll done. We now print a confusion matrix after every epoch on the validation set, and finally on the test set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b70e9d-9199-4fcf-8909-2cbeec52a9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded split info from: splits/split_info.json\n",
      "  => Train files: 108\n",
      "  => Val   files: 12\n",
      "  => Test  files: 35\n",
      "[INFO] Detected channels=24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CnnTransformerModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CnnTransformerModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ InputLayer          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ PermuteLayer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ InputLayer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TimeDistCnnLayer    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,221,168</span> │ PermuteLayer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ MHA                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,051,904</span> │ TimeDistCnnLayer… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ TimeDistCnnLayer… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ AttnAdd (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ TimeDistCnnLayer… │\n",
       "│                     │                   │            │ MHA[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ AttnLN              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ AttnAdd[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ FF1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ AttnLN[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ FF2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ FF1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ FFAdd (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ AttnLN[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                     │                   │            │ FF2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ FFLN                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ FFAdd[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TimeGAP             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ FFLN[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Dense128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ TimeGAP[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Dense128[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ Dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ InputLayer          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m3\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ PermuteLayer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ InputLayer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mPermute\u001b[0m)           │ \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m24\u001b[0m)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TimeDistCnnLayer    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │  \u001b[38;5;34m4,221,168\u001b[0m │ PermuteLayer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ MHA                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │  \u001b[38;5;34m1,051,904\u001b[0m │ TimeDistCnnLayer… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ TimeDistCnnLayer… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ AttnAdd (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ TimeDistCnnLayer… │\n",
       "│                     │                   │            │ MHA[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ AttnLN              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m512\u001b[0m │ AttnAdd[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ FF1 (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │     \u001b[38;5;34m65,792\u001b[0m │ AttnLN[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ FF2 (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │     \u001b[38;5;34m65,792\u001b[0m │ FF1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ FFAdd (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ AttnLN[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                     │                   │            │ FF2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ FFLN                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m512\u001b[0m │ FFAdd[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TimeGAP             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ FFLN[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Dense128 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ TimeGAP[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ Dense128[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ Dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,438,705</span> (20.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,438,705\u001b[0m (20.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,438,705</span> (20.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,438,705\u001b[0m (20.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Starting training for 5 epochs [CnnTransformerModel + Weighted Focal Loss] ...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'CNN+Transformer_TimeDistCnnLayer_TimeCnnBlock_conv2d_9_kernel_momentum' is not a valid scope name. A scope name has to match the following pattern: ^[A-Za-z0-9_.\\\\/>-]*$",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3259/2528077444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;31m# 5) Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n>>> Starting training for {EPOCHS} epochs [CnnTransformerModel + Weighted Focal Loss] ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    282\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'CNN+Transformer_TimeDistCnnLayer_TimeCnnBlock_conv2d_9_kernel_momentum' is not a valid scope name. A scope name has to match the following pattern: ^[A-Za-z0-9_.\\\\/>-]*$"
     ]
    }
   ],
   "source": [
    "# ============================================================================================\n",
    "# ADVANCED SPATIOTEMPORAL MODEL (TimeDistributed CNN then Transformer) + Weighted Focal Loss\n",
    "# NO PLUS SIGNS in layer/model names to fix scope name errors\n",
    "# ============================================================================================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# [1] Configuration\n",
    "# ----------------------------------------------------------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "SPLITS_DIR    = \"splits\"\n",
    "SPLIT_FILE    = \"split_info.json\"\n",
    "SAMPLES_DIR   = \"Stage1_Samples\"\n",
    "MAX_RAM_USAGE = 10_000_000_000  # ~10GB\n",
    "BATCH_SIZE    = 64\n",
    "\n",
    "EPOCHS          = 5\n",
    "STEPS_PER_EPOCH = 50\n",
    "VAL_STEPS       = 10\n",
    "TEST_STEPS      = 10\n",
    "\n",
    "# For confusion matrix partial coverage\n",
    "VAL_CM_STEPS  = 10\n",
    "TEST_CM_STEPS = 10\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# [2] Load Splits\n",
    "# ----------------------------------------------------------------------------\n",
    "def load_splits(splits_dir=SPLITS_DIR, split_file=SPLIT_FILE):\n",
    "    path = os.path.join(splits_dir, split_file)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Split file not found: {path}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        info = json.load(f)\n",
    "    return info\n",
    "\n",
    "split_info = load_splits()\n",
    "train_files = split_info[\"splits\"][\"train\"]\n",
    "val_files   = split_info[\"splits\"][\"validation\"]\n",
    "test_files  = split_info[\"splits\"][\"test\"]\n",
    "\n",
    "print(\"[INFO] Loaded split info from:\", os.path.join(SPLITS_DIR, SPLIT_FILE))\n",
    "print(f\"  => Train files: {len(train_files)}\")\n",
    "print(f\"  => Val   files: {len(val_files)}\")\n",
    "print(f\"  => Test  files: {len(test_files)}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# [3] Generator\n",
    "# ----------------------------------------------------------------------------\n",
    "def debug_print_sample_slices(x_data, y_data, num_checks=3):\n",
    "    \"\"\"Shows random patches for NaN% and label.\"\"\"\n",
    "    if x_data.shape[0] == 0:\n",
    "        return\n",
    "    chosen = np.random.choice(x_data.shape[0], size=min(num_checks, x_data.shape[0]), replace=False)\n",
    "    for i in chosen:\n",
    "        c = np.random.randint(0, x_data.shape[1])  \n",
    "        t = np.random.randint(0, x_data.shape[2])  \n",
    "        patch_slice = x_data[i, c, t]  \n",
    "        nan_count = np.isnan(patch_slice).sum()\n",
    "        nan_pct   = 100.0 * nan_count / (128*128)\n",
    "        print(f\"   Debug idx={i}, ch={c}, t={t}, NaN%={nan_pct:.2f}, label={y_data[i]}\")\n",
    "\n",
    "def chunk_file_generator(file_list, samples_dir, batch_size,\n",
    "                         oversample=True, shuffle_files=True, debug_checks=3):\n",
    "    \"\"\"\n",
    "    Loads chunk files, optionally oversamples the minority class,\n",
    "    debug prints random patches, yields (x_batch, y_batch).\n",
    "    \"\"\"\n",
    "    files = list(file_list)\n",
    "    if shuffle_files:\n",
    "        random.shuffle(files)\n",
    "\n",
    "    for fname in files:\n",
    "        fpath = os.path.join(samples_dir, fname)\n",
    "        if not os.path.exists(fpath):\n",
    "            raise FileNotFoundError(f\"Chunk file not found: {fpath}\")\n",
    "\n",
    "        with np.load(fpath, allow_pickle=True) as data:\n",
    "            x_data = data[\"x_data\"]  # shape [N, channels, time=3, 128,128]\n",
    "            y_data = data[\"y_label\"] # shape [N]\n",
    "\n",
    "        approx_mem = x_data.size*x_data.itemsize + y_data.size*y_data.itemsize\n",
    "        if approx_mem > MAX_RAM_USAGE:\n",
    "            print(f\"[WARNING] chunk {fname} ~{approx_mem/1e9:.2f} GB > limit {MAX_RAM_USAGE/1e9:.2f} GB\")\n",
    "\n",
    "        print(f\"\\n[DEBUG] Loading {fname}, shape={x_data.shape}\")\n",
    "        debug_print_sample_slices(x_data, y_data, debug_checks)\n",
    "\n",
    "        x_data = np.nan_to_num(x_data, nan=0.0)\n",
    "\n",
    "        if oversample:\n",
    "            pos_idx = np.where(y_data==1)[0]\n",
    "            neg_idx = np.where(y_data==0)[0]\n",
    "            if len(pos_idx)>0 and len(neg_idx)>0:\n",
    "                if len(pos_idx)<len(neg_idx):\n",
    "                    factor = len(neg_idx)//len(pos_idx)\n",
    "                    remainder = len(neg_idx)%len(pos_idx)\n",
    "                    replicated = np.concatenate(\n",
    "                        [pos_idx]*factor +\n",
    "                        ([np.random.choice(pos_idx, remainder, replace=False)] if remainder>0 else [])\n",
    "                    )\n",
    "                    new_idx = np.concatenate([neg_idx, replicated])\n",
    "                else:\n",
    "                    factor = len(pos_idx)//len(neg_idx)\n",
    "                    remainder = len(pos_idx)%len(neg_idx)\n",
    "                    replicated = np.concatenate(\n",
    "                        [neg_idx]*factor +\n",
    "                        ([np.random.choice(neg_idx, remainder, replace=False)] if remainder>0 else [])\n",
    "                    )\n",
    "                    new_idx = np.concatenate([pos_idx, replicated])\n",
    "                np.random.shuffle(new_idx)\n",
    "                x_data = x_data[new_idx]\n",
    "                y_data = y_data[new_idx]\n",
    "\n",
    "        idxs = np.arange(x_data.shape[0])\n",
    "        np.random.shuffle(idxs)\n",
    "        start=0\n",
    "        while start<len(idxs):\n",
    "            end = min(start+batch_size, len(idxs))\n",
    "            b_idx= idxs[start:end]\n",
    "            start= end\n",
    "            yield x_data[b_idx], y_data[b_idx]\n",
    "\n",
    "        del x_data, y_data\n",
    "\n",
    "def make_dataset(file_list, samples_dir, batch_size, oversample=True, shuffle_files=True):\n",
    "    def gen():\n",
    "        return chunk_file_generator(\n",
    "            file_list=file_list,\n",
    "            samples_dir=samples_dir,\n",
    "            batch_size=batch_size,\n",
    "            oversample=oversample,\n",
    "            shuffle_files=shuffle_files,\n",
    "            debug_checks=3\n",
    "        )\n",
    "    out_types = (tf.float32, tf.int32)\n",
    "    out_shapes= (\n",
    "        tf.TensorShape([None, None, 3, 128, 128]),\n",
    "        tf.TensorShape([None])\n",
    "    )\n",
    "    return tf.data.Dataset.from_generator(gen, out_types, out_shapes)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# [4] Weighted Focal Loss\n",
    "# ----------------------------------------------------------------------------\n",
    "def weighted_focal_loss(y_true, y_pred, gamma=2.0, alpha=0.75):\n",
    "    \"\"\"\n",
    "    Weighted Focal Loss: alpha=0.75 => heavily bias to minority class.\n",
    "    \"\"\"\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1-1e-7)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    ce = tf.keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "    p_t = y_true*y_pred + (1-y_true)*(1-y_pred)\n",
    "    alpha_factor = y_true*alpha + (1-y_true)*(1-alpha)\n",
    "    mod_factor   = tf.pow(1.0 - p_t, gamma)\n",
    "    return tf.reduce_mean(alpha_factor * mod_factor * ce)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# [5] TimeDistributed CNN + Transformer\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_transformer_model(channels, seq_len=3):\n",
    "    \"\"\"\n",
    "    1) Input => (channels, time=3, 128,128)\n",
    "    2) Permute => (batch, time=3, 128,128, channels)\n",
    "    3) TimeDistCnnBlock => embedding for each time\n",
    "    4) Transformer block => multi-head self-attention across time\n",
    "    5) Global average => Dense => Sigmoid\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=(channels, seq_len, 128,128), name=\"InputLayer\")\n",
    "\n",
    "    x = layers.Permute((2,3,4,1), name=\"PermuteLayer\")(inp)  \n",
    "    # => (batch, time=3, 128,128, channels)\n",
    "\n",
    "    # A small CNN block for each time step\n",
    "    def make_cnn_block():\n",
    "        block = models.Sequential(name=\"TimeCnnBlock\")\n",
    "        block.add(layers.Conv2D(16, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "        block.add(layers.MaxPooling2D((2,2)))  # => 64x64\n",
    "        block.add(layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "        block.add(layers.MaxPooling2D((2,2)))  # => 32x32\n",
    "        block.add(layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "        block.add(layers.MaxPooling2D((2,2)))  # => 16x16\n",
    "        block.add(layers.Flatten())\n",
    "        block.add(layers.Dense(256, activation=\"relu\"))\n",
    "        return block\n",
    "\n",
    "    time_cnn = make_cnn_block()\n",
    "    x = layers.TimeDistributed(time_cnn, name=\"TimeDistCnnLayer\")(x)\n",
    "    # => shape [batch, time=3, 256]\n",
    "\n",
    "    # Transformer\n",
    "    embed_dim = 256\n",
    "    num_heads = 4\n",
    "    ff_dim    = 256\n",
    "\n",
    "    attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name=\"MHA\")(x, x)\n",
    "    x = layers.Add(name=\"AttnAdd\")([x, attn_out])\n",
    "    x = layers.LayerNormalization(name=\"AttnLN\")(x)\n",
    "\n",
    "    ff = layers.Dense(ff_dim, activation=\"relu\", name=\"FF1\")(x)\n",
    "    ff = layers.Dense(embed_dim, name=\"FF2\")(ff)\n",
    "    x  = layers.Add(name=\"FFAdd\")([x, ff])\n",
    "    x  = layers.LayerNormalization(name=\"FFLN\")(x)\n",
    "\n",
    "    # Global average over time => [batch, embed_dim]\n",
    "    x = layers.GlobalAveragePooling1D(name=\"TimeGAP\")(x)\n",
    "\n",
    "    x = layers.Dense(128, activation=\"relu\", name=\"Dense128\")(x)\n",
    "    x = layers.Dropout(0.3, name=\"Dropout\")(x)\n",
    "    out= layers.Dense(1, activation=\"sigmoid\", name=\"Output\")(x)\n",
    "\n",
    "    return models.Model(inp, out, name=\"CnnTransformerModel\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# [6] ConfusionMatrixCallback\n",
    "# ----------------------------------------------------------------------------\n",
    "class ConfusionMatrixCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_dataset, val_steps):\n",
    "        super().__init__()\n",
    "        self.val_dataset= val_dataset\n",
    "        self.val_steps  = val_steps\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        all_preds, all_labels = [], []\n",
    "        step_count=0\n",
    "        for x_b, y_b in self.val_dataset:\n",
    "            preds = self.model.predict(x_b, verbose=0)\n",
    "            preds_bin = (preds>0.5).astype(np.int32)\n",
    "            all_preds.append(preds_bin)\n",
    "            all_labels.append(y_b)\n",
    "            step_count+=1\n",
    "            if self.val_steps is not None and step_count>=self.val_steps:\n",
    "                break\n",
    "        if len(all_preds)>0:\n",
    "            all_preds  = np.concatenate(all_preds).flatten()\n",
    "            all_labels = np.concatenate(all_labels).flatten()\n",
    "            cm = tf.math.confusion_matrix(all_labels, all_preds, num_classes=2)\n",
    "            print(f\"\\nConfusion Matrix (Val) After Epoch {epoch+1}:\\n{cm.numpy()}\\n\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# [7] Build, Train, Evaluate\n",
    "# ----------------------------------------------------------------------------\n",
    "if len(train_files)==0:\n",
    "    raise ValueError(\"No train files found in 'train' split_info\")\n",
    "\n",
    "# 1) Detect channels\n",
    "first_train_file = os.path.join(SAMPLES_DIR, train_files[0])\n",
    "with np.load(first_train_file, allow_pickle=True) as data:\n",
    "    x_sample = data[\"x_data\"]\n",
    "detected_channels= x_sample.shape[1]\n",
    "print(f\"[INFO] Detected channels={detected_channels}\")\n",
    "\n",
    "# 2) Build the model with safe naming\n",
    "model = build_transformer_model(detected_channels, seq_len=3)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-4, clipnorm=5.0),\n",
    "    loss=lambda y_t, y_p: weighted_focal_loss(y_t, y_p, gamma=2.0, alpha=0.75),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# 3) Create Datasets\n",
    "train_ds = make_dataset(train_files, SAMPLES_DIR, BATCH_SIZE, oversample=True,  shuffle_files=True)\n",
    "val_ds   = make_dataset(val_files,   SAMPLES_DIR, BATCH_SIZE, oversample=False, shuffle_files=False)\n",
    "test_ds  = make_dataset(test_files,  SAMPLES_DIR, BATCH_SIZE, oversample=False, shuffle_files=False)\n",
    "\n",
    "# 4) Callback for confusion matrix\n",
    "cm_callback = ConfusionMatrixCallback(val_dataset=val_ds, val_steps=VAL_CM_STEPS)\n",
    "\n",
    "# 5) Train\n",
    "print(f\"\\n>>> Starting training for {EPOCHS} epochs [CnnTransformerModel + Weighted Focal Loss] ...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=VAL_STEPS,\n",
    "    callbacks=[cm_callback]\n",
    ")\n",
    "\n",
    "# 6) Evaluate on test set\n",
    "print(\"\\n>>> Evaluating on test dataset ...\")\n",
    "test_result = model.evaluate(test_ds, steps=TEST_STEPS, return_dict=True)\n",
    "print(\"Test metrics:\", test_result)\n",
    "\n",
    "# 7) Final confusion matrix on test\n",
    "all_test_preds, all_test_labels = [], []\n",
    "steps_count=0\n",
    "for x_b, y_b in test_ds:\n",
    "    pred = model.predict(x_b, verbose=0)\n",
    "    pred_bin = (pred>0.5).astype(np.int32)\n",
    "    all_test_preds.append(pred_bin)\n",
    "    all_test_labels.append(y_b)\n",
    "    steps_count+=1\n",
    "    if (TEST_CM_STEPS is not None) and (steps_count>=TEST_CM_STEPS):\n",
    "        break\n",
    "\n",
    "if len(all_test_preds)>0:\n",
    "    all_test_preds  = np.concatenate(all_test_preds).flatten()\n",
    "    all_test_labels = np.concatenate(all_test_labels).flatten()\n",
    "    test_cm = tf.math.confusion_matrix(all_test_labels, all_test_preds, num_classes=2)\n",
    "    print(\"\\nFinal Test Confusion Matrix:\\n\", test_cm.numpy())\n",
    "\n",
    "print(\"\\nDone! All layer/model names are free of plus signs, so no scope name errors should arise.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164fc6c-64e3-483d-9a38-5728ccf64e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
