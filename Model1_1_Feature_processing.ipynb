{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97e0df8-7591-4ae0-9d85-ebb8ed387a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xarray in /home/ubuntu/.local/lib/python3.10/site-packages (2025.4.0)\n",
      "Requirement already satisfied: netcdf4 in /home/ubuntu/.local/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: dask in /home/ubuntu/.local/lib/python3.10/site-packages (2025.4.1)\n",
      "Requirement already satisfied: pandas>=2.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from xarray) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/ubuntu/.local/lib/python3.10/site-packages (from xarray) (2.2.5)\n",
      "Requirement already satisfied: packaging>=23.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from xarray) (25.0)\n",
      "Requirement already satisfied: cftime in /home/ubuntu/.local/lib/python3.10/site-packages (from netcdf4) (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from netcdf4) (2020.6.20)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask) (3.1.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.13.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask) (8.7.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /usr/lib/python3/dist-packages (from dask) (2024.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/lib/python3/dist-packages (from dask) (5.4.1)\n",
      "Requirement already satisfied: partd>=1.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask) (1.4.2)\n",
      "Requirement already satisfied: click>=8.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask) (8.2.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/ubuntu/.local/lib/python3.10/site-packages (from importlib_metadata>=4.13.0->dask) (3.21.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas>=2.1->xarray) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas>=2.1->xarray) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=2.1->xarray) (2022.1)\n",
      "Requirement already satisfied: locket in /home/ubuntu/.local/lib/python3.10/site-packages (from partd>=1.4.0->dask) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xarray netcdf4 dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b1d0cbd-024a-4f10-994c-d9893f9e3257",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy>=1.12.0 --upgrade -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77942e42-431b-4453-ac66-117f4ed4b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde12051-b09f-4926-9c9d-9aad4838119d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.2.5\n",
      "SciPy version: 1.15.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"SciPy version: {sp.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a78e718e-7cee-4465-bb5c-aae6f571c59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask[distributed] in /home/ubuntu/.local/lib/python3.10/site-packages (2025.4.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask[distributed]) (1.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/lib/python3/dist-packages (from dask[distributed]) (5.4.1)\n",
      "Requirement already satisfied: partd>=1.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask[distributed]) (1.4.2)\n",
      "Requirement already satisfied: click>=8.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask[distributed]) (8.2.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.13.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask[distributed]) (8.7.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /usr/lib/python3/dist-packages (from dask[distributed]) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask[distributed]) (25.0)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask[distributed]) (3.1.1)\n",
      "Requirement already satisfied: distributed==2025.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask[distributed]) (2025.4.1)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /usr/lib/python3/dist-packages (from distributed==2025.4.1->dask[distributed]) (1.26.5)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /usr/lib/python3/dist-packages (from distributed==2025.4.1->dask[distributed]) (3.0.3)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from distributed==2025.4.1->dask[distributed]) (6.4.2)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from distributed==2025.4.1->dask[distributed]) (2.4.0)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in /usr/lib/python3/dist-packages (from distributed==2025.4.1->dask[distributed]) (1.0.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/lib/python3/dist-packages (from distributed==2025.4.1->dask[distributed]) (5.9.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from distributed==2025.4.1->dask[distributed]) (3.1.0)\n",
      "Requirement already satisfied: locket>=1.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from distributed==2025.4.1->dask[distributed]) (1.0.0)\n",
      "Requirement already satisfied: zict>=3.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from distributed==2025.4.1->dask[distributed]) (3.0.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/ubuntu/.local/lib/python3.10/site-packages (from importlib_metadata>=4.13.0->dask[distributed]) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install \"dask[distributed]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79eebfad-c8da-4890-a16b-14c2efed0a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (5.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72d05b-15df-48c8-8f1e-4ee86f92549c",
   "metadata": {},
   "source": [
    "# Creating valid patches valid_patches_under30.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b40ac3d-a044-4c12-b1d6-27692efcc94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset lazily...\n",
      "Selecting band '2005_01_01'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5945/2992297778.py:16: UserWarning: The specified chunks separate the stored chunks along dimension \"lat\" starting at index 1000. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(DATA_FILE, chunks={\"lat\": 1000, \"lon\": 1000})\n",
      "/tmp/ipykernel_5945/2992297778.py:16: UserWarning: The specified chunks separate the stored chunks along dimension \"lon\" starting at index 1000. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(DATA_FILE, chunks={\"lat\": 1000, \"lon\": 1000})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: 6800 x 9000, total patches: 3710\n",
      "Analyzing patches for NaN coverage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch rows: 100%|██████████| 53/53 [00:00<00:00, 792.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid patches (NaN% <= 30.0): 2051 of 3710\n",
      "Finished in 33.23 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "DATA_FILE = \"../Switch2_Fuel_predictors/AWRA-L_e0_monthly_005_Aus.nc\"\n",
    "REFERENCE_MONTH = \"2005_01_01\"\n",
    "PATCH_SIZE = 128\n",
    "THRESHOLD = 30.0\n",
    "OUTPUT_FILE = \"valid_patches_under30.npz\"\n",
    "\n",
    "print(\"Opening dataset lazily...\")\n",
    "ds = xr.open_dataset(DATA_FILE, chunks={\"lat\": 1000, \"lon\": 1000})\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Selecting band '{REFERENCE_MONTH}'...\")\n",
    "data_slice = ds.sel(band=REFERENCE_MONTH)[\"e0\"].load()  # Loads one month's data\n",
    "ds.close()\n",
    "\n",
    "lat_dim, lon_dim = data_slice.shape\n",
    "num_lat_patches = lat_dim // PATCH_SIZE\n",
    "num_lon_patches = lon_dim // PATCH_SIZE\n",
    "total_patches = num_lat_patches * num_lon_patches\n",
    "print(f\"Data shape: {lat_dim} x {lon_dim}, total patches: {total_patches}\")\n",
    "\n",
    "patch_nan_percentages = np.zeros((num_lat_patches, num_lon_patches), dtype=np.float32)\n",
    "data_values = data_slice.values \n",
    "\n",
    "print(\"Analyzing patches for NaN coverage...\")\n",
    "for i in tqdm(range(num_lat_patches), desc=\"Patch rows\"):\n",
    "    for j in range(num_lon_patches):\n",
    "        lat_start, lon_start = i * PATCH_SIZE, j * PATCH_SIZE\n",
    "        patch_data = data_values[lat_start:lat_start + PATCH_SIZE,\n",
    "                                 lon_start:lon_start + PATCH_SIZE]\n",
    "        nan_count = np.isnan(patch_data).sum()\n",
    "        patch_nan_percentages[i, j] = (nan_count / (PATCH_SIZE**2)) * 100\n",
    "\n",
    "valid_mask = patch_nan_percentages <= THRESHOLD\n",
    "valid_indices = np.where(valid_mask)\n",
    "num_valid = len(valid_indices[0])\n",
    "\n",
    "print(f\"\\nValid patches (NaN% <= {THRESHOLD}): {num_valid} of {total_patches}\")\n",
    "\n",
    "\n",
    "lats, lons = data_slice.lat.values, data_slice.lon.values\n",
    "valid_patches = []\n",
    "for i, j in zip(*valid_indices):\n",
    "    lat_start, lon_start = i * PATCH_SIZE, j * PATCH_SIZE\n",
    "    lat_center_idx = lat_start + PATCH_SIZE // 2\n",
    "    lon_center_idx = lon_start + PATCH_SIZE // 2\n",
    "    lat_center_val = lats[lat_center_idx] if lat_center_idx < len(lats) else np.nan\n",
    "    lon_center_val = lons[lon_center_idx] if lon_center_idx < len(lons) else np.nan\n",
    "    valid_patches.append((\n",
    "        i, j,\n",
    "        lat_start, lat_start + PATCH_SIZE,\n",
    "        lon_start, lon_start + PATCH_SIZE,\n",
    "        lat_center_idx, lon_center_idx,\n",
    "        lat_center_val, lon_center_val,\n",
    "        patch_nan_percentages[i, j]\n",
    "    ))\n",
    "\n",
    "valid_patches = np.array(valid_patches, dtype=[\n",
    "    ('patch_i', np.int32),\n",
    "    ('patch_j', np.int32),\n",
    "    ('lat_start', np.int32),\n",
    "    ('lat_end', np.int32),\n",
    "    ('lon_start', np.int32),\n",
    "    ('lon_end', np.int32),\n",
    "    ('lat_center_idx', np.int32),\n",
    "    ('lon_center_idx', np.int32),\n",
    "    ('lat_center_val', np.float32),\n",
    "    ('lon_center_val', np.float32),\n",
    "    ('nan_percentage', np.float32)\n",
    "])\n",
    "\n",
    "np.savez_compressed(\n",
    "    OUTPUT_FILE,\n",
    "    valid_patches=valid_patches,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    lat_dim=lat_dim,\n",
    "    lon_dim=lon_dim,\n",
    "    threshold=THRESHOLD\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Finished in {elapsed:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86fc9d-e4dd-4729-8b7a-1d911fea48c4",
   "metadata": {},
   "source": [
    "# Feature : Switch 1 Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "249a3d20-9c68-40e6-b754-02059bfd04f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading valid patches metadata from 'valid_patches_under30.npz'...\n",
      "Found 2051 valid patches, patch_size=128, threshold=30.0%\n",
      "Domain: 6800 x 9000\n",
      "\n",
      "[1/11] Processing static feature: Aspect_degrees\n",
      "  -> Loading data for 'aspect_degrees' from /home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Aspect_nc_005_Aus.nc...\n",
      "  -> Extracting 2051 patches of size 128x128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved patches to 'static_patches/Aspect_degrees.npz'\n",
      "[2/11] Processing static feature: Distance_to_major_roads\n",
      "  -> Loading data for 'distance' from /home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Distance_to_major_roads_nc_Aus_005.nc...\n",
      "  -> Extracting 2051 patches of size 128x128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved patches to 'static_patches/Distance_to_major_roads.npz'\n",
      "[3/11] Processing static feature: Distance_to_railway_line\n",
      "  -> Loading data for 'distance' from /home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Distance_to_railway_line_nc_Aus_005.nc...\n",
      "  -> Extracting 2051 patches of size 128x128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved patches to 'static_patches/Distance_to_railway_line.npz'\n",
      "[4/11] Processing static feature: Distance_to_transmission_line\n",
      "  -> Loading data for 'distance' from /home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Distance_to_transmission_line_nc_Aus_005.nc...\n",
      "  -> Extracting 2051 patches of size 128x128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved patches to 'static_patches/Distance_to_transmission_line.npz'\n",
      "[5/11] Processing static feature: Elevation\n",
      "  -> Loading data for 'Elevation' from /home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Elevation_nc_005_Aus.nc...\n",
      "  -> Extracting 2051 patches of size 128x128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved patches to 'static_patches/Elevation.npz'\n",
      "[6/11] Processing static feature: Global_Landform\n",
      "  -> Loading data for 'Landform' from /home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Global_Landform_nc_Aus_005.nc...\n",
      "  -> Extracting 2051 patches of size 128x128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved patches to 'static_patches/Global_Landform.npz'\n",
      "[7/11] Processing static feature: Iwashi_Landform\n",
      "  -> Loading data for 'Landform' from /home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Iwashi_nc_Aus_005.nc...\n",
      "  -> Extracting 2051 patches of size 128x128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved patches to 'static_patches/Iwashi_Landform.npz'\n",
      "[8/11] Processing static feature: Population\n",
      "  -> Loading data for 'population' from /home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Population_nc_Aus_005.nc...\n",
      "  -> Extracting 2051 patches of size 128x128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved patches to 'static_patches/Population.npz'\n",
      "[9/11] Processing static feature: Slope\n",
      "  -> Loading data for 'slope' from /home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Slope_nc_Aus_005.nc...\n",
      "  -> Extracting 2051 patches of size 128x128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved patches to 'static_patches/Slope.npz'\n",
      "[10/11] Processing static feature: Vegetation_Type\n",
      "  -> Loading data for 'Vegetation_Type' from /home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Vegetation_nc_Aus_005.nc...\n",
      "  -> Extracting 2051 patches of size 128x128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved patches to 'static_patches/Vegetation_Type.npz'\n",
      "[11/11] Processing static feature: meybeck_Landform\n",
      "  -> Loading data for 'Landform' from /home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/meybeck_nc_Aus_005.nc...\n",
      "  -> Extracting 2051 patches of size 128x128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved patches to 'static_patches/meybeck_Landform.npz'\n",
      "\n",
      "All static features processed. Total time: 37.36s (0.62 min).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import gc\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# List of static features\n",
    "STATIC_FEATURES = [\n",
    "    {\n",
    "        \"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Aspect_nc_005_Aus.nc\",\n",
    "        \"var\": \"aspect_degrees\",\n",
    "        \"name\": \"Aspect_degrees\"\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Distance_to_major_roads_nc_Aus_005.nc\",\n",
    "        \"var\": \"distance\",\n",
    "        \"name\": \"Distance_to_major_roads\"\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Distance_to_railway_line_nc_Aus_005.nc\",\n",
    "        \"var\": \"distance\",\n",
    "        \"name\": \"Distance_to_railway_line\"\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Distance_to_transmission_line_nc_Aus_005.nc\",\n",
    "        \"var\": \"distance\",\n",
    "        \"name\": \"Distance_to_transmission_line\"\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Elevation_nc_005_Aus.nc\",\n",
    "        \"var\": \"Elevation\",\n",
    "        \"name\": \"Elevation\"\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Global_Landform_nc_Aus_005.nc\",\n",
    "        \"var\": \"Landform\",\n",
    "        \"name\": \"Global_Landform\"\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Iwashi_nc_Aus_005.nc\",\n",
    "        \"var\": \"Landform\",\n",
    "        \"name\": \"Iwashi_Landform\"\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Population_nc_Aus_005.nc\",\n",
    "        \"var\": \"population\",\n",
    "        \"name\": \"Population\"\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Slope_nc_Aus_005.nc\",\n",
    "        \"var\": \"slope\",\n",
    "        \"name\": \"Slope\"\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Vegetation_nc_Aus_005.nc\",\n",
    "        \"var\": \"Vegetation_Type\",\n",
    "        \"name\": \"Vegetation_Type\"\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/meybeck_nc_Aus_005.nc\",\n",
    "        \"var\": \"Landform\",\n",
    "        \"name\": \"meybeck_Landform\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load valid patches from your previously-created file \n",
    "    # (e.g. valid_patches_under30.npz or valid_patches_under70.npz)\n",
    "    valid_patches_file = \"valid_patches_under30.npz\"\n",
    "    if not os.path.exists(valid_patches_file):\n",
    "        raise FileNotFoundError(f\"Valid patches file '{valid_patches_file}' not found.\")\n",
    "\n",
    "    print(f\"Loading valid patches metadata from '{valid_patches_file}'...\")\n",
    "    data = np.load(valid_patches_file, allow_pickle=True)\n",
    "\n",
    "    # Convert structured array to list of dictionaries\n",
    "    valid_patches_struct = data[\"valid_patches\"]\n",
    "    valid_patches_list = []\n",
    "    for i in range(len(valid_patches_struct)):\n",
    "        patch_dict = {}\n",
    "        for field in valid_patches_struct.dtype.names:\n",
    "            patch_dict[field] = valid_patches_struct[field][i]\n",
    "        valid_patches_list.append(patch_dict)\n",
    "    \n",
    "    patch_size = int(data[\"patch_size\"])\n",
    "    lat_dim    = int(data[\"lat_dim\"])\n",
    "    lon_dim    = int(data[\"lon_dim\"])\n",
    "    threshold  = float(data[\"threshold\"])\n",
    "    \n",
    "    num_valid_patches = len(valid_patches_list)\n",
    "    print(f\"Found {num_valid_patches} valid patches, patch_size={patch_size}, threshold={threshold}%\")\n",
    "    print(f\"Domain: {lat_dim} x {lon_dim}\\n\")\n",
    "\n",
    "    # We'll save all static features under a single directory\n",
    "    out_root = \"static_patches\"\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "    \n",
    "    # Iterate over each static feature, slice out patches, and store as [patches, patch_size, patch_size].\n",
    "    for idx, feature_info in enumerate(STATIC_FEATURES, start=1):\n",
    "        feature_path = feature_info[\"path\"]\n",
    "        feature_var  = feature_info[\"var\"]\n",
    "        feature_name = feature_info[\"name\"]\n",
    "        \n",
    "        print(f\"[{idx}/{len(STATIC_FEATURES)}] Processing static feature: {feature_name}\")\n",
    "        \n",
    "        if not os.path.exists(feature_path):\n",
    "            print(f\"  -> File not found: {feature_path}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Open the dataset\n",
    "        ds = None\n",
    "        try:\n",
    "            ds = xr.open_dataset(feature_path)\n",
    "            \n",
    "            if feature_var not in ds.data_vars:\n",
    "                print(f\"  -> Variable '{feature_var}' not found in {feature_path}, skipping.\")\n",
    "                ds.close()\n",
    "                continue\n",
    "            \n",
    "            # The shape should be [lat, lon] with no time dimension\n",
    "            # Load the entire array into memory (assuming you have enough RAM)\n",
    "            print(f\"  -> Loading data for '{feature_var}' from {feature_path}...\")\n",
    "            data_2d = ds[feature_var].load().values  # shape: (lat_dim, lon_dim)\n",
    "            ds.close()\n",
    "            ds = None\n",
    "            \n",
    "            # Create an array [num_valid_patches, patch_size, patch_size]\n",
    "            feature_patches = np.zeros((num_valid_patches, patch_size, patch_size), dtype=np.float32)\n",
    "            feature_patches.fill(np.nan)\n",
    "            \n",
    "            print(f\"  -> Extracting {num_valid_patches} patches of size {patch_size}x{patch_size}...\")\n",
    "            for p_idx, patch_info in enumerate(tqdm(valid_patches_list, desc=\"Patch Extraction\", leave=False)):\n",
    "                lat_start = patch_info[\"lat_start\"]\n",
    "                lat_end   = patch_info[\"lat_end\"]\n",
    "                lon_start = patch_info[\"lon_start\"]\n",
    "                lon_end   = patch_info[\"lon_end\"]\n",
    "                \n",
    "                # Extract the patch, copy to avoid referencing original array\n",
    "                patch_array = data_2d[lat_start:lat_end, lon_start:lon_end].copy()\n",
    "                feature_patches[p_idx] = patch_array\n",
    "            \n",
    "            # Build metadata\n",
    "            metadata = {\n",
    "                \"feature_name\": feature_name,\n",
    "                \"feature_var\": feature_var,\n",
    "                \"patch_size\": patch_size,\n",
    "                \"num_patches\": num_valid_patches,\n",
    "                \"threshold_nan\": threshold\n",
    "            }\n",
    "            \n",
    "            # Save as .npz\n",
    "            out_file = os.path.join(out_root, f\"{feature_name}.npz\")\n",
    "            np.savez_compressed(\n",
    "                out_file,\n",
    "                data=feature_patches,\n",
    "                metadata=metadata\n",
    "            )\n",
    "            \n",
    "            print(f\"  -> Saved patches to '{out_file}'\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del data_2d\n",
    "            del feature_patches\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  -> Error processing {feature_name}: {e}\")\n",
    "            if ds:\n",
    "                ds.close()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"\\nAll static features processed. Total time: {elapsed:.2f}s ({elapsed/60:.2f} min).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d577472-7656-465f-8cdd-112f89dc83c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation of processed static feature data...\n",
      "Original metadata: 2051 valid patches, patch size: 128x128, threshold: 30.0%\n",
      "✓ Output directory 'static_patches' exists.\n",
      "✓ All 11 expected static feature files exist.\n",
      "Validating content of 11 static feature files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Files: 100%|██████████| 11/11 [00:05<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results: 10/11 files are valid.\n",
      "\n",
      "Issues found:\n",
      "  - File: static_patches/Elevation.npz, Issue: Contains unrealistic values: Elevation out of realistic range: min=-60.0, max=2172.0\n",
      "\n",
      "Performing detailed validation on a sample file...\n",
      "Detailed validation of static_patches/Aspect_degrees.npz:\n",
      "  - Total patches: 2051\n",
      "  - Valid patches (NaN% <= 30.0%): 1677\n",
      "  - Invalid patches: 374\n",
      "  - Highest NaN%: 100.00%\n",
      "  - Average NaN%: 18.26%\n",
      "\n",
      "NaN Percentage Distribution:\n",
      "   0-10 %: 1665 patches ( 81.2%) ████████████████████████████████████████\n",
      "  10-20 %:    0 patches (  0.0%) \n",
      "  20-30 %:   12 patches (  0.6%) \n",
      "  30-40 %:    0 patches (  0.0%) \n",
      "  40-50 %:    3 patches (  0.1%) \n",
      "  50-60 %:    0 patches (  0.0%) \n",
      "  60-70 %:    1 patches (  0.0%) \n",
      "  70-80 %:    0 patches (  0.0%) \n",
      "  80-90 %:    2 patches (  0.1%) \n",
      "  90-100%:    0 patches (  0.0%) \n",
      "\n",
      "Checking for inconsistencies across static features...\n",
      "Inconsistencies detected between static features:\n",
      "  - Different NaN patterns between Aspect_degrees.npz and Distance_to_major_roads.npz: 388/2051 patches (18.9%)\n",
      "  - Different NaN patterns between Aspect_degrees.npz and Distance_to_railway_line.npz: 388/2051 patches (18.9%)\n",
      "  - Different NaN patterns between Aspect_degrees.npz and Distance_to_transmission_line.npz: 388/2051 patches (18.9%)\n",
      "  - Different NaN patterns between Aspect_degrees.npz and Global_Landform.npz: 207/2051 patches (10.1%)\n",
      "  - Different NaN patterns between Aspect_degrees.npz and Iwashi_Landform.npz: 181/2051 patches (8.8%)\n",
      "  - Different NaN patterns between Aspect_degrees.npz and Population.npz: 388/2051 patches (18.9%)\n",
      "  - Different NaN patterns between Aspect_degrees.npz and Vegetation_Type.npz: 388/2051 patches (18.9%)\n",
      "  - Different NaN patterns between Aspect_degrees.npz and meybeck_Landform.npz: 158/2051 patches (7.7%)\n",
      "\n",
      "Validation completed in 10.60 seconds (0.18 minutes).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define the parameters for static features\n",
    "STATIC_FEATURES = [\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Aspect_nc_005_Aus.nc\", \"var\": \"aspect_degrees\", \"name\": \"Aspect_degrees\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Distance_to_major_roads_nc_Aus_005.nc\", \"var\": \"distance\", \"name\": \"Distance_to_major_roads\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Distance_to_railway_line_nc_Aus_005.nc\", \"var\": \"distance\", \"name\": \"Distance_to_railway_line\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Distance_to_transmission_line_nc_Aus_005.nc\", \"var\": \"distance\", \"name\": \"Distance_to_transmission_line\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Elevation_nc_005_Aus.nc\", \"var\": \"Elevation\", \"name\": \"Elevation\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Global_Landform_nc_Aus_005.nc\", \"var\": \"Landform\", \"name\": \"Global_Landform\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Iwashi_nc_Aus_005.nc\", \"var\": \"Landform\", \"name\": \"Iwashi_Landform\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Population_nc_Aus_005.nc\", \"var\": \"population\", \"name\": \"Population\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Slope_nc_Aus_005.nc\", \"var\": \"slope\", \"name\": \"Slope\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Vegetation_nc_Aus_005.nc\", \"var\": \"Vegetation_Type\", \"name\": \"Vegetation_Type\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/meybeck_nc_Aus_005.nc\", \"var\": \"Landform\", \"name\": \"meybeck_Landform\"}\n",
    "]\n",
    "\n",
    "OUT_ROOT = \"static_patches\"\n",
    "\n",
    "def validate_static_data_structure():\n",
    "    \"\"\"\n",
    "    Validates that all expected static feature files exist and have the correct structure.\n",
    "    \"\"\"\n",
    "    print(\"Starting validation of processed static feature data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load the original valid patches metadata for verification\n",
    "    valid_patches_data = np.load(\"valid_patches_under30.npz\", allow_pickle=True)\n",
    "    valid_patches = valid_patches_data[\"valid_patches\"]\n",
    "    expected_num_patches = len(valid_patches)\n",
    "    expected_patch_size = int(valid_patches_data[\"patch_size\"])\n",
    "    threshold = float(valid_patches_data[\"threshold\"])\n",
    "    \n",
    "    print(f\"Original metadata: {expected_num_patches} valid patches, \"\n",
    "          f\"patch size: {expected_patch_size}x{expected_patch_size}, threshold: {threshold}%\")\n",
    "    \n",
    "    # Check if output directory exists\n",
    "    if not os.path.exists(OUT_ROOT):\n",
    "        print(f\"WARNING: Output directory '{OUT_ROOT}' does not exist!\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"✓ Output directory '{OUT_ROOT}' exists.\")\n",
    "    \n",
    "    # Verify all static feature npz files exist\n",
    "    feature_names = [feature[\"name\"] for feature in STATIC_FEATURES]\n",
    "    expected_files = [os.path.join(OUT_ROOT, f\"{name}.npz\") for name in feature_names]\n",
    "    \n",
    "    missing_files = [f for f in expected_files if not os.path.exists(f)]\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"WARNING: {len(missing_files)}/{len(expected_files)} static feature files are missing!\")\n",
    "        for f in missing_files:\n",
    "            print(f\"  - {f}\")\n",
    "    else:\n",
    "        print(f\"✓ All {len(expected_files)} expected static feature files exist.\")\n",
    "    \n",
    "    # Validate content of files\n",
    "    existing_files = [f for f in expected_files if os.path.exists(f)]\n",
    "    print(f\"Validating content of {len(existing_files)} static feature files...\")\n",
    "    \n",
    "    validation_results = []\n",
    "    for file_path in tqdm(existing_files, desc=\"Validating Files\"):\n",
    "        result = validate_static_file_content(file_path, expected_num_patches, expected_patch_size, threshold)\n",
    "        validation_results.append(result)\n",
    "    \n",
    "    # Summarize validation results\n",
    "    success_count = sum(1 for r in validation_results if r[\"valid\"])\n",
    "    print(f\"\\nValidation Results: {success_count}/{len(validation_results)} files are valid.\")\n",
    "    \n",
    "    if success_count < len(validation_results):\n",
    "        print(\"\\nIssues found:\")\n",
    "        for r in validation_results:\n",
    "            if not r[\"valid\"]:\n",
    "                print(f\"  - File: {r['file']}, Issue: {r['issue']}\")\n",
    "    \n",
    "    # Perform a deeper validation on one file to check all patches\n",
    "    if existing_files:\n",
    "        print(\"\\nPerforming detailed validation on a sample file...\")\n",
    "        sample_file = existing_files[0]\n",
    "        detailed_result = validate_static_file_detailed(sample_file, valid_patches, threshold)\n",
    "        \n",
    "        print(f\"Detailed validation of {sample_file}:\")\n",
    "        print(f\"  - Total patches: {detailed_result['total_patches']}\")\n",
    "        print(f\"  - Valid patches (NaN% <= {threshold}%): {detailed_result['valid_patches']}\")\n",
    "        print(f\"  - Invalid patches: {detailed_result['invalid_patches']}\")\n",
    "        print(f\"  - Highest NaN%: {detailed_result['max_nan_pct']:.2f}%\")\n",
    "        print(f\"  - Average NaN%: {detailed_result['avg_nan_pct']:.2f}%\")\n",
    "        \n",
    "        # Print NaN percentage distribution in text form\n",
    "        if detailed_result['nan_percentages']:\n",
    "            nan_percentages = detailed_result['nan_percentages']\n",
    "            \n",
    "            # Define ranges for text-based distribution\n",
    "            ranges = [(0, 10), (10, 20), (20, 30), (30, 40), (40, 50), \n",
    "                      (50, 60), (60, 70), (70, 80), (80, 90), (90, 100)]\n",
    "            \n",
    "            print(\"\\nNaN Percentage Distribution:\")\n",
    "            for low, high in ranges:\n",
    "                count = sum(1 for p in nan_percentages if low <= p < high)\n",
    "                percentage = (count / len(nan_percentages)) * 100\n",
    "                bar_length = int(percentage / 2)  # Scale for display\n",
    "                bar = '█' * bar_length\n",
    "                print(f\"  {low:2d}-{high:<3d}%: {count:4d} patches ({percentage:5.1f}%) {bar}\")\n",
    "    \n",
    "    # Check for inconsistencies across static features\n",
    "    if len(existing_files) >= 2:\n",
    "        print(\"\\nChecking for inconsistencies across static features...\")\n",
    "        inconsistency_results = check_cross_feature_consistency(existing_files, threshold)\n",
    "        \n",
    "        if inconsistency_results['inconsistent_files']:\n",
    "            print(\"Inconsistencies detected between static features:\")\n",
    "            for issue in inconsistency_results['issues']:\n",
    "                print(f\"  - {issue}\")\n",
    "        else:\n",
    "            print(\"✓ No inconsistencies detected across static features.\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"\\nValidation completed in {elapsed:.2f} seconds ({elapsed/60:.2f} minutes).\")\n",
    "\n",
    "def validate_static_file_content(file_path, expected_num_patches, expected_patch_size, threshold):\n",
    "    \"\"\"\n",
    "    Validates the content of a single static feature file.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"file\": file_path,\n",
    "        \"valid\": True,\n",
    "        \"issue\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load the file\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        \n",
    "        # Check if required keys exist\n",
    "        required_keys = ['data', 'metadata']\n",
    "        missing_keys = [k for k in required_keys if k not in data]\n",
    "        if missing_keys:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Missing keys: {', '.join(missing_keys)}\"\n",
    "            return result\n",
    "        \n",
    "        # Check data shape - static features should be 3D [patches, height, width]\n",
    "        data_array = data['data']\n",
    "        if len(data_array.shape) != 3:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Expected 3D array, got {len(data_array.shape)}D\"\n",
    "            return result\n",
    "        \n",
    "        num_patches, patch_height, patch_width = data_array.shape\n",
    "        \n",
    "        if num_patches != expected_num_patches:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Expected {expected_num_patches} patches, got {num_patches}\"\n",
    "            return result\n",
    "        \n",
    "        if patch_height != expected_patch_size or patch_width != expected_patch_size:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Expected {expected_patch_size}x{expected_patch_size} patch size, got {patch_height}x{patch_width}\"\n",
    "            return result\n",
    "        \n",
    "        # Check metadata structure\n",
    "        metadata = data['metadata'].item() if isinstance(data['metadata'], np.ndarray) else data['metadata']\n",
    "        if not isinstance(metadata, dict):\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Metadata is not a dictionary\"\n",
    "            return result\n",
    "        \n",
    "        # Validate static feature metadata - check if required fields exist\n",
    "        required_metadata = [\"feature_name\", \"feature_var\", \"patch_size\", \"num_patches\", \"threshold_nan\"]\n",
    "        missing_metadata = [k for k in required_metadata if k not in metadata]\n",
    "        if missing_metadata:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Missing metadata fields: {', '.join(missing_metadata)}\"\n",
    "            return result\n",
    "        \n",
    "        # Check for unrealistic values based on feature type\n",
    "        feature_name = os.path.basename(file_path).replace('.npz', '')\n",
    "        unrealistic_values = check_unrealistic_static_values(data_array, feature_name)\n",
    "        if unrealistic_values:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Contains unrealistic values: {unrealistic_values}\"\n",
    "            return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        result[\"valid\"] = False\n",
    "        result[\"issue\"] = f\"Error loading/parsing file: {e}\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "def check_unrealistic_static_values(data_array, feature_name):\n",
    "    \"\"\"\n",
    "    Check for unrealistic values in static features based on the feature type.\n",
    "    Returns description of problems found or None if values are reasonable.\n",
    "    \"\"\"\n",
    "    # Flatten for easier analysis, ignoring NaNs\n",
    "    valid_data = data_array[~np.isnan(data_array)]\n",
    "    \n",
    "    if len(valid_data) == 0:\n",
    "        return \"Data contains only NaN values\"\n",
    "    \n",
    "    # Define reasonable ranges for different static features\n",
    "    if \"Aspect_degrees\" in feature_name:\n",
    "        # Aspect should be between 0 and 360 degrees\n",
    "        if np.min(valid_data) < 0 or np.max(valid_data) > 360:\n",
    "            return f\"Aspect out of range 0-360: min={np.min(valid_data):.1f}, max={np.max(valid_data):.1f}\"\n",
    "    \n",
    "    elif \"Distance\" in feature_name:\n",
    "        # Distance should be >= 0\n",
    "        if np.min(valid_data) < 0:\n",
    "            return f\"Negative distance values: min={np.min(valid_data):.1f}\"\n",
    "    \n",
    "    elif \"Elevation\" in feature_name:\n",
    "        # Elevation in Australia generally between -15m and 2228m (Mt Kosciuszko)\n",
    "        # Allow a bit more range for potential errors/outliers\n",
    "        if np.min(valid_data) < -20 or np.max(valid_data) > 2500:\n",
    "            return f\"Elevation out of realistic range: min={np.min(valid_data):.1f}, max={np.max(valid_data):.1f}\"\n",
    "    \n",
    "    elif \"Landform\" in feature_name:\n",
    "        # Landform is typically categorical data with integer values\n",
    "        if not np.all(np.equal(np.mod(valid_data, 1), 0)):\n",
    "            return \"Landform contains non-integer values\"\n",
    "    \n",
    "    elif \"Population\" in feature_name:\n",
    "        # Population should be >= 0\n",
    "        if np.min(valid_data) < 0:\n",
    "            return f\"Negative population values: min={np.min(valid_data):.1f}\"\n",
    "    \n",
    "    elif \"Slope\" in feature_name:\n",
    "        # Slope typically in degrees (0-90) or percent\n",
    "        if np.min(valid_data) < 0:\n",
    "            return f\"Negative slope values: min={np.min(valid_data):.1f}\"\n",
    "        # If in degrees, shouldn't exceed 90\n",
    "        if np.max(valid_data) <= 90:\n",
    "            pass\n",
    "        # If in percent, extremely steep slopes might go over 100% but rarely over 1000%\n",
    "        elif np.max(valid_data) > 1000:\n",
    "            return f\"Extremely high slope values: max={np.max(valid_data):.1f}\"\n",
    "    \n",
    "    elif \"Vegetation_Type\" in feature_name:\n",
    "        # Vegetation type is typically categorical data with integer values\n",
    "        if not np.all(np.equal(np.mod(valid_data, 1), 0)):\n",
    "            return \"Vegetation type contains non-integer values\"\n",
    "    \n",
    "    # Check for other anomalies\n",
    "    if np.all(valid_data == valid_data[0]):\n",
    "        return \"Data contains only a single repeated value\"\n",
    "    \n",
    "    # Check for NaN percentage\n",
    "    nan_percentage = np.isnan(data_array).sum() / data_array.size * 100\n",
    "    if nan_percentage > 50:\n",
    "        return f\"High percentage of NaN values: {nan_percentage:.1f}%\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "def validate_static_file_detailed(file_path, valid_patches, threshold):\n",
    "    \"\"\"\n",
    "    Performs a detailed validation of one static feature file, checking all patches.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'total_patches': 0,\n",
    "        'valid_patches': 0,\n",
    "        'invalid_patches': 0,\n",
    "        'max_nan_pct': 0,\n",
    "        'avg_nan_pct': 0,\n",
    "        'nan_percentages': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load the file\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        data_array = data['data']\n",
    "        \n",
    "        num_patches, patch_height, patch_width = data_array.shape\n",
    "        result['total_patches'] = num_patches\n",
    "        \n",
    "        # Check NaN percentages for each patch\n",
    "        nan_percentages = []\n",
    "        for patch_idx in range(num_patches):\n",
    "            patch = data_array[patch_idx]\n",
    "            nan_count = np.isnan(patch).sum()\n",
    "            total_cells = patch_height * patch_width\n",
    "            nan_percentage = (nan_count / total_cells) * 100\n",
    "            \n",
    "            nan_percentages.append(nan_percentage)\n",
    "            \n",
    "            if nan_percentage <= threshold:\n",
    "                result['valid_patches'] += 1\n",
    "            else:\n",
    "                result['invalid_patches'] += 1\n",
    "        \n",
    "        result['nan_percentages'] = nan_percentages\n",
    "        result['max_nan_pct'] = max(nan_percentages) if nan_percentages else 0\n",
    "        result['avg_nan_pct'] = sum(nan_percentages) / len(nan_percentages) if nan_percentages else 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in detailed validation: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def check_cross_feature_consistency(file_paths, threshold):\n",
    "    \"\"\"\n",
    "    Checks for inconsistencies across different static feature files.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'inconsistent_files': False,\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load metadata from all files\n",
    "        metadata_list = []\n",
    "        nan_patterns = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            data = np.load(file_path, allow_pickle=True)\n",
    "            \n",
    "            # Extract metadata\n",
    "            metadata = data['metadata'].item() if isinstance(data['metadata'], np.ndarray) else data['metadata']\n",
    "            metadata_list.append({\n",
    "                'file': file_path,\n",
    "                'name': metadata.get('feature_name', os.path.basename(file_path).replace('.npz', '')),\n",
    "                'num_patches': metadata.get('num_patches', None),\n",
    "                'patch_size': metadata.get('patch_size', None)\n",
    "            })\n",
    "            \n",
    "            # Extract NaN pattern (where data is NaN)\n",
    "            data_array = data['data']\n",
    "            nan_pattern = np.isnan(data_array)\n",
    "            \n",
    "            # For memory efficiency, we'll just remember the total NaN count for each patch\n",
    "            nan_counts = np.sum(nan_pattern, axis=(1, 2))  # Sum over height and width dimensions\n",
    "            nan_patterns.append({\n",
    "                'file': file_path,\n",
    "                'nan_counts': nan_counts\n",
    "            })\n",
    "        \n",
    "        # Check for metadata consistency\n",
    "        num_patches_values = set(m['num_patches'] for m in metadata_list if m['num_patches'] is not None)\n",
    "        if len(num_patches_values) > 1:\n",
    "            result['inconsistent_files'] = True\n",
    "            result['issues'].append(f\"Inconsistent number of patches across files: {num_patches_values}\")\n",
    "        \n",
    "        patch_size_values = set(m['patch_size'] for m in metadata_list if m['patch_size'] is not None)\n",
    "        if len(patch_size_values) > 1:\n",
    "            result['inconsistent_files'] = True\n",
    "            result['issues'].append(f\"Inconsistent patch sizes across files: {patch_size_values}\")\n",
    "        \n",
    "        # Check for NaN pattern consistency\n",
    "        # We expect all static features to have NaNs in the same locations (patches)\n",
    "        if len(nan_patterns) >= 2:\n",
    "            reference = nan_patterns[0]\n",
    "            ref_file = os.path.basename(reference['file'])\n",
    "            ref_counts = reference['nan_counts']\n",
    "            \n",
    "            for pattern in nan_patterns[1:]:\n",
    "                curr_file = os.path.basename(pattern['file'])\n",
    "                curr_counts = pattern['nan_counts']\n",
    "                \n",
    "                # Compare NaN counts for each patch\n",
    "                if not np.array_equal(ref_counts, curr_counts):\n",
    "                    # Find how many patches have different NaN counts\n",
    "                    diff_patches = np.sum(ref_counts != curr_counts)\n",
    "                    total_patches = len(ref_counts)\n",
    "                    diff_percentage = (diff_patches / total_patches) * 100\n",
    "                    \n",
    "                    if diff_percentage > 1:  # Allow small differences (< 1%)\n",
    "                        result['inconsistent_files'] = True\n",
    "                        result['issues'].append(\n",
    "                            f\"Different NaN patterns between {ref_file} and {curr_file}: \"\n",
    "                            f\"{diff_patches}/{total_patches} patches ({diff_percentage:.1f}%)\"\n",
    "                        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['inconsistent_files'] = True\n",
    "        result['issues'].append(f\"Error checking cross-feature consistency: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set a random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    validate_static_data_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57d4216c-2bb9-46c2-9907-dcb6617c5d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing NaN percentages in static feature files...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Static Features:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing feature: Aspect_degrees\n",
      "  Loading data for 'aspect_degrees'...\n",
      "  Data shape: (6800, 9000)\n",
      "  Total elements: 61,200,000\n",
      "  NaN count: 28,668,148\n",
      "  NaN percentage: 46.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Static Features:   9%|▉         | 1/11 [00:00<00:08,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Value range: [0.0, 359.892]\n",
      "  Number of unique values: 142,201\n",
      "\n",
      "Analyzing feature: Distance_to_major_roads\n",
      "  Loading data for 'distance'...\n",
      "  Data shape: (6800, 9000)\n",
      "  Total elements: 61,200,000\n",
      "  NaN count: 0\n",
      "  NaN percentage: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Static Features:  18%|█▊        | 2/11 [00:03<00:14,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Value range: [0.0, 1387.126801538223]\n",
      "  Number of unique values: 58,924,284\n",
      "\n",
      "Analyzing feature: Distance_to_railway_line\n",
      "  Loading data for 'distance'...\n",
      "  Data shape: (6800, 9000)\n",
      "  Total elements: 61,200,000\n",
      "  NaN count: 0\n",
      "  NaN percentage: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Static Features:  27%|██▋       | 3/11 [00:05<00:14,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Value range: [0.0, 1385.1285348620188]\n",
      "  Number of unique values: 60,469,973\n",
      "\n",
      "Analyzing feature: Distance_to_transmission_line\n",
      "  Loading data for 'distance'...\n",
      "  Data shape: (6800, 9000)\n",
      "  Total elements: 61,200,000\n",
      "  NaN count: 0\n",
      "  NaN percentage: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Static Features:  36%|███▋      | 4/11 [00:07<00:13,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Value range: [0.0, 1385.038332245596]\n",
      "  Number of unique values: 60,422,301\n",
      "\n",
      "Analyzing feature: Elevation\n",
      "  Loading data for 'Elevation'...\n",
      "  Data shape: (6800, 9000)\n",
      "  Total elements: 61,200,000\n",
      "  NaN count: 28,632,801\n",
      "  NaN percentage: 46.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Static Features:  45%|████▌     | 5/11 [00:08<00:09,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Value range: [-60.0, 2172.0]\n",
      "  Number of unique values: 2,108\n",
      "\n",
      "Analyzing feature: Global_Landform\n",
      "  Loading data for 'Landform'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Static Features:  55%|█████▍    | 6/11 [00:08<00:05,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Data shape: (6800, 9000)\n",
      "  Total elements: 61,200,000\n",
      "  NaN count: 33,438,202\n",
      "  NaN percentage: 54.64%\n",
      "  Value range: [101.0, 242.0]\n",
      "  Number of unique values: 9\n",
      "\n",
      "Analyzing feature: Iwashi_Landform\n",
      "  Loading data for 'Landform'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Static Features:  64%|██████▎   | 7/11 [00:08<00:03,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Data shape: (6800, 9000)\n",
      "  Total elements: 61,200,000\n",
      "  NaN count: 33,527,006\n",
      "  NaN percentage: 54.78%\n",
      "  Value range: [1.0, 16.0]\n",
      "  Number of unique values: 16\n",
      "\n",
      "Analyzing feature: Population\n",
      "  Loading data for 'population'...\n",
      "  Data shape: (6800, 9000)\n",
      "  Total elements: 61,200,000\n",
      "  NaN count: 0\n",
      "  NaN percentage: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Static Features:  73%|███████▎  | 8/11 [00:09<00:02,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Value range: [0.0, 32560.697265625]\n",
      "  Number of unique values: 80,225\n",
      "\n",
      "Analyzing feature: Slope\n",
      "  Loading data for 'slope'...\n",
      "  Data shape: (6800, 9000)\n",
      "  Total elements: 61,200,000\n",
      "  NaN count: 28,668,148\n",
      "  NaN percentage: 46.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Static Features:  82%|████████▏ | 9/11 [00:10<00:01,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Value range: [0.0, 89.83]\n",
      "  Number of unique values: 618\n",
      "\n",
      "Analyzing feature: Vegetation_Type\n",
      "  Loading data for 'Vegetation_Type'...\n",
      "  Data shape: (6800, 9000)\n",
      "  Total elements: 61,200,000\n",
      "  NaN count: 0\n",
      "  NaN percentage: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Static Features:  91%|█████████ | 10/11 [00:10<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Value range: [0, 99]\n",
      "  Number of unique values: 33\n",
      "\n",
      "Analyzing feature: meybeck_Landform\n",
      "  Loading data for 'Landform'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Static Features: 100%|██████████| 11/11 [00:11<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Data shape: (6800, 9000)\n",
      "  Total elements: 61,200,000\n",
      "  NaN count: 33,496,632\n",
      "  NaN percentage: 54.73%\n",
      "  Value range: [1.0, 14.0]\n",
      "  Number of unique values: 13\n",
      "\n",
      "================================================================================\n",
      "Feature Name                   Shape           NaN %      Value Range          Unique Values  \n",
      "--------------------------------------------------------------------------------\n",
      "Aspect_degrees                 6800x9000       46.84%     [0.0, 359.892]       142,201        \n",
      "Distance_to_major_roads        6800x9000       0.00%      [0.0, 1387.126801538223] 58,924,284     \n",
      "Distance_to_railway_line       6800x9000       0.00%      [0.0, 1385.1285348620188] 60,469,973     \n",
      "Distance_to_transmission_line  6800x9000       0.00%      [0.0, 1385.038332245596] 60,422,301     \n",
      "Elevation                      6800x9000       46.79%     [-60.0, 2172.0]      2,108          \n",
      "Global_Landform                6800x9000       54.64%     [101.0, 242.0]       9              \n",
      "Iwashi_Landform                6800x9000       54.78%     [1.0, 16.0]          16             \n",
      "Population                     6800x9000       0.00%      [0.0, 32560.697265625] 80,225         \n",
      "Slope                          6800x9000       46.84%     [0.0, 89.83]         618            \n",
      "Vegetation_Type                6800x9000       0.00%      [0, 99]              33             \n",
      "meybeck_Landform               6800x9000       54.73%     [1.0, 14.0]          13             \n",
      "================================================================================\n",
      "\n",
      "Overall Statistics:\n",
      "  Average NaN percentage across all features: 27.69%\n",
      "  Maximum NaN percentage: 54.78%\n",
      "  Minimum NaN percentage: 0.00%\n",
      "  Feature with highest NaN percentage: Iwashi_Landform (54.78%)\n",
      "  Feature with lowest NaN percentage: Distance_to_major_roads (0.00%)\n",
      "\n",
      "Checking for suspicious NaN patterns...\n",
      "  WARNING: 3 features have more than 50% NaN values:\n",
      "    - Global_Landform\n",
      "    - Iwashi_Landform\n",
      "    - meybeck_Landform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define the static features to analyze\n",
    "STATIC_FEATURES = [\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Aspect_nc_005_Aus.nc\", \"var\": \"aspect_degrees\", \"name\": \"Aspect_degrees\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Distance_to_major_roads_nc_Aus_005.nc\", \"var\": \"distance\", \"name\": \"Distance_to_major_roads\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Distance_to_railway_line_nc_Aus_005.nc\", \"var\": \"distance\", \"name\": \"Distance_to_railway_line\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Distance_to_transmission_line_nc_Aus_005.nc\", \"var\": \"distance\", \"name\": \"Distance_to_transmission_line\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Elevation_nc_005_Aus.nc\", \"var\": \"Elevation\", \"name\": \"Elevation\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Global_Landform_nc_Aus_005.nc\", \"var\": \"Landform\", \"name\": \"Global_Landform\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Iwashi_nc_Aus_005.nc\", \"var\": \"Landform\", \"name\": \"Iwashi_Landform\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Population_nc_Aus_005.nc\", \"var\": \"population\", \"name\": \"Population\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Slope_nc_Aus_005.nc\", \"var\": \"slope\", \"name\": \"Slope\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/Vegetation_nc_Aus_005.nc\", \"var\": \"Vegetation_Type\", \"name\": \"Vegetation_Type\"},\n",
    "    {\"path\": \"/home/ubuntu/Data-Seasonal-forecast/Switch1_Static_predictors/meybeck_nc_Aus_005.nc\", \"var\": \"Landform\", \"name\": \"meybeck_Landform\"}\n",
    "]\n",
    "\n",
    "def analyze_nan_percentages():\n",
    "    \"\"\"\n",
    "    Analyzes and prints the percentage of NaN values in each static feature file.\n",
    "    \"\"\"\n",
    "    print(\"Analyzing NaN percentages in static feature files...\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for feature in tqdm(STATIC_FEATURES, desc=\"Processing Static Features\"):\n",
    "        feature_path = feature[\"path\"]\n",
    "        feature_var = feature[\"var\"]\n",
    "        feature_name = feature[\"name\"]\n",
    "        \n",
    "        print(f\"\\nAnalyzing feature: {feature_name}\")\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(feature_path):\n",
    "            print(f\"  ERROR: File not found: {feature_path}\")\n",
    "            results.append({\n",
    "                \"name\": feature_name, \n",
    "                \"nan_pct\": None, \n",
    "                \"error\": \"File not found\"\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Open dataset\n",
    "            ds = xr.open_dataset(feature_path)\n",
    "            \n",
    "            # Check if variable exists\n",
    "            if feature_var not in ds.data_vars:\n",
    "                print(f\"  ERROR: Variable '{feature_var}' not found in dataset\")\n",
    "                ds.close()\n",
    "                results.append({\n",
    "                    \"name\": feature_name, \n",
    "                    \"nan_pct\": None, \n",
    "                    \"error\": f\"Variable '{feature_var}' not found\"\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Load the data\n",
    "            print(f\"  Loading data for '{feature_var}'...\")\n",
    "            data = ds[feature_var].load().values\n",
    "            \n",
    "            # Calculate NaN percentage\n",
    "            total_elements = data.size\n",
    "            nan_count = np.sum(np.isnan(data))\n",
    "            nan_percentage = (nan_count / total_elements) * 100\n",
    "            \n",
    "            print(f\"  Data shape: {data.shape}\")\n",
    "            print(f\"  Total elements: {total_elements:,}\")\n",
    "            print(f\"  NaN count: {nan_count:,}\")\n",
    "            print(f\"  NaN percentage: {nan_percentage:.2f}%\")\n",
    "            \n",
    "            # Add to results\n",
    "            results.append({\n",
    "                \"name\": feature_name,\n",
    "                \"shape\": data.shape,\n",
    "                \"total_elements\": total_elements,\n",
    "                \"nan_count\": nan_count,\n",
    "                \"nan_pct\": nan_percentage,\n",
    "                \"error\": None\n",
    "            })\n",
    "            \n",
    "            # Close the dataset\n",
    "            ds.close()\n",
    "            \n",
    "            # Get min/max values (excluding NaNs)\n",
    "            valid_data = data[~np.isnan(data)]\n",
    "            if len(valid_data) > 0:\n",
    "                min_val = np.min(valid_data)\n",
    "                max_val = np.max(valid_data)\n",
    "                unique_vals = len(np.unique(valid_data))\n",
    "                print(f\"  Value range: [{min_val}, {max_val}]\")\n",
    "                print(f\"  Number of unique values: {unique_vals:,}\")\n",
    "                \n",
    "                # Store additional statistics\n",
    "                results[-1][\"min_val\"] = min_val\n",
    "                results[-1][\"max_val\"] = max_val\n",
    "                results[-1][\"unique_vals\"] = unique_vals\n",
    "            else:\n",
    "                print(\"  WARNING: No valid data (all values are NaN)\")\n",
    "                \n",
    "            # Clean up\n",
    "            del data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: Failed to process file: {e}\")\n",
    "            results.append({\n",
    "                \"name\": feature_name, \n",
    "                \"nan_pct\": None, \n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{'Feature Name':<30} {'Shape':<15} {'NaN %':<10} {'Value Range':<20} {'Unique Values':<15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for result in results:\n",
    "        name = result[\"name\"]\n",
    "        \n",
    "        if result[\"error\"] is not None:\n",
    "            print(f\"{name:<30} ERROR: {result['error']}\")\n",
    "        else:\n",
    "            shape_str = \"x\".join(map(str, result[\"shape\"]))\n",
    "            nan_pct = f\"{result['nan_pct']:.2f}%\" if result[\"nan_pct\"] is not None else \"N/A\"\n",
    "            value_range = f\"[{result.get('min_val', 'N/A')}, {result.get('max_val', 'N/A')}]\"\n",
    "            unique_vals = f\"{result.get('unique_vals', 'N/A'):,}\" if \"unique_vals\" in result else \"N/A\"\n",
    "            \n",
    "            print(f\"{name:<30} {shape_str:<15} {nan_pct:<10} {value_range:<20} {unique_vals:<15}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    valid_results = [r for r in results if r[\"nan_pct\"] is not None]\n",
    "    if valid_results:\n",
    "        avg_nan_pct = sum(r[\"nan_pct\"] for r in valid_results) / len(valid_results)\n",
    "        max_nan_pct = max(r[\"nan_pct\"] for r in valid_results)\n",
    "        min_nan_pct = min(r[\"nan_pct\"] for r in valid_results)\n",
    "        \n",
    "        print(f\"\\nOverall Statistics:\")\n",
    "        print(f\"  Average NaN percentage across all features: {avg_nan_pct:.2f}%\")\n",
    "        print(f\"  Maximum NaN percentage: {max_nan_pct:.2f}%\")\n",
    "        print(f\"  Minimum NaN percentage: {min_nan_pct:.2f}%\")\n",
    "        \n",
    "        # Feature with highest NaN percentage\n",
    "        highest_nan_feature = max(valid_results, key=lambda r: r[\"nan_pct\"])\n",
    "        print(f\"  Feature with highest NaN percentage: {highest_nan_feature['name']} ({highest_nan_feature['nan_pct']:.2f}%)\")\n",
    "        \n",
    "        # Feature with lowest NaN percentage\n",
    "        lowest_nan_feature = min(valid_results, key=lambda r: r[\"nan_pct\"])\n",
    "        print(f\"  Feature with lowest NaN percentage: {lowest_nan_feature['name']} ({lowest_nan_feature['nan_pct']:.2f}%)\")\n",
    "    \n",
    "    # Check for suspicious patterns in NaN distributions\n",
    "    print(\"\\nChecking for suspicious NaN patterns...\")\n",
    "    \n",
    "    # Categorize features by NaN percentage\n",
    "    high_nan_features = [r[\"name\"] for r in valid_results if r[\"nan_pct\"] > 50]\n",
    "    if high_nan_features:\n",
    "        print(f\"  WARNING: {len(high_nan_features)} features have more than 50% NaN values:\")\n",
    "        for name in high_nan_features:\n",
    "            print(f\"    - {name}\")\n",
    "    \n",
    "    # Look for outliers in NaN percentages\n",
    "    if len(valid_results) > 3:\n",
    "        nan_percentages = [r[\"nan_pct\"] for r in valid_results]\n",
    "        avg = sum(nan_percentages) / len(nan_percentages)\n",
    "        std_dev = np.std(nan_percentages)\n",
    "        \n",
    "        outliers = []\n",
    "        for result in valid_results:\n",
    "            if abs(result[\"nan_pct\"] - avg) > 2 * std_dev:\n",
    "                outliers.append(result[\"name\"])\n",
    "        \n",
    "        if outliers:\n",
    "            print(f\"  NOTE: {len(outliers)} features have NaN percentages that are statistical outliers:\")\n",
    "            for name in outliers:\n",
    "                feature_result = next(r for r in valid_results if r[\"name\"] == name)\n",
    "                print(f\"    - {name}: {feature_result['nan_pct']:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_nan_percentages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0b28c-2d1d-4bdd-b6f2-dd51073d01a2",
   "metadata": {},
   "source": [
    "# Feature : Switch 2 Fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "050b670f-ef49-49f8-8319-126b9630e525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading valid patches metadata (with <=30% NaNs)...\n",
      "Found 2051 valid patches in 'valid_patches_under30.npz'\n",
      "Patch size: 128, domain: 6800x9000, threshold: 30.0%\n",
      "Generated 216 required band strings from 2005 to 2022\n",
      "Using 4 parallel processes for 7 features\n",
      "Starting parallel processing of features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 126/126 [16:44<00:00,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully processed 7 features: E0, NDVI, SS, Sd, S0, qtot, etot\n",
      "\n",
      "All features processed. Total time: 1004.59 seconds (16.74 minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import gc\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "FEATURES = [\n",
    "    {\"path\": \"../Switch2_Fuel_predictors/AWRA-L_e0_monthly_005_Aus.nc\", \"var\": \"e0\",    \"name\": \"E0\"},\n",
    "    {\"path\": \"../Switch2_Fuel_predictors/NDVI_monthly_005_Aus.nc\",      \"var\": \"NDVI\", \"name\": \"NDVI\"},\n",
    "    {\"path\": \"../Switch2_Fuel_predictors/AWRA-L_ss_monthly_005_Aus.nc\", \"var\": \"SS\",   \"name\": \"SS\"},\n",
    "    {\"path\": \"../Switch2_Fuel_predictors/AWRA-L_sd_monthly_005_Aus.nc\", \"var\": \"Sd\",   \"name\": \"Sd\"},\n",
    "    {\"path\": \"../Switch2_Fuel_predictors/AWRA-L_s0_monthly_005_Aus.nc\", \"var\": \"S0\",   \"name\": \"S0\"},\n",
    "    {\"path\": \"../Switch2_Fuel_predictors/AWRA-L_qtot_monthly_005_Aus.nc\",\"var\": \"qtot\",\"name\": \"qtot\"},\n",
    "    {\"path\": \"../Switch2_Fuel_predictors/AWRA-L_etot_monthly_005_Aus.nc\",\"var\": \"etot\",\"name\": \"etot\"}\n",
    "]\n",
    "\n",
    "START_YEAR = 2005\n",
    "END_YEAR   = 2022\n",
    "\n",
    "def generate_required_bands(start_year, end_year):\n",
    "    \"\"\"\n",
    "    Builds a list of required band strings for each month from start_year to end_year.\n",
    "    \"\"\"\n",
    "    required = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            band_str = f\"{year:04d}_{month:02d}_01\"\n",
    "            required.append(band_str)\n",
    "    return required\n",
    "\n",
    "def process_feature(args):\n",
    "    \"\"\"\n",
    "    Process a single feature file. All arguments are passed as a single tuple to\n",
    "    support multiprocessing.\n",
    "    \"\"\"\n",
    "    feature_info, valid_patches_list, patch_size, required_bands, out_root, proc_idx = args\n",
    "    \n",
    "    feature_path = feature_info[\"path\"]\n",
    "    feature_var  = feature_info[\"var\"]\n",
    "    feature_name = feature_info[\"name\"]\n",
    "    \n",
    "    \n",
    "    progress_file = os.path.join(out_root, f\"progress_{feature_name}.txt\")\n",
    "    with open(progress_file, 'w') as f:\n",
    "        f.write(f\"0/{END_YEAR-START_YEAR+1}\\n\")\n",
    "    \n",
    "    if not os.path.exists(feature_path):\n",
    "        with open(progress_file, 'w') as f:\n",
    "            f.write(f\"ERROR: File not found\\n\")\n",
    "        return None\n",
    "    \n",
    "    num_valid = len(valid_patches_list)\n",
    "    \n",
    "    try:\n",
    "      \n",
    "        ds = xr.open_dataset(feature_path)\n",
    "        \n",
    "        if feature_var not in ds.data_vars:\n",
    "            with open(progress_file, 'w') as f:\n",
    "                f.write(f\"ERROR: Variable not found\\n\")\n",
    "            ds.close()\n",
    "            return None\n",
    "        \n",
    "    \n",
    "        actual_bands = set(str(b) for b in ds[\"band\"].values)\n",
    "        \n",
    "       \n",
    "        missing_bands = [b for b in required_bands if b not in actual_bands]\n",
    "        if missing_bands:\n",
    "            with open(progress_file, 'w') as f:\n",
    "                f.write(f\"ERROR: Missing bands\\n\")\n",
    "            ds.close()\n",
    "            return None\n",
    "        \n",
    "       ement\n",
    "        for i, year in enumerate(range(START_YEAR, END_YEAR + 1)):\n",
    "            # Update progress file\n",
    "            with open(progress_file, 'w') as f:\n",
    "                f.write(f\"{i}/{END_YEAR-START_YEAR+1}\\n\")\n",
    "            \n",
    "   \n",
    "            year_bands = [b for b in required_bands if b.startswith(f\"{year}_\")]\n",
    "            \n",
    "        \n",
    "            months_in_year = len(year_bands)\n",
    "            year_data = np.zeros((months_in_year, num_valid, patch_size, patch_size), dtype=np.float32)\n",
    "            year_data.fill(np.nan)  \n",
    "          \n",
    "            for month_idx, band_str in enumerate(year_bands):\n",
    "                try:\n",
    "                   \n",
    "                    band_data = ds[feature_var].sel(band=band_str).load().values\n",
    "                    \n",
    "                    # Extract all patches\n",
    "                    for patch_idx, patch_info in enumerate(valid_patches_list):\n",
    "                        lat_start = patch_info[\"lat_start\"]\n",
    "                        lat_end   = patch_info[\"lat_end\"]\n",
    "                        lon_start = patch_info[\"lon_start\"]\n",
    "                        lon_end   = patch_info[\"lon_end\"]\n",
    "                        \n",
    "                        patch = band_data[lat_start:lat_end, lon_start:lon_end].copy()\n",
    "                        year_data[month_idx, patch_idx] = patch\n",
    "                    \n",
    "                    # Clean up band data \n",
    "                    del band_data\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    with open(os.path.join(out_root, f\"error_{feature_name}.txt\"), 'a') as f:\n",
    "                        f.write(f\"Error processing band {band_str}: {e}\\n\")\n",
    "                    continue\n",
    "            \n",
    "         \n",
    "            year_dir = os.path.join(out_root, str(year))\n",
    "            os.makedirs(year_dir, exist_ok=True)\n",
    "            \n",
    "     \n",
    "            out_file = os.path.join(year_dir, f\"{feature_name}.npz\")\n",
    "            \n",
    "      \n",
    "            metadata = {\n",
    "                \"feature_name\": feature_name,\n",
    "                \"feature_var\": feature_var,\n",
    "                \"year\": year,\n",
    "                \"bands\": year_bands,\n",
    "                \"patch_size\": patch_size,\n",
    "                \"num_patches\": num_valid\n",
    "            }\n",
    "            \n",
    "        \n",
    "            patch_indices = {}\n",
    "            for i, patch_info in enumerate(valid_patches_list):\n",
    "                patch_i = patch_info[\"patch_i\"]\n",
    "                patch_j = patch_info[\"patch_j\"]\n",
    "                patch_indices[f\"{patch_i}_{patch_j}\"] = i\n",
    "            \n",
    "            \n",
    "            np.savez_compressed(\n",
    "                out_file,\n",
    "                data=year_data,\n",
    "                metadata=metadata,\n",
    "                patch_indices=patch_indices\n",
    "            )\n",
    "            \n",
    "       \n",
    "            del year_data\n",
    "            gc.collect()\n",
    "            \n",
    "        ds.close()\n",
    "        \n",
    "       \n",
    "        with open(progress_file, 'w') as f:\n",
    "            f.write(f\"COMPLETE\\n\")\n",
    "            \n",
    "        return feature_name\n",
    "        \n",
    "    except Exception as e:\n",
    "        with open(os.path.join(out_root, f\"error_{feature_name}.txt\"), 'a') as f:\n",
    "            f.write(f\"Unexpected error: {e}\\n\")\n",
    "        return None\n",
    "\n",
    "def monitor_progress(out_root, features, total_years):\n",
    "    \"\"\"\n",
    "    Monitor progress of all processes and display in a single progress bar.\n",
    "    \"\"\"\n",
    "    features_done = set()\n",
    "    \n",
    "    with tqdm(total=len(features) * total_years, desc=\"Overall Progress\") as pbar:\n",
    "        previous_progress = 0\n",
    "        \n",
    "        while len(features_done) < len(features):\n",
    "            current_progress = 0\n",
    "            \n",
    "            for feature in features:\n",
    "                feature_name = feature[\"name\"]\n",
    "                if feature_name in features_done:\n",
    "                    current_progress += total_years\n",
    "                    continue\n",
    "                    \n",
    "                progress_file = os.path.join(out_root, f\"progress_{feature_name}.txt\")\n",
    "                if os.path.exists(progress_file):\n",
    "                    try:\n",
    "                        with open(progress_file, 'r') as f:\n",
    "                            content = f.read().strip()\n",
    "                            if content == \"COMPLETE\":\n",
    "                                features_done.add(feature_name)\n",
    "                                current_progress += total_years\n",
    "                            elif content.startswith(\"ERROR\"):\n",
    "                                features_done.add(feature_name)  \n",
    "                            else:\n",
    "                                try:\n",
    "                                    progress, total = content.split('/')\n",
    "                                    current_progress += int(progress)\n",
    "                                except:\n",
    "                                    pass\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(current_progress - previous_progress)\n",
    "            previous_progress = current_progress\n",
    "            \n",
    "         \n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            if len(features_done) == len(features):\n",
    "                # One final update\n",
    "                pbar.update(len(features) * total_years - previous_progress)\n",
    "                break\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Loading valid patches metadata (with <=30% NaNs)...\")\n",
    "    data = np.load(\"valid_patches_under30.npz\", allow_pickle=True)\n",
    "    \n",
    " \n",
    "    valid_patches_struct = data[\"valid_patches\"]\n",
    "    valid_patches_list = []\n",
    "    \n",
    "    for i in range(len(valid_patches_struct)):\n",
    "        patch = {}\n",
    "        for field in valid_patches_struct.dtype.names:\n",
    "            patch[field] = valid_patches_struct[field][i]\n",
    "        valid_patches_list.append(patch)\n",
    "    \n",
    "    patch_size = int(data[\"patch_size\"])\n",
    "    lat_dim    = int(data[\"lat_dim\"])\n",
    "    lon_dim    = int(data[\"lon_dim\"])\n",
    "    threshold  = float(data[\"threshold\"])\n",
    "    \n",
    "    num_valid = len(valid_patches_list)\n",
    "    print(f\"Found {num_valid} valid patches in 'valid_patches_under30.npz'\")\n",
    "    print(f\"Patch size: {patch_size}, domain: {lat_dim}x{lon_dim}, threshold: {threshold}%\")\n",
    "\n",
    "  \n",
    "    required_bands = generate_required_bands(START_YEAR, END_YEAR)\n",
    "    num_bands = len(required_bands)\n",
    "    print(f\"Generated {num_bands} required band strings from {START_YEAR} to {END_YEAR}\")\n",
    "\n",
    "   \n",
    "    out_root = \"fire\"\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "    \n",
    " \n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        os.makedirs(os.path.join(out_root, str(year)), exist_ok=True)\n",
    "    \n",
    "  \n",
    "    num_processes = min(4, len(FEATURES))\n",
    "    print(f\"Using {num_processes} parallel processes for {len(FEATURES)} features\")\n",
    "    \n",
    " \n",
    "    process_args = []\n",
    "    for i, feature in enumerate(FEATURES):\n",
    "        args = (\n",
    "            feature,               \n",
    "            valid_patches_list,  \n",
    "            patch_size,           \n",
    "            required_bands,     \n",
    "            out_root,            \n",
    "            i                      \n",
    "        )\n",
    "        process_args.append(args)\n",
    "    \n",
    "\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "    print(\"Starting parallel processing of features...\")\n",
    "    \n",
    " \n",
    "    pool.map_async(process_feature, process_args)\n",
    "    \n",
    "\n",
    "    total_years = END_YEAR - START_YEAR + 1\n",
    "    monitor_progress(out_root, FEATURES, total_years)\n",
    "    \n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    \n",
    "    completed_features = []\n",
    "    for feature in FEATURES:\n",
    "        feature_name = feature[\"name\"]\n",
    "        progress_file = os.path.join(out_root, f\"progress_{feature_name}.txt\")\n",
    "        if os.path.exists(progress_file):\n",
    "            with open(progress_file, 'r') as f:\n",
    "                content = f.read().strip()\n",
    "                if content == \"COMPLETE\":\n",
    "                    completed_features.append(feature_name)\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(completed_features)} features: {', '.join(completed_features)}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"\\nAll features processed. Total time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    mp.freeze_support()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2980e41-eb0a-49b3-98c5-2f61c9a6831a",
   "metadata": {},
   "source": [
    "## Validation of Patches : Switch 2 Fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8daef99-07ad-43a5-99ef-1acff38a0dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation of processed data...\n",
      "Original metadata: 2051 valid patches, patch size: 128x128, threshold: 30.0%\n",
      "✓ All 18 year directories exist.\n",
      "✓ All 126 expected data files exist.\n",
      "Validating content of 20 randomly selected files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Files: 100%|██████████| 20/20 [00:57<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results: 20/20 files are valid.\n",
      "\n",
      "Performing detailed validation on one file...\n",
      "Detailed validation of fire/2005/E0.npz:\n",
      "  - Total patches: 2051\n",
      "  - Valid patches (NaN% <= 30.0%): 2051\n",
      "  - Invalid patches: 0\n",
      "  - Highest NaN%: 29.98%\n",
      "  - Average NaN%: 1.11%\n",
      "\n",
      "NaN Percentage Distribution:\n",
      "   0-10 %: 1940 patches ( 94.6%) ███████████████████████████████████████████████\n",
      "  10-20 %:   83 patches (  4.0%) ██\n",
      "  20-30 %:   28 patches (  1.4%) \n",
      "  30-40 %:    0 patches (  0.0%) \n",
      "  40-50 %:    0 patches (  0.0%) \n",
      "  50-60 %:    0 patches (  0.0%) \n",
      "  60-70 %:    0 patches (  0.0%) \n",
      "  70-80 %:    0 patches (  0.0%) \n",
      "  80-90 %:    0 patches (  0.0%) \n",
      "  90-100%:    0 patches (  0.0%) \n",
      "\n",
      "Validating consistency across months...\n",
      "Month-to-month consistency in fire/2005/E0.npz:\n",
      "  - Number of months: 12\n",
      "  - Consistent shape: True\n",
      "  - Month-to-month NaN pattern consistency: 100.00%\n",
      "  - Are all patches valid (NaN% <= 30.0%)? True\n",
      "\n",
      "Validation completed in 63.08 seconds (1.05 minutes).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "FEATURES = [\"E0\", \"NDVI\", \"SS\", \"Sd\", \"S0\", \"qtot\", \"etot\"]\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2022\n",
    "PATCH_SIZE = 128\n",
    "OUT_ROOT = \"fire\"\n",
    "\n",
    "def validate_data_structure():\n",
    "    \"\"\"\n",
    "    Validates that all expected files exist and have the correct structure.\n",
    "    \"\"\"\n",
    "    print(\"Starting validation of processed data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "  \n",
    "    valid_patches_data = np.load(\"valid_patches_under30.npz\", allow_pickle=True)\n",
    "    valid_patches = valid_patches_data[\"valid_patches\"]\n",
    "    expected_num_patches = len(valid_patches)\n",
    "    expected_patch_size = int(valid_patches_data[\"patch_size\"])\n",
    "    threshold = float(valid_patches_data[\"threshold\"])\n",
    "    \n",
    "    print(f\"Original metadata: {expected_num_patches} valid patches, \"\n",
    "          f\"patch size: {expected_patch_size}x{expected_patch_size}, threshold: {threshold}%\")\n",
    "    \n",
    "    # Check all required directories exist\n",
    "    years = list(range(START_YEAR, END_YEAR + 1))\n",
    "    missing_dirs = []\n",
    "    \n",
    "    for year in years:\n",
    "        year_dir = os.path.join(OUT_ROOT, str(year))\n",
    "        if not os.path.exists(year_dir):\n",
    "            missing_dirs.append(year_dir)\n",
    "    \n",
    "    if missing_dirs:\n",
    "        print(f\"WARNING: {len(missing_dirs)} year directories are missing!\")\n",
    "        for d in missing_dirs[:5]:  \n",
    "            print(f\"  - {d}\")\n",
    "    else:\n",
    "        print(f\"✓ All {len(years)} year directories exist.\")\n",
    "    \n",
    "   \n",
    "    expected_files = []\n",
    "    for year in years:\n",
    "        for feature in FEATURES:\n",
    "            expected_files.append(os.path.join(OUT_ROOT, str(year), f\"{feature}.npz\"))\n",
    "    \n",
    "    missing_files = [f for f in expected_files if not os.path.exists(f)]\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"WARNING: {len(missing_files)} data files are missing!\")\n",
    "        for f in missing_files[:5]:  # Show only the first few\n",
    "            print(f\"  - {f}\")\n",
    "    else:\n",
    "        print(f\"✓ All {len(expected_files)} expected data files exist.\")\n",
    "    \n",
    "\n",
    "    num_to_sample = min(20, len(expected_files))\n",
    "    sample_files = np.random.choice(\n",
    "        [f for f in expected_files if os.path.exists(f)], \n",
    "        size=num_to_sample, \n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    validation_results = []\n",
    "    \n",
    "    print(f\"Validating content of {num_to_sample} randomly selected files...\")\n",
    "    for file_path in tqdm(sample_files, desc=\"Validating Files\"):\n",
    "        result = validate_file_content(file_path, expected_num_patches, expected_patch_size, threshold)\n",
    "        validation_results.append(result)\n",
    "    \n",
    "   \n",
    "    success_count = sum(1 for r in validation_results if r[\"valid\"])\n",
    "    print(f\"\\nValidation Results: {success_count}/{len(validation_results)} files are valid.\")\n",
    "    \n",
    "    if success_count < len(validation_results):\n",
    "        print(\"\\nIssues found:\")\n",
    "        for r in validation_results:\n",
    "            if not r[\"valid\"]:\n",
    "                print(f\"  - File: {r['file']}, Issue: {r['issue']}\")\n",
    "    \n",
    "   \n",
    "    print(\"\\nPerforming detailed validation on one file...\")\n",
    "    sample_file = expected_files[0]\n",
    "    if os.path.exists(sample_file):\n",
    "        detailed_result = validate_file_detailed(sample_file, valid_patches, threshold)\n",
    "        print(f\"Detailed validation of {sample_file}:\")\n",
    "        print(f\"  - Total patches: {detailed_result['total_patches']}\")\n",
    "        print(f\"  - Valid patches (NaN% <= {threshold}%): {detailed_result['valid_patches']}\")\n",
    "        print(f\"  - Invalid patches: {detailed_result['invalid_patches']}\")\n",
    "        print(f\"  - Highest NaN%: {detailed_result['max_nan_pct']:.2f}%\")\n",
    "        print(f\"  - Average NaN%: {detailed_result['avg_nan_pct']:.2f}%\")\n",
    "        \n",
    "     \n",
    "        if detailed_result['nan_percentages']:\n",
    "            nan_percentages = detailed_result['nan_percentages']\n",
    "            \n",
    "          \n",
    "            ranges = [(0, 10), (10, 20), (20, 30), (30, 40), (40, 50), \n",
    "                      (50, 60), (60, 70), (70, 80), (80, 90), (90, 100)]\n",
    "            \n",
    "            print(\"\\nNaN Percentage Distribution:\")\n",
    "            for low, high in ranges:\n",
    "                count = sum(1 for p in nan_percentages if low <= p < high)\n",
    "                percentage = (count / len(nan_percentages)) * 100\n",
    "                bar_length = int(percentage / 2)  # Scale for display\n",
    "                bar = '█' * bar_length\n",
    "                print(f\"  {low:2d}-{high:<3d}%: {count:4d} patches ({percentage:5.1f}%) {bar}\")\n",
    "    \n",
    " \n",
    "    print(\"\\nValidating consistency across months...\")\n",
    "    \n",
    "\n",
    "    test_feature = FEATURES[0]\n",
    "    test_year = START_YEAR\n",
    "    test_file = os.path.join(OUT_ROOT, str(test_year), f\"{test_feature}.npz\")\n",
    "    \n",
    "    if os.path.exists(test_file):\n",
    "        consistency_result = validate_month_consistency(test_file, threshold)\n",
    "        print(f\"Month-to-month consistency in {test_file}:\")\n",
    "        print(f\"  - Number of months: {consistency_result['num_months']}\")\n",
    "        print(f\"  - Consistent shape: {consistency_result['consistent_shape']}\")\n",
    "        print(f\"  - Month-to-month NaN pattern consistency: {consistency_result['nan_consistency']:.2f}%\")\n",
    "        print(f\"  - Are all patches valid (NaN% <= {threshold}%)? {consistency_result['all_valid']}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"\\nValidation completed in {elapsed:.2f} seconds ({elapsed/60:.2f} minutes).\")\n",
    "\n",
    "def validate_file_content(file_path, expected_num_patches, expected_patch_size, threshold):\n",
    "    \"\"\"\n",
    "    Validates the content of a single data file.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"file\": file_path,\n",
    "        \"valid\": True,\n",
    "        \"issue\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "     \n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        \n",
    "    \n",
    "        required_keys = ['data', 'metadata', 'patch_indices']\n",
    "        missing_keys = [k for k in required_keys if k not in data]\n",
    "        if missing_keys:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Missing keys: {', '.join(missing_keys)}\"\n",
    "            return result\n",
    "        \n",
    "        # Check data shape\n",
    "        data_array = data['data']\n",
    "        if len(data_array.shape) != 4:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Expected 4D array, got {len(data_array.shape)}D\"\n",
    "            return result\n",
    "        \n",
    "        _, num_patches, patch_height, patch_width = data_array.shape\n",
    "        \n",
    "        if num_patches != expected_num_patches:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Expected {expected_num_patches} patches, got {num_patches}\"\n",
    "            return result\n",
    "        \n",
    "        if patch_height != expected_patch_size or patch_width != expected_patch_size:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Expected {expected_patch_size}x{expected_patch_size} patch size, got {patch_height}x{patch_width}\"\n",
    "            return result\n",
    "        \n",
    " \n",
    "        metadata = data['metadata'].item() if isinstance(data['metadata'], np.ndarray) else data['metadata']\n",
    "        if not isinstance(metadata, dict):\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Metadata is not a dictionary\"\n",
    "            return result\n",
    "        \n",
    " \n",
    "        patch_indices = data['patch_indices'].item() if isinstance(data['patch_indices'], np.ndarray) else data['patch_indices']\n",
    "        if not isinstance(patch_indices, dict):\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Patch indices is not a dictionary\"\n",
    "            return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        result[\"valid\"] = False\n",
    "        result[\"issue\"] = f\"Error loading/parsing file: {e}\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "def validate_file_detailed(file_path, valid_patches, threshold):\n",
    "    \"\"\"\n",
    "    Performs a detailed validation of one file, checking all patches.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'total_patches': 0,\n",
    "        'valid_patches': 0,\n",
    "        'invalid_patches': 0,\n",
    "        'max_nan_pct': 0,\n",
    "        'avg_nan_pct': 0,\n",
    "        'nan_percentages': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "      \n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        data_array = data['data']\n",
    "        \n",
    "        num_months, num_patches, patch_height, patch_width = data_array.shape\n",
    "        result['total_patches'] = num_patches\n",
    "        \n",
    " \n",
    "        month_idx = 0  # First month\n",
    "        \n",
    "        nan_percentages = []\n",
    "        for patch_idx in range(num_patches):\n",
    "            patch = data_array[month_idx, patch_idx]\n",
    "            nan_count = np.isnan(patch).sum()\n",
    "            total_cells = patch_height * patch_width\n",
    "            nan_percentage = (nan_count / total_cells) * 100\n",
    "            \n",
    "            nan_percentages.append(nan_percentage)\n",
    "            \n",
    "            if nan_percentage <= threshold:\n",
    "                result['valid_patches'] += 1\n",
    "            else:\n",
    "                result['invalid_patches'] += 1\n",
    "        \n",
    "        result['nan_percentages'] = nan_percentages\n",
    "        result['max_nan_pct'] = max(nan_percentages) if nan_percentages else 0\n",
    "        result['avg_nan_pct'] = sum(nan_percentages) / len(nan_percentages) if nan_percentages else 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in detailed validation: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def validate_month_consistency(file_path, threshold):\n",
    "    \"\"\"\n",
    "    Validates consistency across all months in a file.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'num_months': 0,\n",
    "        'consistent_shape': True,\n",
    "        'nan_consistency': 100.0,  \n",
    "        'all_valid': True\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load the file\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        data_array = data['data']\n",
    "        \n",
    "        num_months, num_patches, patch_height, patch_width = data_array.shape\n",
    "        result['num_months'] = num_months\n",
    "        \n",
    "\n",
    "        for month_idx in range(num_months):\n",
    "            month_data = data_array[month_idx]\n",
    "            if month_data.shape != (num_patches, patch_height, patch_width):\n",
    "                result['consistent_shape'] = False\n",
    "                break\n",
    "        \n",
    "\n",
    "        reference_month = np.isnan(data_array[0])\n",
    "        total_cells = num_patches * patch_height * patch_width\n",
    "        \n",
    "        for month_idx in range(1, num_months):\n",
    "            current_month = np.isnan(data_array[month_idx])\n",
    "            differences = np.sum(reference_month != current_month)\n",
    "            if differences > 0:\n",
    "    \n",
    "                consistency_pct = 100 - (differences / total_cells * 100)\n",
    "                result['nan_consistency'] = min(result['nan_consistency'], consistency_pct)\n",
    "        \n",
    "  \n",
    "        for month_idx in range(num_months):\n",
    "            for patch_idx in range(num_patches):\n",
    "                patch = data_array[month_idx, patch_idx]\n",
    "                nan_count = np.isnan(patch).sum()\n",
    "                total_cells_patch = patch_height * patch_width\n",
    "                nan_percentage = (nan_count / total_cells_patch) * 100\n",
    "                \n",
    "                if nan_percentage > threshold:\n",
    "                    result['all_valid'] = False\n",
    "                    break\n",
    "            \n",
    "            if not result['all_valid']:\n",
    "                break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in month consistency validation: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    np.random.seed(42)\n",
    "    validate_data_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80a3a13-d47b-4a5b-a852-035a43bffdb6",
   "metadata": {},
   "source": [
    "#  Feature : Switch 3 Climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb505cf-0605-4565-94b2-0db434ef6411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading valid patches metadata (with <=30% NaNs)...\n",
      "Found 2051 valid patches in 'valid_patches_under30.npz'\n",
      "Patch size: 128, domain: 6800x9000, threshold: 30.0%\n",
      "Generated 216 required band strings from 2005 to 2022\n",
      "Using 4 parallel processes for 6 features\n",
      "Starting parallel processing of features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 108/108 [30:47<00:00, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully processed 6 features: Lightning, Precipitation, Maximum_Temperature, Minimum_Temperature, Vapor_Pressure_09, Vapor_Pressure_15\n",
      "\n",
      "All features processed. Total time: 1848.04 seconds (30.80 minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import gc\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "FEATURES = [\n",
    "    {\"path\": \"../Switch3_Climate_predictors/Llightning_nc_005_Aus.nc\", \"var\": \"lightning\", \"name\": \"Lightning\"},\n",
    "    {\"path\": \"../Switch3_Climate_predictors/precip_monthly_005_Aus.nc\", \"var\": \"precip\",   \"name\": \"Precipitation\"},\n",
    "    {\"path\": \"../Switch3_Climate_predictors/tmax_monthly_005_Aus.nc\",   \"var\": \"tmax\",     \"name\": \"Maximum_Temperature\"},\n",
    "    {\"path\": \"../Switch3_Climate_predictors/tmin_monthly_005_Aus.nc\",   \"var\": \"tmin\",     \"name\": \"Minimum_Temperature\"},\n",
    "    {\"path\": \"../Switch3_Climate_predictors/vapourpresh09_monthly_005_Aus.nc\", \"var\": \"vapourpres\", \"name\": \"Vapor_Pressure_09\"},\n",
    "    {\"path\": \"../Switch3_Climate_predictors/vapourpresh15_monthly_005_Aus.nc\", \"var\": \"vapourpres\", \"name\": \"Vapor_Pressure_15\"}\n",
    "]\n",
    "\n",
    "START_YEAR = 2005\n",
    "END_YEAR   = 2022\n",
    "\n",
    "def generate_required_bands(start_year, end_year):\n",
    "    \"\"\"\n",
    "    Builds a list of required band strings for each month from start_year to end_year.\n",
    "    \"\"\"\n",
    "    required = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            band_str = f\"{year:04d}_{month:02d}_01\"\n",
    "            required.append(band_str)\n",
    "    return required\n",
    "\n",
    "def process_feature(args):\n",
    "    \"\"\"\n",
    "    Process a single feature file. All arguments are passed as a single tuple to\n",
    "    support multiprocessing.\n",
    "    \"\"\"\n",
    "    feature_info, valid_patches_list, patch_size, required_bands, out_root, proc_idx = args\n",
    "    \n",
    "    feature_path = feature_info[\"path\"]\n",
    "    feature_var  = feature_info[\"var\"]\n",
    "    feature_name = feature_info[\"name\"]\n",
    "    \n",
    " \n",
    "    progress_file = os.path.join(out_root, f\"progress_{feature_name}.txt\")\n",
    "    with open(progress_file, 'w') as f:\n",
    "        f.write(f\"0/{END_YEAR-START_YEAR+1}\\n\")\n",
    "    \n",
    "    if not os.path.exists(feature_path):\n",
    "        with open(progress_file, 'w') as f:\n",
    "            f.write(f\"ERROR: File not found\\n\")\n",
    "        return None\n",
    "    \n",
    "    num_valid = len(valid_patches_list)\n",
    "    \n",
    "    try:\n",
    "       \n",
    "        ds = xr.open_dataset(feature_path)\n",
    "        \n",
    "        if feature_var not in ds.data_vars:\n",
    "            with open(progress_file, 'w') as f:\n",
    "                f.write(f\"ERROR: Variable not found\\n\")\n",
    "            ds.close()\n",
    "            return None\n",
    "        \n",
    "\n",
    "        actual_bands = set(str(b) for b in ds[\"band\"].values)\n",
    "        \n",
    "\n",
    "        missing_bands = [b for b in required_bands if b not in actual_bands]\n",
    "        if missing_bands:\n",
    "            with open(progress_file, 'w') as f:\n",
    "                f.write(f\"ERROR: Missing bands\\n\")\n",
    "            ds.close()\n",
    "            return None\n",
    "        \n",
    "\n",
    "        for i, year in enumerate(range(START_YEAR, END_YEAR + 1)):\n",
    "            # Update progress file\n",
    "            with open(progress_file, 'w') as f:\n",
    "                f.write(f\"{i}/{END_YEAR-START_YEAR+1}\\n\")\n",
    "            \n",
    "        \n",
    "            year_bands = [b for b in required_bands if b.startswith(f\"{year}_\")]\n",
    "            \n",
    "       \n",
    "            months_in_year = len(year_bands)\n",
    "            year_data = np.zeros((months_in_year, num_valid, patch_size, patch_size), dtype=np.float32)\n",
    "            year_data.fill(np.nan)  # Initialize with NaNs\n",
    "            \n",
    "        \n",
    "            for month_idx, band_str in enumerate(year_bands):\n",
    "                try:\n",
    "                \n",
    "                    band_data = ds[feature_var].sel(band=band_str).load().values\n",
    "                    \n",
    "           \n",
    "                    for patch_idx, patch_info in enumerate(valid_patches_list):\n",
    "                        lat_start = patch_info[\"lat_start\"]\n",
    "                        lat_end   = patch_info[\"lat_end\"]\n",
    "                        lon_start = patch_info[\"lon_start\"]\n",
    "                        lon_end   = patch_info[\"lon_end\"]\n",
    "                        \n",
    "                        patch = band_data[lat_start:lat_end, lon_start:lon_end].copy()\n",
    "                        year_data[month_idx, patch_idx] = patch\n",
    "                    \n",
    "           \n",
    "                    del band_data\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    with open(os.path.join(out_root, f\"error_{feature_name}.txt\"), 'a') as f:\n",
    "                        f.write(f\"Error processing band {band_str}: {e}\\n\")\n",
    "                    continue\n",
    "            \n",
    "    \n",
    "            year_dir = os.path.join(out_root, str(year))\n",
    "            os.makedirs(year_dir, exist_ok=True)\n",
    "            \n",
    " \n",
    "            out_file = os.path.join(year_dir, f\"{feature_name}.npz\")\n",
    "            \n",
    "     \n",
    "            metadata = {\n",
    "                \"feature_name\": feature_name,\n",
    "                \"feature_var\": feature_var,\n",
    "                \"year\": year,\n",
    "                \"bands\": year_bands,\n",
    "                \"patch_size\": patch_size,\n",
    "                \"num_patches\": num_valid\n",
    "            }\n",
    "            \n",
    "     \n",
    "            patch_indices = {}\n",
    "            for i, patch_info in enumerate(valid_patches_list):\n",
    "                patch_i = patch_info[\"patch_i\"]\n",
    "                patch_j = patch_info[\"patch_j\"]\n",
    "                patch_indices[f\"{patch_i}_{patch_j}\"] = i\n",
    "            \n",
    "    \n",
    "            np.savez_compressed(\n",
    "                out_file,\n",
    "                data=year_data,\n",
    "                metadata=metadata,\n",
    "                patch_indices=patch_indices\n",
    "            )\n",
    "            \n",
    "    \n",
    "            del year_data\n",
    "            gc.collect()\n",
    "            \n",
    "        ds.close()\n",
    "        \n",
    "\n",
    "        with open(progress_file, 'w') as f:\n",
    "            f.write(f\"COMPLETE\\n\")\n",
    "            \n",
    "        return feature_name\n",
    "        \n",
    "    except Exception as e:\n",
    "        with open(os.path.join(out_root, f\"error_{feature_name}.txt\"), 'a') as f:\n",
    "            f.write(f\"Unexpected error: {e}\\n\")\n",
    "        return None\n",
    "\n",
    "def monitor_progress(out_root, features, total_years):\n",
    "    \"\"\"\n",
    "    Monitor progress of all processes and display in a single progress bar.\n",
    "    \"\"\"\n",
    "    features_done = set()\n",
    "    \n",
    "    with tqdm(total=len(features) * total_years, desc=\"Overall Progress\") as pbar:\n",
    "        previous_progress = 0\n",
    "        \n",
    "        while len(features_done) < len(features):\n",
    "            current_progress = 0\n",
    "            \n",
    "            for feature in features:\n",
    "                feature_name = feature[\"name\"]\n",
    "                if feature_name in features_done:\n",
    "                    current_progress += total_years\n",
    "                    continue\n",
    "                    \n",
    "                progress_file = os.path.join(out_root, f\"progress_{feature_name}.txt\")\n",
    "                if os.path.exists(progress_file):\n",
    "                    try:\n",
    "                        with open(progress_file, 'r') as f:\n",
    "                            content = f.read().strip()\n",
    "                            if content == \"COMPLETE\":\n",
    "                                features_done.add(feature_name)\n",
    "                                current_progress += total_years\n",
    "                            elif content.startswith(\"ERROR\"):\n",
    "                                features_done.add(feature_name)  # Count as done but errored\n",
    "                            else:\n",
    "                                try:\n",
    "                                    progress, total = content.split('/')\n",
    "                                    current_progress += int(progress)\n",
    "                                except:\n",
    "                                    pass\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(current_progress - previous_progress)\n",
    "            previous_progress = current_progress\n",
    "            \n",
    "            # Don't update too frequently\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            if len(features_done) == len(features):\n",
    "                # One final update\n",
    "                pbar.update(len(features) * total_years - previous_progress)\n",
    "                break\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Loading valid patches metadata (with <=30% NaNs)...\")\n",
    "    data = np.load(\"valid_patches_under30.npz\", allow_pickle=True)\n",
    "    \n",
    " \n",
    "    valid_patches_struct = data[\"valid_patches\"]\n",
    "    valid_patches_list = []\n",
    "    \n",
    "    for i in range(len(valid_patches_struct)):\n",
    "        patch = {}\n",
    "        for field in valid_patches_struct.dtype.names:\n",
    "            patch[field] = valid_patches_struct[field][i]\n",
    "        valid_patches_list.append(patch)\n",
    "    \n",
    "    patch_size = int(data[\"patch_size\"])\n",
    "    lat_dim    = int(data[\"lat_dim\"])\n",
    "    lon_dim    = int(data[\"lon_dim\"])\n",
    "    threshold  = float(data[\"threshold\"])\n",
    "    \n",
    "    num_valid = len(valid_patches_list)\n",
    "    print(f\"Found {num_valid} valid patches in 'valid_patches_under30.npz'\")\n",
    "    print(f\"Patch size: {patch_size}, domain: {lat_dim}x{lon_dim}, threshold: {threshold}%\")\n",
    "\n",
    "\n",
    "    required_bands = generate_required_bands(START_YEAR, END_YEAR)\n",
    "    num_bands = len(required_bands)\n",
    "    print(f\"Generated {num_bands} required band strings from {START_YEAR} to {END_YEAR}\")\n",
    "\n",
    "\n",
    "    out_root = \"climate\"\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "    \n",
    " \n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        os.makedirs(os.path.join(out_root, str(year)), exist_ok=True)\n",
    "    \n",
    "\n",
    "    num_processes = min(4, len(FEATURES))\n",
    "    print(f\"Using {num_processes} parallel processes for {len(FEATURES)} features\")\n",
    "    \n",
    "\n",
    "    process_args = []\n",
    "    for i, feature in enumerate(FEATURES):\n",
    "        args = (\n",
    "            feature,                # Feature info\n",
    "            valid_patches_list,     # Valid patches\n",
    "            patch_size,             # Patch size\n",
    "            required_bands,         # Required bands\n",
    "            out_root,               # Output root directory\n",
    "            i                       # Process index\n",
    "        )\n",
    "        process_args.append(args)\n",
    "    \n",
    "\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "    print(\"Starting parallel processing of features...\")\n",
    "    \n",
    "  \n",
    "    pool.map_async(process_feature, process_args)\n",
    "    \n",
    "  \n",
    "    total_years = END_YEAR - START_YEAR + 1\n",
    "    monitor_progress(out_root, FEATURES, total_years)\n",
    "    \n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "\n",
    "    completed_features = []\n",
    "    for feature in FEATURES:\n",
    "        feature_name = feature[\"name\"]\n",
    "        progress_file = os.path.join(out_root, f\"progress_{feature_name}.txt\")\n",
    "        if os.path.exists(progress_file):\n",
    "            with open(progress_file, 'r') as f:\n",
    "                content = f.read().strip()\n",
    "                if content == \"COMPLETE\":\n",
    "                    completed_features.append(feature_name)\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(completed_features)} features: {', '.join(completed_features)}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"\\nAll features processed. Total time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    mp.freeze_support()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ede0fa7-1dca-484c-b848-8239f71e474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation of processed climate data...\n",
      "Original metadata: 2051 valid patches, patch size: 128x128, threshold: 30.0%\n",
      "✓ All 18 year directories exist.\n",
      "✓ All 108 expected data files exist.\n",
      "Validating content of 20 randomly selected files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Files: 100%|██████████| 20/20 [01:49<00:00,  5.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results: 20/20 files are valid.\n",
      "\n",
      "Performing detailed validation on one file...\n",
      "Detailed validation of climate/2005/Lightning.npz:\n",
      "  - Total patches: 2051\n",
      "  - Valid patches (NaN% <= 30.0%): 2051\n",
      "  - Invalid patches: 0\n",
      "  - Highest NaN%: 0.00%\n",
      "  - Average NaN%: 0.00%\n",
      "\n",
      "NaN Percentage Distribution:\n",
      "   0-10 %: 2051 patches (100.0%) ██████████████████████████████████████████████████\n",
      "  10-20 %:    0 patches (  0.0%) \n",
      "  20-30 %:    0 patches (  0.0%) \n",
      "  30-40 %:    0 patches (  0.0%) \n",
      "  40-50 %:    0 patches (  0.0%) \n",
      "  50-60 %:    0 patches (  0.0%) \n",
      "  60-70 %:    0 patches (  0.0%) \n",
      "  70-80 %:    0 patches (  0.0%) \n",
      "  80-90 %:    0 patches (  0.0%) \n",
      "  90-100%:    0 patches (  0.0%) \n",
      "\n",
      "Validating consistency across months...\n",
      "Month-to-month consistency in climate/2005/Lightning.npz:\n",
      "  - Number of months: 12\n",
      "  - Consistent shape: True\n",
      "  - Month-to-month NaN pattern consistency: 100.00%\n",
      "  - Are all patches valid (NaN% <= 30.0%)? True\n",
      "\n",
      "Analyzing temporal variation in climate data...\n",
      "Temporal variation in climate/2005/Lightning.npz:\n",
      "  - Mean month-to-month change: 0.0050\n",
      "  - Maximum month-to-month change: 0.2934\n",
      "  - Standard deviation of values: 0.0753\n",
      "  - Detected likely seasonal pattern in the data\n",
      "\n",
      "Validation completed in 122.22 seconds (2.04 minutes).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define the parameters that should match our processed climate data\n",
    "FEATURES = [\"Lightning\", \"Precipitation\", \"Maximum_Temperature\", \"Minimum_Temperature\", \n",
    "            \"Vapor_Pressure_09\", \"Vapor_Pressure_15\"]\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2022\n",
    "PATCH_SIZE = 128\n",
    "OUT_ROOT = \"climate\"\n",
    "\n",
    "def validate_data_structure():\n",
    "    \"\"\"\n",
    "    Validates that all expected files exist and have the correct structure.\n",
    "    \"\"\"\n",
    "    print(\"Starting validation of processed climate data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load the original valid patches metadata for verification\n",
    "    valid_patches_data = np.load(\"valid_patches_under30.npz\", allow_pickle=True)\n",
    "    valid_patches = valid_patches_data[\"valid_patches\"]\n",
    "    expected_num_patches = len(valid_patches)\n",
    "    expected_patch_size = int(valid_patches_data[\"patch_size\"])\n",
    "    threshold = float(valid_patches_data[\"threshold\"])\n",
    "    \n",
    "    print(f\"Original metadata: {expected_num_patches} valid patches, \"\n",
    "          f\"patch size: {expected_patch_size}x{expected_patch_size}, threshold: {threshold}%\")\n",
    "    \n",
    "    # Check all required directories exist\n",
    "    years = list(range(START_YEAR, END_YEAR + 1))\n",
    "    missing_dirs = []\n",
    "    \n",
    "    for year in years:\n",
    "        year_dir = os.path.join(OUT_ROOT, str(year))\n",
    "        if not os.path.exists(year_dir):\n",
    "            missing_dirs.append(year_dir)\n",
    "    \n",
    "    if missing_dirs:\n",
    "        print(f\"WARNING: {len(missing_dirs)} year directories are missing!\")\n",
    "        for d in missing_dirs[:5]:  # Show only the first few\n",
    "            print(f\"  - {d}\")\n",
    "    else:\n",
    "        print(f\"✓ All {len(years)} year directories exist.\")\n",
    "    \n",
    "    # Verify all npz files exist\n",
    "    expected_files = []\n",
    "    for year in years:\n",
    "        for feature in FEATURES:\n",
    "            expected_files.append(os.path.join(OUT_ROOT, str(year), f\"{feature}.npz\"))\n",
    "    \n",
    "    missing_files = [f for f in expected_files if not os.path.exists(f)]\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"WARNING: {len(missing_files)} data files are missing!\")\n",
    "        for f in missing_files[:5]:  # Show only the first few\n",
    "            print(f\"  - {f}\")\n",
    "    else:\n",
    "        print(f\"✓ All {len(expected_files)} expected data files exist.\")\n",
    "    \n",
    "    # Validate content of files (sample a subset for efficiency)\n",
    "    num_to_sample = min(20, len(expected_files))\n",
    "    sample_files = np.random.choice(\n",
    "        [f for f in expected_files if os.path.exists(f)], \n",
    "        size=num_to_sample, \n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    validation_results = []\n",
    "    \n",
    "    print(f\"Validating content of {num_to_sample} randomly selected files...\")\n",
    "    for file_path in tqdm(sample_files, desc=\"Validating Files\"):\n",
    "        result = validate_file_content(file_path, expected_num_patches, expected_patch_size, threshold)\n",
    "        validation_results.append(result)\n",
    "    \n",
    "    # Summarize validation results\n",
    "    success_count = sum(1 for r in validation_results if r[\"valid\"])\n",
    "    print(f\"\\nValidation Results: {success_count}/{len(validation_results)} files are valid.\")\n",
    "    \n",
    "    if success_count < len(validation_results):\n",
    "        print(\"\\nIssues found:\")\n",
    "        for r in validation_results:\n",
    "            if not r[\"valid\"]:\n",
    "                print(f\"  - File: {r['file']}, Issue: {r['issue']}\")\n",
    "    \n",
    "    # Perform a deeper validation on one file to check all patches\n",
    "    print(\"\\nPerforming detailed validation on one file...\")\n",
    "    sample_file = expected_files[0]\n",
    "    if os.path.exists(sample_file):\n",
    "        detailed_result = validate_file_detailed(sample_file, valid_patches, threshold)\n",
    "        print(f\"Detailed validation of {sample_file}:\")\n",
    "        print(f\"  - Total patches: {detailed_result['total_patches']}\")\n",
    "        print(f\"  - Valid patches (NaN% <= {threshold}%): {detailed_result['valid_patches']}\")\n",
    "        print(f\"  - Invalid patches: {detailed_result['invalid_patches']}\")\n",
    "        print(f\"  - Highest NaN%: {detailed_result['max_nan_pct']:.2f}%\")\n",
    "        print(f\"  - Average NaN%: {detailed_result['avg_nan_pct']:.2f}%\")\n",
    "        \n",
    "        # Print NaN percentage distribution in text form instead of histogram\n",
    "        if detailed_result['nan_percentages']:\n",
    "            nan_percentages = detailed_result['nan_percentages']\n",
    "            \n",
    "            # Define ranges for text-based distribution\n",
    "            ranges = [(0, 10), (10, 20), (20, 30), (30, 40), (40, 50), \n",
    "                      (50, 60), (60, 70), (70, 80), (80, 90), (90, 100)]\n",
    "            \n",
    "            print(\"\\nNaN Percentage Distribution:\")\n",
    "            for low, high in ranges:\n",
    "                count = sum(1 for p in nan_percentages if low <= p < high)\n",
    "                percentage = (count / len(nan_percentages)) * 100\n",
    "                bar_length = int(percentage / 2)  # Scale for display\n",
    "                bar = '█' * bar_length\n",
    "                print(f\"  {low:2d}-{high:<3d}%: {count:4d} patches ({percentage:5.1f}%) {bar}\")\n",
    "    \n",
    "    # Validate consistency across months\n",
    "    print(\"\\nValidating consistency across months...\")\n",
    "    \n",
    "    # Select one feature and year for consistency check\n",
    "    test_feature = FEATURES[0]\n",
    "    test_year = START_YEAR\n",
    "    test_file = os.path.join(OUT_ROOT, str(test_year), f\"{test_feature}.npz\")\n",
    "    \n",
    "    if os.path.exists(test_file):\n",
    "        consistency_result = validate_month_consistency(test_file, threshold)\n",
    "        print(f\"Month-to-month consistency in {test_file}:\")\n",
    "        print(f\"  - Number of months: {consistency_result['num_months']}\")\n",
    "        print(f\"  - Consistent shape: {consistency_result['consistent_shape']}\")\n",
    "        print(f\"  - Month-to-month NaN pattern consistency: {consistency_result['nan_consistency']:.2f}%\")\n",
    "        print(f\"  - Are all patches valid (NaN% <= {threshold}%)? {consistency_result['all_valid']}\")\n",
    "    \n",
    "    # Additional check for climate data: analyze temporal variation\n",
    "    if os.path.exists(test_file):\n",
    "        print(\"\\nAnalyzing temporal variation in climate data...\")\n",
    "        temporal_result = analyze_temporal_variation(test_file)\n",
    "        print(f\"Temporal variation in {test_file}:\")\n",
    "        print(f\"  - Mean month-to-month change: {temporal_result['mean_change']:.4f}\")\n",
    "        print(f\"  - Maximum month-to-month change: {temporal_result['max_change']:.4f}\")\n",
    "        print(f\"  - Standard deviation of values: {temporal_result['std_dev']:.4f}\")\n",
    "        \n",
    "        # Check for seasonal patterns (simple check)\n",
    "        if temporal_result['has_seasonal_pattern']:\n",
    "            print(\"  - Detected likely seasonal pattern in the data\")\n",
    "        else:\n",
    "            print(\"  - No clear seasonal pattern detected\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"\\nValidation completed in {elapsed:.2f} seconds ({elapsed/60:.2f} minutes).\")\n",
    "\n",
    "def validate_file_content(file_path, expected_num_patches, expected_patch_size, threshold):\n",
    "    \"\"\"\n",
    "    Validates the content of a single data file.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"file\": file_path,\n",
    "        \"valid\": True,\n",
    "        \"issue\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load the file\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        \n",
    "        # Check if required keys exist\n",
    "        required_keys = ['data', 'metadata', 'patch_indices']\n",
    "        missing_keys = [k for k in required_keys if k not in data]\n",
    "        if missing_keys:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Missing keys: {', '.join(missing_keys)}\"\n",
    "            return result\n",
    "        \n",
    "        # Check data shape\n",
    "        data_array = data['data']\n",
    "        if len(data_array.shape) != 4:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Expected 4D array, got {len(data_array.shape)}D\"\n",
    "            return result\n",
    "        \n",
    "        # Climate data typically has shape [months, patches, height, width]\n",
    "        num_months, num_patches, patch_height, patch_width = data_array.shape\n",
    "        \n",
    "        if num_patches != expected_num_patches:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Expected {expected_num_patches} patches, got {num_patches}\"\n",
    "            return result\n",
    "        \n",
    "        if patch_height != expected_patch_size or patch_width != expected_patch_size:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Expected {expected_patch_size}x{expected_patch_size} patch size, got {patch_height}x{patch_width}\"\n",
    "            return result\n",
    "        \n",
    "        # Check metadata structure\n",
    "        metadata = data['metadata'].item() if isinstance(data['metadata'], np.ndarray) else data['metadata']\n",
    "        if not isinstance(metadata, dict):\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Metadata is not a dictionary\"\n",
    "            return result\n",
    "        \n",
    "        # Validate climate metadata - check if required fields exist\n",
    "        required_metadata = [\"feature_name\", \"feature_var\", \"year\", \"bands\", \"patch_size\", \"num_patches\"]\n",
    "        missing_metadata = [k for k in required_metadata if k not in metadata]\n",
    "        if missing_metadata:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Missing metadata fields: {', '.join(missing_metadata)}\"\n",
    "            return result\n",
    "        \n",
    "        # Check patch indices structure\n",
    "        patch_indices = data['patch_indices'].item() if isinstance(data['patch_indices'], np.ndarray) else data['patch_indices']\n",
    "        if not isinstance(patch_indices, dict):\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Patch indices is not a dictionary\"\n",
    "            return result\n",
    "        \n",
    "        # Check if months match expected value (should be 12 for climate data, per year)\n",
    "        if num_months != 12:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Expected 12 months per year, got {num_months}\"\n",
    "            return result\n",
    "        \n",
    "        # Climate-specific: check for unrealistic values\n",
    "        feature_name = os.path.basename(file_path).replace('.npz', '')\n",
    "        unrealistic_values = check_unrealistic_values(data_array, feature_name)\n",
    "        if unrealistic_values:\n",
    "            result[\"valid\"] = False\n",
    "            result[\"issue\"] = f\"Contains unrealistic values: {unrealistic_values}\"\n",
    "            return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        result[\"valid\"] = False\n",
    "        result[\"issue\"] = f\"Error loading/parsing file: {e}\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "def check_unrealistic_values(data_array, feature_name):\n",
    "    \"\"\"\n",
    "    Check for unrealistic values based on the feature type.\n",
    "    Returns description of problems found or None if values are reasonable.\n",
    "    \"\"\"\n",
    "    # Flatten for easier analysis, ignoring NaNs\n",
    "    valid_data = data_array[~np.isnan(data_array)]\n",
    "    \n",
    "    if len(valid_data) == 0:\n",
    "        return \"Data contains only NaN values\"\n",
    "    \n",
    "    # Define reasonable ranges for different climate features\n",
    "    if \"Temperature\" in feature_name:\n",
    "        # Temperature in degrees Celsius, realistic range from -80 to +60\n",
    "        if np.min(valid_data) < -80 or np.max(valid_data) > 60:\n",
    "            return f\"Temperature out of realistic range: min={np.min(valid_data):.1f}, max={np.max(valid_data):.1f}\"\n",
    "    \n",
    "    elif \"Precipitation\" in feature_name:\n",
    "        # Precipitation should be >= 0 and generally < 2000mm per month\n",
    "        if np.min(valid_data) < 0:\n",
    "            return f\"Negative precipitation values: min={np.min(valid_data):.1f}\"\n",
    "        if np.max(valid_data) > 2000:\n",
    "            return f\"Extremely high precipitation: max={np.max(valid_data):.1f}\"\n",
    "    \n",
    "    elif \"Lightning\" in feature_name:\n",
    "        # Lightning counts should be >= 0\n",
    "        if np.min(valid_data) < 0:\n",
    "            return f\"Negative lightning counts: min={np.min(valid_data):.1f}\"\n",
    "    \n",
    "    elif \"Vapor_Pressure\" in feature_name:\n",
    "        # Vapor pressure typically between 0-101.3 kPa (1 atm)\n",
    "        if np.min(valid_data) < 0:\n",
    "            return f\"Negative vapor pressure: min={np.min(valid_data):.1f}\"\n",
    "        if np.max(valid_data) > 101.3:\n",
    "            return f\"Vapor pressure exceeds 1 atm: max={np.max(valid_data):.1f}\"\n",
    "    \n",
    "    # Check for other anomalies\n",
    "    if np.all(valid_data == valid_data[0]):\n",
    "        return \"Data contains only a single repeated value\"\n",
    "    \n",
    "    # Check for NaN percentage\n",
    "    nan_percentage = np.isnan(data_array).sum() / data_array.size * 100\n",
    "    if nan_percentage > 50:\n",
    "        return f\"High percentage of NaN values: {nan_percentage:.1f}%\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "def validate_file_detailed(file_path, valid_patches, threshold):\n",
    "    \"\"\"\n",
    "    Performs a detailed validation of one file, checking all patches.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'total_patches': 0,\n",
    "        'valid_patches': 0,\n",
    "        'invalid_patches': 0,\n",
    "        'max_nan_pct': 0,\n",
    "        'avg_nan_pct': 0,\n",
    "        'nan_percentages': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load the file\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        data_array = data['data']\n",
    "        \n",
    "        num_months, num_patches, patch_height, patch_width = data_array.shape\n",
    "        result['total_patches'] = num_patches\n",
    "        \n",
    "        # Sample a month to check NaN percentages\n",
    "        month_idx = 0  # First month\n",
    "        \n",
    "        nan_percentages = []\n",
    "        for patch_idx in range(num_patches):\n",
    "            patch = data_array[month_idx, patch_idx]\n",
    "            nan_count = np.isnan(patch).sum()\n",
    "            total_cells = patch_height * patch_width\n",
    "            nan_percentage = (nan_count / total_cells) * 100\n",
    "            \n",
    "            nan_percentages.append(nan_percentage)\n",
    "            \n",
    "            if nan_percentage <= threshold:\n",
    "                result['valid_patches'] += 1\n",
    "            else:\n",
    "                result['invalid_patches'] += 1\n",
    "        \n",
    "        result['nan_percentages'] = nan_percentages\n",
    "        result['max_nan_pct'] = max(nan_percentages) if nan_percentages else 0\n",
    "        result['avg_nan_pct'] = sum(nan_percentages) / len(nan_percentages) if nan_percentages else 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in detailed validation: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def validate_month_consistency(file_path, threshold):\n",
    "    \"\"\"\n",
    "    Validates consistency across all months in a file.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'num_months': 0,\n",
    "        'consistent_shape': True,\n",
    "        'nan_consistency': 100.0,  # Percentage of consistent NaN patterns\n",
    "        'all_valid': True\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load the file\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        data_array = data['data']\n",
    "        \n",
    "        num_months, num_patches, patch_height, patch_width = data_array.shape\n",
    "        result['num_months'] = num_months\n",
    "        \n",
    "        # Check shape consistency across months\n",
    "        for month_idx in range(num_months):\n",
    "            month_data = data_array[month_idx]\n",
    "            if month_data.shape != (num_patches, patch_height, patch_width):\n",
    "                result['consistent_shape'] = False\n",
    "                break\n",
    "        \n",
    "        # Check NaN pattern consistency across months\n",
    "        # We'll compare each month to the first month\n",
    "        reference_month = np.isnan(data_array[0])\n",
    "        total_cells = num_patches * patch_height * patch_width\n",
    "        \n",
    "        for month_idx in range(1, num_months):\n",
    "            current_month = np.isnan(data_array[month_idx])\n",
    "            differences = np.sum(reference_month != current_month)\n",
    "            if differences > 0:\n",
    "                # There are some differences in NaN patterns\n",
    "                consistency_pct = 100 - (differences / total_cells * 100)\n",
    "                result['nan_consistency'] = min(result['nan_consistency'], consistency_pct)\n",
    "        \n",
    "        # Check if all patches in all months are valid\n",
    "        for month_idx in range(num_months):\n",
    "            for patch_idx in range(num_patches):\n",
    "                patch = data_array[month_idx, patch_idx]\n",
    "                nan_count = np.isnan(patch).sum()\n",
    "                total_cells_patch = patch_height * patch_width\n",
    "                nan_percentage = (nan_count / total_cells_patch) * 100\n",
    "                \n",
    "                if nan_percentage > threshold:\n",
    "                    result['all_valid'] = False\n",
    "                    break\n",
    "            \n",
    "            if not result['all_valid']:\n",
    "                break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in month consistency validation: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def analyze_temporal_variation(file_path):\n",
    "    \"\"\"\n",
    "    Analyzes temporal variation in the climate data.\n",
    "    This is a climate-specific function to check for realistic seasonal patterns.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'mean_change': 0.0,\n",
    "        'max_change': 0.0,\n",
    "        'std_dev': 0.0,\n",
    "        'has_seasonal_pattern': False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load the file\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        data_array = data['data']\n",
    "        feature_name = data['metadata'].item()['feature_name']\n",
    "        \n",
    "        num_months, num_patches, patch_height, patch_width = data_array.shape\n",
    "        \n",
    "        # For efficiency, sample a subset of patches\n",
    "        num_to_sample = min(50, num_patches)\n",
    "        patch_indices = np.random.choice(num_patches, num_to_sample, replace=False)\n",
    "        \n",
    "        # Calculate month-to-month changes\n",
    "        changes = []\n",
    "        for patch_idx in patch_indices:\n",
    "            # Average values for each month in this patch\n",
    "            monthly_means = []\n",
    "            for month_idx in range(num_months):\n",
    "                patch = data_array[month_idx, patch_idx]\n",
    "                valid_values = patch[~np.isnan(patch)]\n",
    "                if len(valid_values) > 0:\n",
    "                    monthly_means.append(np.mean(valid_values))\n",
    "                else:\n",
    "                    monthly_means.append(np.nan)\n",
    "            \n",
    "            # Calculate changes between consecutive months\n",
    "            monthly_means = np.array(monthly_means)\n",
    "            valid_months = ~np.isnan(monthly_means)\n",
    "            if np.sum(valid_months) > 1:\n",
    "                valid_means = monthly_means[valid_months]\n",
    "                month_to_month = np.abs(np.diff(valid_means))\n",
    "                if len(month_to_month) > 0:\n",
    "                    changes.extend(month_to_month)\n",
    "        \n",
    "        if changes:\n",
    "            result['mean_change'] = np.mean(changes)\n",
    "            result['max_change'] = np.max(changes)\n",
    "            \n",
    "            # Calculate overall standard deviation of the data\n",
    "            all_valid_data = data_array[~np.isnan(data_array)]\n",
    "            if len(all_valid_data) > 0:\n",
    "                result['std_dev'] = np.std(all_valid_data)\n",
    "            \n",
    "            # Simple check for seasonal patterns based on feature type\n",
    "            # For temperature: we expect higher values in summer months (seasonal)\n",
    "            # For precipitation: may show seasonal patterns depending on location\n",
    "            # For lightning: often follows seasonal patterns\n",
    "            has_pattern = False\n",
    "            \n",
    "            # This is a simplified check - in reality would need more sophisticated analysis\n",
    "            # and would depend on hemisphere (northern/southern) for seasonality timing\n",
    "            if \"Temperature\" in feature_name or \"Lightning\" in feature_name:\n",
    "                has_pattern = True\n",
    "            elif \"Precipitation\" in feature_name or \"Vapor_Pressure\" in feature_name:\n",
    "                # Check if there's reasonable variation throughout the year\n",
    "                # Simple test: If std dev is > 10% of mean, likely has seasonal pattern\n",
    "                if len(all_valid_data) > 0:\n",
    "                    mean_value = np.mean(all_valid_data)\n",
    "                    if mean_value != 0 and result['std_dev'] / mean_value > 0.1:\n",
    "                        has_pattern = True\n",
    "            \n",
    "            result['has_seasonal_pattern'] = has_pattern\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in temporal variation analysis: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set a random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    validate_data_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b9effa-e253-4837-acd6-1e71c6d45e86",
   "metadata": {},
   "source": [
    "# TARGET VARIABLE : BURNED AREA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c9b67-a1e4-4d7f-b32e-fceb307f71aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading valid patches from 'valid_patches_under30.npz'...\n",
      "Found 2051 valid patches (<= 30.0% NaNs).\n",
      "Patch size = 128x128\n",
      "\n",
      "Generated 216 required bands from 2005 to 2022.\n",
      "Using 8 parallel processes for 18 years\n",
      "Starting parallel processing of years...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 216/216 [02:03<00:00,  1.75it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import gc\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Path and variable name for your burned-area dataset\n",
    "BURNED_DATASET = \"/home/ubuntu/Data-Seasonal-forecast/Response-Burned_area/MODIS_BA_nc_005_Aus.nc\"\n",
    "BURNED_VARIABLE = \"burned_area\"\n",
    "BURNED_NAME = \"BurnedArea\"\n",
    "\n",
    "# Example year range\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2022\n",
    "\n",
    "def generate_year_month_bands(start_year, end_year):\n",
    "    \"\"\"\n",
    "    Builds a list of required band strings in the format 'YYYY-MM'.\n",
    "    Example: '2001-01', '2001-02', ..., '2001-12', '2002-01', etc.\n",
    "    \"\"\"\n",
    "    required = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            band_str = f\"{year:04d}-{month:02d}\"\n",
    "            required.append(band_str)\n",
    "    return required\n",
    "\n",
    "def process_year(args):\n",
    "    \"\"\"\n",
    "    Process a single year of burned area data. All arguments are passed as a single tuple to\n",
    "    support multiprocessing.\n",
    "    \"\"\"\n",
    "    year, ds, required_bands, valid_patches_list, patch_size, out_root = args\n",
    "    \n",
    "    # Create a progress file for this process to track progress\n",
    "    progress_file = os.path.join(out_root, f\"progress_{year}.txt\")\n",
    "    with open(progress_file, 'w') as f:\n",
    "        f.write(f\"0/12\\n\")  # Assuming 12 months per year\n",
    "    \n",
    "    try:\n",
    "        # Gather the band strings for that year\n",
    "        year_str = f\"{year:04d}-\"\n",
    "        year_bands = [b for b in required_bands if b.startswith(year_str)]\n",
    "        months_in_year = len(year_bands)\n",
    "        num_valid_patches = len(valid_patches_list)\n",
    "        \n",
    "        # Allocate [months, patches, patch_size, patch_size]\n",
    "        year_data = np.zeros((months_in_year, num_valid_patches, patch_size, patch_size), dtype=np.float32)\n",
    "        year_data.fill(np.nan)\n",
    "        \n",
    "        for month_idx, band_str in enumerate(year_bands):\n",
    "            # Update progress\n",
    "            with open(progress_file, 'w') as f:\n",
    "                f.write(f\"{month_idx + 1}/12\\n\")\n",
    "                \n",
    "            try:\n",
    "                # Load the 2D array for that band\n",
    "                band_2d = ds[BURNED_VARIABLE].sel(band=band_str).load().values  # shape=(lat_dim, lon_dim)\n",
    "                \n",
    "                # Extract each valid patch\n",
    "                for p_idx, patch_info in enumerate(valid_patches_list):\n",
    "                    lat_start = patch_info[\"lat_start\"]\n",
    "                    lat_end = patch_info[\"lat_end\"]\n",
    "                    lon_start = patch_info[\"lon_start\"]\n",
    "                    lon_end = patch_info[\"lon_end\"]\n",
    "                    \n",
    "                    subarray = band_2d[lat_start:lat_end, lon_start:lon_end].copy()\n",
    "                    year_data[month_idx, p_idx] = subarray\n",
    "                \n",
    "                del band_2d\n",
    "            except Exception as e:\n",
    "                with open(os.path.join(out_root, f\"error_{year}.txt\"), 'a') as f:\n",
    "                    f.write(f\"Could not load band '{band_str}': {e}\\n\")\n",
    "        \n",
    "        # Save one .npz file per year\n",
    "        year_dir = os.path.join(out_root, f\"{year}\")\n",
    "        os.makedirs(year_dir, exist_ok=True)\n",
    "        \n",
    "        out_file = os.path.join(year_dir, f\"{BURNED_NAME}.npz\")\n",
    "        metadata = {\n",
    "            \"feature_name\": BURNED_NAME,\n",
    "            \"feature_var\": BURNED_VARIABLE,\n",
    "            \"year\": year,\n",
    "            \"bands\": year_bands,\n",
    "            \"patch_size\": patch_size,\n",
    "            \"num_patches\": num_valid_patches\n",
    "        }\n",
    "        \n",
    "        # Build patch index map\n",
    "        patch_indices = {}\n",
    "        for i, patch_info in enumerate(valid_patches_list):\n",
    "            pi = patch_info[\"patch_i\"]\n",
    "            pj = patch_info[\"patch_j\"]\n",
    "            patch_indices[f\"{pi}_{pj}\"] = i\n",
    "        \n",
    "        np.savez_compressed(\n",
    "            out_file,\n",
    "            data=year_data,\n",
    "            metadata=metadata,\n",
    "            patch_indices=patch_indices\n",
    "        )\n",
    "        \n",
    "        del year_data\n",
    "        gc.collect()\n",
    "        \n",
    "        # Mark as complete\n",
    "        with open(progress_file, 'w') as f:\n",
    "            f.write(f\"COMPLETE\\n\")\n",
    "            \n",
    "        return year\n",
    "        \n",
    "    except Exception as e:\n",
    "        with open(os.path.join(out_root, f\"error_{year}.txt\"), 'a') as f:\n",
    "            f.write(f\"Unexpected error: {e}\\n\")\n",
    "        return None\n",
    "\n",
    "def monitor_progress(out_root, years):\n",
    "    \"\"\"\n",
    "    Monitor progress of all processes and display in a single progress bar.\n",
    "    \"\"\"\n",
    "    years_done = set()\n",
    "    total_years = len(years)\n",
    "    \n",
    "    with tqdm(total=total_years * 12, desc=\"Overall Progress\") as pbar:\n",
    "        previous_progress = 0\n",
    "        \n",
    "        while len(years_done) < total_years:\n",
    "            current_progress = 0\n",
    "            \n",
    "            for year in years:\n",
    "                if year in years_done:\n",
    "                    current_progress += 12  # 12 months already counted\n",
    "                    continue\n",
    "                    \n",
    "                progress_file = os.path.join(out_root, f\"progress_{year}.txt\")\n",
    "                if os.path.exists(progress_file):\n",
    "                    try:\n",
    "                        with open(progress_file, 'r') as f:\n",
    "                            content = f.read().strip()\n",
    "                            if content == \"COMPLETE\":\n",
    "                                years_done.add(year)\n",
    "                                current_progress += 12\n",
    "                            else:\n",
    "                                try:\n",
    "                                    progress, total = content.split('/')\n",
    "                                    current_progress += int(progress)\n",
    "                                except:\n",
    "                                    pass\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(current_progress - previous_progress)\n",
    "            previous_progress = current_progress\n",
    "            \n",
    "            # Don't update too frequently\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            if len(years_done) == total_years:\n",
    "                # One final update\n",
    "                pbar.update(total_years * 12 - previous_progress)\n",
    "                break\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1) Load valid patches from 'valid_patches_under30.npz'\n",
    "    valid_patches_file = \"valid_patches_under30.npz\"\n",
    "    if not os.path.exists(valid_patches_file):\n",
    "        raise FileNotFoundError(f\"Valid patches file '{valid_patches_file}' not found.\")\n",
    "    \n",
    "    print(f\"Loading valid patches from '{valid_patches_file}'...\")\n",
    "    data = np.load(valid_patches_file, allow_pickle=True)\n",
    "    \n",
    "    # Convert structured array to a list of dictionaries\n",
    "    valid_struct = data[\"valid_patches\"]\n",
    "    valid_patches_list = []\n",
    "    for i in range(len(valid_struct)):\n",
    "        patch_dict = {}\n",
    "        for field in valid_struct.dtype.names:\n",
    "            patch_dict[field] = valid_struct[field][i]\n",
    "        valid_patches_list.append(patch_dict)\n",
    "    \n",
    "    patch_size = int(data[\"patch_size\"])\n",
    "    threshold = float(data[\"threshold\"])\n",
    "    num_valid_patches = len(valid_patches_list)\n",
    "\n",
    "    print(f\"Found {num_valid_patches} valid patches (<= {threshold}% NaNs).\")\n",
    "    print(f\"Patch size = {patch_size}x{patch_size}\\n\")\n",
    "\n",
    "    # 2) Generate the list of band strings like 'YYYY-MM'\n",
    "    required_bands = generate_year_month_bands(START_YEAR, END_YEAR)\n",
    "    print(f\"Generated {len(required_bands)} required bands from {START_YEAR} to {END_YEAR}.\")\n",
    "\n",
    "    # 3) Open the burned-area dataset\n",
    "    if not os.path.exists(BURNED_DATASET):\n",
    "        raise FileNotFoundError(f\"Dataset '{BURNED_DATASET}' not found.\")\n",
    "    \n",
    "    ds = xr.open_dataset(BURNED_DATASET)\n",
    "    if BURNED_VARIABLE not in ds.data_vars:\n",
    "        ds.close()\n",
    "        raise KeyError(f\"Variable '{BURNED_VARIABLE}' not found in the dataset.\")\n",
    "    \n",
    "    # 4) Verify that all required bands exist\n",
    "    actual_bands = set(str(b) for b in ds[\"band\"].values)  # e.g. {\"2000-11\", \"2000-12\", \"2001-01\", ...}\n",
    "    missing_bands = [b for b in required_bands if b not in actual_bands]\n",
    "    if missing_bands:\n",
    "        ds.close()\n",
    "        print(f\"Missing some required 'YYYY-MM' bands (showing up to 10): {missing_bands[:10]}\")\n",
    "        raise ValueError(\"Not all required year-month bands are present in the dataset.\")\n",
    "\n",
    "    # 5) Create an output folder\n",
    "    out_root = \"Target\"\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "    \n",
    "    # Create year directories\n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        os.makedirs(os.path.join(out_root, str(year)), exist_ok=True)\n",
    "    \n",
    "    # 6) Prepare for parallel processing by year\n",
    "    years = list(range(START_YEAR, END_YEAR + 1))\n",
    "    \n",
    "    # Determine number of processes to use (adjust according to memory requirements)\n",
    "    # Since we're only dealing with one dataset at a time, we can use more processes\n",
    "    num_processes = min(8, len(years), mp.cpu_count())\n",
    "    print(f\"Using {num_processes} parallel processes for {len(years)} years\")\n",
    "    \n",
    "    # Prepare arguments for each process\n",
    "    process_args = []\n",
    "    for year in years:\n",
    "        args = (\n",
    "            year,              # Year to process\n",
    "            ds,                # Dataset (shared)\n",
    "            required_bands,    # Required bands\n",
    "            valid_patches_list, # Valid patches\n",
    "            patch_size,        # Patch size\n",
    "            out_root          # Output root directory\n",
    "        )\n",
    "        process_args.append(args)\n",
    "    \n",
    "    # Start the multiprocessing pool\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "    print(\"Starting parallel processing of years...\")\n",
    "    \n",
    "    # Start processes\n",
    "    pool.map_async(process_year, process_args)\n",
    "    \n",
    "    # Start monitoring thread for progress display\n",
    "    monitor_progress(out_root, years)\n",
    "    \n",
    "    # Wait for all processes to complete\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # Close the dataset\n",
    "    ds.close()\n",
    "    \n",
    "    # Check which years were successfully processed\n",
    "    completed_years = []\n",
    "    for year in years:\n",
    "        progress_file = os.path.join(out_root, f\"progress_{year}.txt\")\n",
    "        if os.path.exists(progress_file):\n",
    "            with open(progress_file, 'r') as f:\n",
    "                content = f.read().strip()\n",
    "                if content == \"COMPLETE\":\n",
    "                    completed_years.append(year)\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(completed_years)} years out of {len(years)}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nAll done. Burned-area data processed for {START_YEAR}-{END_YEAR}, in 'YYYY-MM' format.\")\n",
    "    print(f\"Total time: {elapsed:.2f}s ({elapsed/60:.2f} min).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Required for Windows multiprocessing\n",
    "    mp.freeze_support()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc41f8ec-575d-42d7-a8da-9cbd12243c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Burned Area Analysis Results:\n",
      "================================================================================\n",
      "Band       | Patches with Fire (%)               | Avg Burned Area in Affected Patches (%)\n",
      "--------------------------------------------------------------------------------\n",
      "2005-01    | 16.67   % (  342/2051 ) | 0.53    %\n",
      "2005-02    | 16.97   % (  348/2051 ) | 0.54    %\n",
      "2005-03    | 17.94   % (  368/2051 ) | 0.27    %\n",
      "2005-04    | 14.97   % (  307/2051 ) | 1.21    %\n",
      "2005-05    | 16.04   % (  329/2051 ) | 2.17    %\n",
      "2005-06    | 7.95    % (  163/2051 ) | 1.68    %\n",
      "2005-07    | 9.61    % (  197/2051 ) | 1.60    %\n",
      "2005-08    | 9.80    % (  201/2051 ) | 1.98    %\n",
      "2005-09    | 12.24   % (  251/2051 ) | 2.15    %\n",
      "2005-10    | 18.87   % (  387/2051 ) | 2.45    %\n",
      "2005-11    | 22.82   % (  468/2051 ) | 1.70    %\n",
      "2005-12    | 21.36   % (  438/2051 ) | 0.86    %\n",
      "2006-01    | 10.73   % (  220/2051 ) | 0.42    %\n",
      "2006-02    | 7.85    % (  161/2051 ) | 0.15    %\n",
      "2006-03    | 10.53   % (  216/2051 ) | 0.16    %\n",
      "2006-04    | 11.85   % (  243/2051 ) | 0.56    %\n",
      "2006-05    | 16.87   % (  346/2051 ) | 1.04    %\n",
      "2006-06    | 14.38   % (  295/2051 ) | 1.13    %\n",
      "2006-07    | 14.77   % (  303/2051 ) | 1.12    %\n",
      "2006-08    | 16.38   % (  336/2051 ) | 3.00    %\n",
      "2006-09    | 19.75   % (  405/2051 ) | 4.12    %\n",
      "2006-10    | 24.82   % (  509/2051 ) | 3.25    %\n",
      "2006-11    | 35.98   % (  738/2051 ) | 5.89    %\n",
      "2006-12    | 29.74   % (  610/2051 ) | 2.22    %\n",
      "2007-01    | 11.70   % (  240/2051 ) | 0.42    %\n",
      "2007-02    | 15.85   % (  325/2051 ) | 0.81    %\n",
      "2007-03    | 11.90   % (  244/2051 ) | 0.23    %\n",
      "2007-04    | 12.68   % (  260/2051 ) | 0.59    %\n",
      "2007-05    | 11.99   % (  246/2051 ) | 1.62    %\n",
      "2007-06    | 10.97   % (  225/2051 ) | 1.91    %\n",
      "2007-07    | 13.70   % (  281/2051 ) | 1.60    %\n",
      "2007-08    | 14.04   % (  288/2051 ) | 2.82    %\n",
      "2007-09    | 19.84   % (  407/2051 ) | 6.30    %\n",
      "2007-10    | 23.31   % (  478/2051 ) | 5.04    %\n",
      "2007-11    | 25.01   % (  513/2051 ) | 3.43    %\n",
      "2007-12    | 24.28   % (  498/2051 ) | 1.36    %\n",
      "2008-01    | 15.07   % (  309/2051 ) | 1.28    %\n",
      "2008-02    | 8.58    % (  176/2051 ) | 0.20    %\n",
      "2008-03    | 10.29   % (  211/2051 ) | 0.23    %\n",
      "2008-04    | 15.16   % (  311/2051 ) | 1.25    %\n",
      "2008-05    | 13.26   % (  272/2051 ) | 2.78    %\n",
      "2008-06    | 10.87   % (  223/2051 ) | 1.17    %\n",
      "2008-07    | 10.82   % (  222/2051 ) | 1.50    %\n",
      "2008-08    | 12.53   % (  257/2051 ) | 2.06    %\n",
      "2008-09    | 12.97   % (  266/2051 ) | 1.68    %\n",
      "2008-10    | 20.82   % (  427/2051 ) | 3.89    %\n",
      "2008-11    | 22.82   % (  468/2051 ) | 2.25    %\n",
      "2008-12    | 15.26   % (  313/2051 ) | 0.41    %\n",
      "2009-01    | 8.00    % (  164/2051 ) | 0.26    %\n",
      "2009-02    | 7.22    % (  148/2051 ) | 0.55    %\n",
      "2009-03    | 11.56   % (  237/2051 ) | 0.22    %\n",
      "2009-04    | 16.53   % (  339/2051 ) | 1.51    %\n",
      "2009-05    | 16.24   % (  333/2051 ) | 1.89    %\n",
      "2009-06    | 11.41   % (  234/2051 ) | 1.34    %\n",
      "2009-07    | 12.14   % (  249/2051 ) | 1.21    %\n",
      "2009-08    | 13.60   % (  279/2051 ) | 1.30    %\n",
      "2009-09    | 14.97   % (  307/2051 ) | 2.02    %\n",
      "2009-10    | 20.28   % (  416/2051 ) | 4.48    %\n",
      "2009-11    | 26.67   % (  547/2051 ) | 3.78    %\n",
      "2009-12    | 19.80   % (  406/2051 ) | 2.30    %\n",
      "2010-01    | 16.63   % (  341/2051 ) | 0.35    %\n",
      "2010-02    | 9.26    % (  190/2051 ) | 0.89    %\n",
      "2010-03    | 10.39   % (  213/2051 ) | 0.28    %\n",
      "2010-04    | 12.73   % (  261/2051 ) | 0.73    %\n",
      "2010-05    | 14.48   % (  297/2051 ) | 1.09    %\n",
      "2010-06    | 10.04   % (  206/2051 ) | 1.59    %\n",
      "2010-07    | 9.80    % (  201/2051 ) | 1.92    %\n",
      "2010-08    | 10.82   % (  222/2051 ) | 2.01    %\n",
      "2010-09    | 13.21   % (  271/2051 ) | 1.00    %\n",
      "2010-10    | 15.65   % (  321/2051 ) | 1.40    %\n",
      "2010-11    | 17.36   % (  356/2051 ) | 1.68    %\n",
      "2010-12    | 10.82   % (  222/2051 ) | 0.48    %\n",
      "2011-01    | 10.48   % (  215/2051 ) | 0.68    %\n",
      "2011-02    | 9.80    % (  201/2051 ) | 0.25    %\n",
      "2011-03    | 15.36   % (  315/2051 ) | 0.45    %\n",
      "2011-04    | 19.45   % (  399/2051 ) | 1.07    %\n",
      "2011-05    | 21.26   % (  436/2051 ) | 2.42    %\n",
      "2011-06    | 17.65   % (  362/2051 ) | 1.71    %\n",
      "2011-07    | 17.55   % (  360/2051 ) | 1.72    %\n",
      "2011-08    | 21.06   % (  432/2051 ) | 4.00    %\n",
      "2011-09    | 30.08   % (  617/2051 ) | 9.24    %\n",
      "2011-10    | 39.01   % (  800/2051 ) | 6.10    %\n",
      "2011-11    | 31.25   % (  641/2051 ) | 2.20    %\n",
      "2011-12    | 25.60   % (  525/2051 ) | 1.94    %\n",
      "2012-01    | 23.01   % (  472/2051 ) | 1.49    %\n",
      "2012-02    | 23.31   % (  478/2051 ) | 1.03    %\n",
      "2012-03    | 19.06   % (  391/2051 ) | 0.24    %\n",
      "2012-04    | 22.72   % (  466/2051 ) | 1.81    %\n",
      "2012-05    | 21.45   % (  440/2051 ) | 2.35    %\n",
      "2012-06    | 17.60   % (  361/2051 ) | 2.35    %\n",
      "2012-07    | 16.63   % (  341/2051 ) | 1.94    %\n",
      "2012-08    | 21.99   % (  451/2051 ) | 2.76    %\n",
      "2012-09    | 28.23   % (  579/2051 ) | 4.55    %\n",
      "2012-10    | 38.76   % (  795/2051 ) | 6.34    %\n",
      "2012-11    | 40.91   % (  839/2051 ) | 4.06    %\n",
      "2012-12    | 32.52   % (  667/2051 ) | 1.71    %\n",
      "2013-01    | 29.01   % (  595/2051 ) | 1.03    %\n",
      "2013-02    | 14.24   % (  292/2051 ) | 0.41    %\n",
      "2013-03    | 13.51   % (  277/2051 ) | 0.30    %\n",
      "2013-04    | 17.06   % (  350/2051 ) | 1.19    %\n",
      "2013-05    | 11.36   % (  233/2051 ) | 1.13    %\n",
      "2013-06    | 10.73   % (  220/2051 ) | 1.49    %\n",
      "2013-07    | 11.56   % (  237/2051 ) | 1.43    %\n",
      "2013-08    | 13.46   % (  276/2051 ) | 1.61    %\n",
      "2013-09    | 15.60   % (  320/2051 ) | 1.82    %\n",
      "2013-10    | 20.53   % (  421/2051 ) | 2.73    %\n",
      "2013-11    | 24.91   % (  511/2051 ) | 1.56    %\n",
      "2013-12    | 19.55   % (  401/2051 ) | 0.67    %\n",
      "2014-01    | 14.97   % (  307/2051 ) | 0.96    %\n",
      "2014-02    | 8.09    % (  166/2051 ) | 0.40    %\n",
      "2014-03    | 13.16   % (  270/2051 ) | 0.45    %\n",
      "2014-04    | 15.70   % (  322/2051 ) | 0.78    %\n",
      "2014-05    | 15.07   % (  309/2051 ) | 1.55    %\n",
      "2014-06    | 12.19   % (  250/2051 ) | 2.64    %\n",
      "2014-07    | 13.55   % (  278/2051 ) | 1.60    %\n",
      "2014-08    | 15.36   % (  315/2051 ) | 2.71    %\n",
      "2014-09    | 17.80   % (  365/2051 ) | 3.79    %\n",
      "2014-10    | 27.30   % (  560/2051 ) | 4.46    %\n",
      "2014-11    | 29.69   % (  609/2051 ) | 2.77    %\n",
      "2014-12    | 22.18   % (  455/2051 ) | 0.82    %\n",
      "2015-01    | 15.02   % (  308/2051 ) | 0.46    %\n",
      "2015-02    | 10.68   % (  219/2051 ) | 0.55    %\n",
      "2015-03    | 15.26   % (  313/2051 ) | 0.76    %\n",
      "2015-04    | 19.31   % (  396/2051 ) | 2.29    %\n",
      "2015-05    | 15.99   % (  328/2051 ) | 3.08    %\n",
      "2015-06    | 12.34   % (  253/2051 ) | 1.55    %\n",
      "2015-07    | 9.85    % (  202/2051 ) | 1.86    %\n",
      "2015-08    | 11.95   % (  245/2051 ) | 2.16    %\n",
      "2015-09    | 14.68   % (  301/2051 ) | 2.37    %\n",
      "2015-10    | 25.50   % (  523/2051 ) | 3.18    %\n",
      "2015-11    | 31.74   % (  651/2051 ) | 2.65    %\n",
      "2015-12    | 19.89   % (  408/2051 ) | 0.73    %\n",
      "2016-01    | 13.21   % (  271/2051 ) | 0.50    %\n",
      "2016-02    | 11.07   % (  227/2051 ) | 0.19    %\n",
      "2016-03    | 14.48   % (  297/2051 ) | 0.31    %\n",
      "2016-04    | 14.09   % (  289/2051 ) | 1.51    %\n",
      "2016-05    | 16.09   % (  330/2051 ) | 0.77    %\n",
      "2016-06    | 8.19    % (  168/2051 ) | 1.64    %\n",
      "2016-07    | 8.68    % (  178/2051 ) | 2.45    %\n",
      "2016-08    | 8.82    % (  181/2051 ) | 2.00    %\n",
      "2016-09    | 10.14   % (  208/2051 ) | 0.67    %\n",
      "2016-10    | 14.43   % (  296/2051 ) | 2.60    %\n",
      "2016-11    | 23.65   % (  485/2051 ) | 2.67    %\n",
      "2016-12    | 18.97   % (  389/2051 ) | 0.83    %\n",
      "2017-01    | 6.78    % (  139/2051 ) | 0.10    %\n",
      "2017-02    | 10.87   % (  223/2051 ) | 0.29    %\n",
      "2017-03    | 15.21   % (  312/2051 ) | 0.57    %\n",
      "2017-04    | 21.60   % (  443/2051 ) | 0.67    %\n",
      "2017-05    | 22.62   % (  464/2051 ) | 1.89    %\n",
      "2017-06    | 16.28   % (  334/2051 ) | 1.80    %\n",
      "2017-07    | 13.94   % (  286/2051 ) | 1.10    %\n",
      "2017-08    | 15.94   % (  327/2051 ) | 1.74    %\n",
      "2017-09    | 19.70   % (  404/2051 ) | 4.28    %\n",
      "2017-10    | 25.35   % (  520/2051 ) | 2.82    %\n",
      "2017-11    | 29.06   % (  596/2051 ) | 3.71    %\n",
      "2017-12    | 27.74   % (  569/2051 ) | 2.55    %\n",
      "2018-01    | 17.21   % (  353/2051 ) | 0.65    %\n",
      "2018-02    | 11.12   % (  228/2051 ) | 0.29    %\n",
      "2018-03    | 14.77   % (  303/2051 ) | 0.65    %\n",
      "2018-04    | 19.84   % (  407/2051 ) | 2.88    %\n",
      "2018-05    | 19.06   % (  391/2051 ) | 3.18    %\n",
      "2018-06    | 13.65   % (  280/2051 ) | 1.70    %\n",
      "2018-07    | 14.82   % (  304/2051 ) | 1.38    %\n",
      "2018-08    | 13.65   % (  280/2051 ) | 1.47    %\n",
      "2018-09    | 14.09   % (  289/2051 ) | 2.58    %\n",
      "2018-10    | 25.40   % (  521/2051 ) | 4.38    %\n",
      "2018-11    | 27.06   % (  555/2051 ) | 3.53    %\n",
      "2018-12    | 25.99   % (  533/2051 ) | 1.95    %\n",
      "2019-01    | 23.26   % (  477/2051 ) | 1.58    %\n",
      "2019-02    | 17.26   % (  354/2051 ) | 1.06    %\n",
      "2019-03    | 16.67   % (  342/2051 ) | 0.82    %\n",
      "2019-04    | 13.51   % (  277/2051 ) | 0.93    %\n",
      "2019-05    | 11.21   % (  230/2051 ) | 1.60    %\n",
      "2019-06    | 8.73    % (  179/2051 ) | 2.22    %\n",
      "2019-07    | 9.02    % (  185/2051 ) | 2.44    %\n",
      "2019-08    | 9.85    % (  202/2051 ) | 1.69    %\n",
      "2019-09    | 10.97   % (  225/2051 ) | 1.56    %\n",
      "2019-10    | 16.33   % (  335/2051 ) | 2.21    %\n",
      "2019-11    | 20.62   % (  423/2051 ) | 2.32    %\n",
      "2019-12    | 28.13   % (  577/2051 ) | 2.79    %\n",
      "2020-01    | 14.92   % (  306/2051 ) | 1.70    %\n",
      "2020-02    | 6.87    % (  141/2051 ) | 0.85    %\n",
      "2020-03    | 6.97    % (  143/2051 ) | 0.30    %\n",
      "2020-04    | 11.12   % (  228/2051 ) | 0.53    %\n",
      "2020-05    | 12.09   % (  248/2051 ) | 1.39    %\n",
      "2020-06    | 8.19    % (  168/2051 ) | 1.64    %\n",
      "2020-07    | 8.14    % (  167/2051 ) | 2.47    %\n",
      "2020-08    | 9.17    % (  188/2051 ) | 0.82    %\n",
      "2020-09    | 10.19   % (  209/2051 ) | 0.79    %\n",
      "2020-10    | 13.94   % (  286/2051 ) | 0.93    %\n",
      "2020-11    | 18.24   % (  374/2051 ) | 1.42    %\n",
      "2020-12    | 10.29   % (  211/2051 ) | 0.47    %\n",
      "2021-01    | 7.26    % (  149/2051 ) | 0.19    %\n",
      "2021-02    | 5.41    % (  111/2051 ) | 0.21    %\n",
      "2021-03    | 10.29   % (  211/2051 ) | 0.39    %\n",
      "2021-04    | 13.07   % (  268/2051 ) | 0.93    %\n",
      "2021-05    | 16.77   % (  344/2051 ) | 2.06    %\n",
      "2021-06    | 12.53   % (  257/2051 ) | 0.92    %\n",
      "2021-07    | 11.31   % (  232/2051 ) | 1.34    %\n",
      "2021-08    | 12.19   % (  250/2051 ) | 1.63    %\n",
      "2021-09    | 13.41   % (  275/2051 ) | 1.26    %\n",
      "2021-10    | 20.19   % (  414/2051 ) | 2.84    %\n",
      "2021-11    | 18.28   % (  375/2051 ) | 2.28    %\n",
      "2021-12    | 18.67   % (  383/2051 ) | 1.57    %\n",
      "2022-01    | 13.75   % (  282/2051 ) | 0.80    %\n",
      "2022-02    | 11.80   % (  242/2051 ) | 0.42    %\n",
      "2022-03    | 15.99   % (  328/2051 ) | 0.76    %\n",
      "2022-04    | 17.50   % (  359/2051 ) | 2.55    %\n",
      "2022-05    | 13.07   % (  268/2051 ) | 2.24    %\n",
      "2022-06    | 11.95   % (  245/2051 ) | 1.71    %\n",
      "2022-07    | 10.82   % (  222/2051 ) | 1.66    %\n",
      "2022-08    | 11.85   % (  243/2051 ) | 2.11    %\n",
      "2022-09    | 10.68   % (  219/2051 ) | 1.82    %\n",
      "2022-10    | 16.58   % (  340/2051 ) | 2.02    %\n",
      "2022-11    | 18.48   % (  379/2051 ) | 1.88    %\n",
      "2022-12    | 15.50   % (  318/2051 ) | 0.59    %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "TARGET_DIR = \"Target\"\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2022\n",
    "FEATURE_NAME = \"BurnedArea\"\n",
    "\n",
    "def analyze_burned_areas():\n",
    "    \"\"\"\n",
    "    Analyze and print burned area data:\n",
    "    1. For each band (e.g., \"2005-01\"), the percentage of patches with at least 1 burned pixel\n",
    "    2. For patches with burned pixels, the percentage of pixels that are burned (have value 1)\n",
    "    \"\"\"\n",
    "    print(\"\\nBurned Area Analysis Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"{:<10} | {:<35} | {:<35}\".format(\n",
    "        \"Band\", \n",
    "        \"Patches with Fire (%)\", \n",
    "        \"Avg Burned Area in Affected Patches (%)\"\n",
    "    ))\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Process each year\n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        year_dir = os.path.join(TARGET_DIR, str(year))\n",
    "        npz_file = os.path.join(year_dir, f\"{FEATURE_NAME}.npz\")\n",
    "        \n",
    "        if not os.path.exists(npz_file):\n",
    "            # print(f\"Warning: File for year {year} not found: {npz_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Load the data with allow_pickle=True\n",
    "        data = np.load(npz_file, allow_pickle=True)\n",
    "        year_data = data['data']  # Shape: [months, patches, patch_height, patch_width]\n",
    "        \n",
    "        metadata = data['metadata'].item()\n",
    "        bands = metadata['bands']  # e.g., ['2005-01', '2005-02', ...]\n",
    "        \n",
    "        # Process each month/band\n",
    "        for month_idx, band in enumerate(bands):\n",
    "            # Get all patches for this month\n",
    "            monthly_patches = year_data[month_idx]  # Shape: [patches, patch_height, patch_width]\n",
    "            \n",
    "            total_patches = monthly_patches.shape[0]\n",
    "            patches_with_fire = 0\n",
    "            fire_percentages = []\n",
    "            \n",
    "            # Analyze each patch\n",
    "            for patch_idx in range(total_patches):\n",
    "                patch = monthly_patches[patch_idx]  # Shape: [patch_height, patch_width]\n",
    "                \n",
    "                # Check if patch has at least one burned pixel\n",
    "                if np.any(patch == 1):\n",
    "                    patches_with_fire += 1\n",
    "                    \n",
    "                    # Calculate percentage of burned area within this patch\n",
    "                    total_pixels = patch.size\n",
    "                    burned_pixels = np.sum(patch == 1)\n",
    "                    fire_percent = (burned_pixels / total_pixels) * 100\n",
    "                    fire_percentages.append(fire_percent)\n",
    "            \n",
    "            # Calculate and print results for this band\n",
    "            fire_presence_percent = (patches_with_fire / total_patches) * 100 if total_patches > 0 else 0\n",
    "            avg_fire_percent = np.mean(fire_percentages) if fire_percentages else 0\n",
    "            \n",
    "            print(\"{:<10} | {:<8.2f}% ({:>5}/{:<5}) | {:<8.2f}%\".format(\n",
    "                band,\n",
    "                fire_presence_percent,\n",
    "                patches_with_fire, total_patches,\n",
    "                avg_fire_percent\n",
    "            ))\n",
    "\n",
    "def main():\n",
    "    # Check if the Target directory exists\n",
    "    if not os.path.exists(TARGET_DIR):\n",
    "        print(f\"Error: Target directory '{TARGET_DIR}' not found.\")\n",
    "        return\n",
    "    \n",
    "    # Analyze and print the data\n",
    "    analyze_burned_areas()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0a216-ef97-47d5-ad6c-a42689e6a1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
